"""ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ëª¨ë“ˆ - jieba & MeCab í™œìš©"""

import logging
import numpy as np
import regex
import re
import itertools
from typing import List, Callable
import jieba
import MeCab
import os

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •ê°’
DEFAULT_MIN_TOKENS = 1
DEFAULT_MAX_TOKENS = 50
DEFAULT_SIMILARITY_THRESHOLD = 0.7

# jiebaì™€ MeCab ì´ˆê¸°í™”
try:
    # ì‚¬ìš©ì ì‚¬ì „ ê²½ë¡œë¥¼ .venv/Scripts/user.dicë¡œ ì§€ì •
    mecabrc_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir/mecabrc'
    dicdir_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir'
    userdic_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir/user.dic'
    mecab = MeCab.Tagger(f'-r {mecabrc_path} -d {dicdir_path} -u {userdic_path}')
    print("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ")
    logger.info("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ") # -dëŠ” ì‚¬ì „ ë””ë ‰í† ë¦¬, -uëŠ” ì‚¬ìš©ì ì‚¬ì „ ê²½ë¡œ
except Exception as e:
    print(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    logger.warning(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    mecab = None

# ë¯¸ë¦¬ ì»´íŒŒì¼ëœ ì •ê·œì‹
hanja_re = regex.compile(r'\p{Han}+')
hangul_re = regex.compile(r'^\p{Hangul}+$')

def split_src_meaning_units(
    text: str,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    by_space: bool = False,
    **kwargs
):
    """ì›ë¬¸(í•œë¬¸)ì€ ë¬´ì¡°ê±´ jiebaë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  (tokenizer ì¸ì ë¬´ì‹œ)"""
    # tokenizer ì¸ì ë¬´ì‹œ, ë¬´ì¡°ê±´ jieba ì‚¬ìš©
    
    # 1ë‹¨ê³„: ì–´ì ˆ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ì–´ì ˆ ë‚´ë¶€ëŠ” ì ˆëŒ€ ìª¼ê°œì§€ì§€ ì•ŠìŒ)
    # ì „ê° ì½œë¡  ë’¤ì—ë§Œ ê³µë°±ì„ ì¶”ê°€í•˜ì—¬ "ì „ìš´(ç®‹äº‘)ï¼š" + "ê°ˆëŒ€ëŠ”" í˜•íƒœë¡œ ë¶„í• 
    words = text.replace('\n', ' ').replace('ï¼š', 'ï¼š ').split()
    if not words:
        return []
    
    # 2ë‹¨ê³„: jieba ë¶„ì„ ê²°ê³¼ ì°¸ê³ 
    jieba_tokens = list(jieba.cut(text))
    
    # 3ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì–´ì ˆë“¤ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
    units = []
    i = 0
    
    while i < len(words):
        word = words[i]
        
        # í•œì+ì¡°ì‚¬ íŒ¨í„´ (ë””í´íŠ¸ë¡œ í•­ìƒ ìˆ˜í–‰)
        if hanja_re.search(word):
            # í˜„ì¬ ì–´ì ˆì´ í•œìë¥¼ í¬í•¨í•˜ë©´ í•˜ë‚˜ì˜ ì˜ë¯¸ ë‹¨ìœ„
            units.append(word)
            i += 1
            continue
        
        # í•œê¸€ ì–´ì ˆë“¤ ì²˜ë¦¬ - jieba ë¶„ì„ ê²°ê³¼ ì°¸ê³ 
        if hangul_re.match(word):
            # jiebaê°€ ì œì•ˆí•˜ëŠ” ê²½ê³„ë¥¼ ì°¸ê³ í•´ì„œ ì˜ë¯¸ ë‹¨ìœ„ ê²°ì •
            group = [word]
            j = i + 1
            
            # ë‹¤ìŒ ì–´ì ˆë“¤ê³¼ ë¬¶ì„ì§€ jieba ê²°ê³¼ ì°¸ê³ í•´ì„œ ê²°ì •
            while j < len(words) and hangul_re.match(words[j]):
                # jieba í† í°ì—ì„œ ì—°ì†ì„± í™•ì¸
                should_group = _should_group_words_by_jieba(group + [words[j]], jieba_tokens)
                if should_group:
                    group.append(words[j])
                    j += 1
                else:
                    break
            
            units.append(' '.join(group))
            i = j
            continue
        
        # ê¸°íƒ€ ì–´ì ˆ (ìˆ«ì, êµ¬ë‘ì  ë“±)
        units.append(word)
        i += 1
    
    return units

def _should_group_words_by_jieba(word_group: List[str], jieba_tokens: List[str]) -> bool:
    """jieba ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ì„œ ì–´ì ˆë“¤ì„ ë¬¶ì„ì§€ ê²°ì •"""
    combined = ''.join(word_group)
    
    # jieba í† í° ì¤‘ì—ì„œ í˜„ì¬ ì¡°í•©ê³¼ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ìˆìœ¼ë©´ ë¬¶ê¸°
    for token in jieba_tokens:
        if token.replace(' ', '') == combined.replace(' ', ''):
            return True
    
    # ê¸¸ì´ ì œí•œ
    if len(combined) > 10:
        return False
    
    return len(word_group) <= 3

def split_inside_chunk(chunk: str) -> List[str]:
    """ë²ˆì—­ë¬¸ ì²­í¬ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  - MeCab ë¶„ì„ ì°¸ê³  (ê°œì„ ëœ ë²„ì „)"""
    
    if not chunk or not chunk.strip():
        return []
    
    # 1ë‹¨ê³„: ì–´ì ˆ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ì–´ì ˆ ë‚´ë¶€ëŠ” ì ˆëŒ€ ìª¼ê°œì§€ì§€ ì•ŠìŒ)
    # ì „ê° ì½œë¡  ë’¤ì—ë§Œ ê³µë°±ì„ ì¶”ê°€í•˜ì—¬ "ì „ìš´(ç®‹äº‘)ï¼š" + "ê°ˆëŒ€ëŠ”" í˜•íƒœë¡œ ë¶„í• 
    words = chunk.replace('ï¼š', 'ï¼š ').split()
    if not words:
        return []
    
    # 2ë‹¨ê³„: MeCab ë¶„ì„ ê²°ê³¼ ì°¸ê³ 
    morpheme_info = []
    if mecab:
        result = mecab.parse(chunk)
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    pos = parts[1].split(',')[0]
                    morpheme_info.append((surface, pos))
    
    # 3ë‹¨ê³„: MeCab ë¶„ì„ ê²°ê³¼ë¥¼ í™œìš©í•œ ì˜ë¯¸ ë‹¨ìœ„ ê·¸ë£¹í™”
    units = []
    current_group = []
    
    for word in words:
        current_group.append(word)
        
        # ì „ê° ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ë‹¨ì–´ëŠ” ì¦‰ì‹œ ë‹¨ìœ„ ì™„ì„± (í•˜ë“œ ê²½ê³„)
        if word.endswith('ï¼š') or word == 'ï¼š':
            units.append(' '.join(current_group))
            current_group = []
            continue
        
        # MeCab ë¶„ì„ ê²°ê³¼ë¡œ ê²½ê³„ íŒë‹¨ (í’ˆì‚¬ ì •ë³´ í™œìš©)
        should_break = _should_break_by_mecab(word, morpheme_info) if morpheme_info else False
        
        if should_break and current_group:
            units.append(' '.join(current_group))
            current_group = []
    
    if current_group:
        units.append(' '.join(current_group))
    
    return [unit.strip() for unit in units if unit.strip()]

def _should_break_by_mecab(word: str, morpheme_info: List[tuple]) -> bool:
    """MeCab ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ì„œ ì˜ë¯¸ ë‹¨ìœ„ ê²½ê³„ ê²°ì • - ë³´ì¡°ì‚¬(JX) ê°•í™”"""
    
    # wordì— í•´ë‹¹í•˜ëŠ” í˜•íƒœì†Œë“¤ì˜ í’ˆì‚¬ í™•ì¸ - ì •í™•í•œ ë§¤ì¹­
    for surface, pos in morpheme_info:
        # ë‹¨ì–´ê°€ í•´ë‹¹ í˜•íƒœì†Œë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸ (ë” ì •í™•í•œ ë§¤ì¹­)
        if word.endswith(surface):
            # 1. ê°•í•œ ê²½ê³„ ì‹ í˜¸ - ì¢…ê²°ì–´ë¯¸, êµ¬ë‘ì 
            if pos in ['EF', 'SF', 'SP']:
                return True
            
            # 2. ë³´ì¡°ì‚¬(JX) - ë§¤ìš° ì¤‘ìš”í•œ ë¬¸ë²•ì  í‘œì§€ë¡œ ê°•í™” ì²˜ë¦¬
            if pos == 'JX':
                return True  # ëª¨ë“  ë³´ì¡°ì‚¬ì—ì„œ ë¶„í• 
            
            # 3. ì£¼ìš” ì¡°ì‚¬ë“¤ - ì˜ë¯¸ ë‹¨ìœ„ ê²½ê³„
            if pos in ['JKS', 'JKO', 'JKC', 'JKB', 'JKG', 'JKV', 'JKQ']:
                return True  # ëª¨ë“  ì¡°ì‚¬ì—ì„œ ë¶„í• 
            
            # 4. ì—°ê²°ì–´ë¯¸(EC) - ë¬¸ì¥ ì—°ê²°
            if pos == 'EC':
                return True  # ëª¨ë“  ì—°ê²°ì–´ë¯¸ì—ì„œ ë¶„í• 
            
            # 5. ëª…ì‚¬í˜• ì „ì„±ì–´ë¯¸(ETN) - ëª…ì‚¬í™”
            if pos == 'ETN':
                return True
            
            # 6. ê´€í˜•í˜• ì „ì„±ì–´ë¯¸(ETM) - ê´€í˜•ì–´í™”
            if pos == 'ETM':
                return True
            
            # 7. ë™ì‚¬, í˜•ìš©ì‚¬ ì–´ê°„ ë‹¤ìŒì—ì„œ ê²½ê³„  
            if pos in ['VV', 'VA', 'VX']:
                return len(surface) >= 1  # ê¸¸ì´ 1 ì´ìƒì¸ ìš©ì–¸ ì–´ê°„ì—ì„œ ë¶„í• 
            
            # 8. ì¤‘ìš”í•œ ë¶€ì‚¬ì—ì„œ ë¶„í•  (MAG, MAJ)
            if pos in ['MAG', 'MAJ'] and len(surface) >= 2:
                return True  # ê¸¸ì´ 2 ì´ìƒì¸ ë¶€ì‚¬ì—ì„œ ë¶„í• 
    
    return False

def find_target_span_end_simple(src_unit: str, remaining_tgt: str) -> int:
    """ê°„ë‹¨í•œ íƒ€ê²Ÿ ìŠ¤íŒ¬ íƒìƒ‰"""
    hanja_chars = regex.findall(r'\p{Han}+', src_unit)
    if not hanja_chars:
        return 0
    last = hanja_chars[-1]
    idx = remaining_tgt.rfind(last)
    if idx == -1:
        return len(remaining_tgt)
    end = idx + len(last)
    next_space = remaining_tgt.find(' ', end)
    return next_space + 1 if next_space != -1 else len(remaining_tgt)

def find_target_span_end_semantic(
    src_unit: str,
    remaining_tgt: str,
    embed_func: Callable,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD
) -> int:
    """ìµœì í™”ëœ íƒ€ê²Ÿ ìŠ¤íŒ¬ íƒìƒ‰ í•¨ìˆ˜"""
    if not src_unit or not remaining_tgt:
        return 0
        
    try:
        # 1) ì›ë¬¸ ì„ë² ë”© (ë‹¨ì¼ ê³„ì‚°)
        src_emb = embed_func([src_unit])[0]
        
        # 2) ë²ˆì—­ë¬¸ í† í° ë¶„ë¦¬ ë° ëˆ„ì  ê¸¸ì´ ê³„ì‚°
        tgt_tokens = remaining_tgt.split()
        if not tgt_tokens:
            return 0
            
        upper = min(len(tgt_tokens), max_tokens)
        cumulative_lengths = [0]
        current_length = 0
        
        for tok in tgt_tokens:
            current_length += len(tok) + 1
            cumulative_lengths.append(current_length)
            
        # 3) í›„ë³´ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        candidates = []
        candidate_indices = []
        
        step_size = 1 if upper <= 10 else 2
        
        for end_i in range(min_tokens-1, upper, step_size):
            cand = " ".join(tgt_tokens[:end_i+1])
            candidates.append(cand)
            candidate_indices.append(end_i)
            
        # 4) ë°°ì¹˜ ì„ë² ë”©
        cand_embs = embed_func(candidates)
        
        # 5) ìµœì  ë§¤ì¹­ íƒìƒ‰
        best_score = -1.0
        best_end_idx = cumulative_lengths[-1]
        
        for i, emb in enumerate(cand_embs):
            score = np.dot(src_emb, emb) / (np.linalg.norm(src_emb) * np.linalg.norm(emb) + 1e-8)
            
            end_i = candidate_indices[i]
            length_ratio = (end_i + 1) / len(tgt_tokens)
            length_penalty = min(1.0, length_ratio * 2)
            
            adjusted_score = score * length_penalty
            
            if adjusted_score > best_score and score >= similarity_threshold:
                best_score = adjusted_score
                best_end_idx = cumulative_lengths[end_i + 1]
                
        return best_end_idx
        
    except Exception as e:
        logger.warning(f"ì˜ë¯¸ ë§¤ì¹­ ì˜¤ë¥˜, ë‹¨ìˆœ ë§¤ì¹­ìœ¼ë¡œ ëŒ€ì²´: {e}")
        return find_target_span_end_simple(src_unit, remaining_tgt)

def split_tgt_by_src_units(src_units: List[str], tgt_text: str) -> List[str]:
    """ì›ë¬¸ ë‹¨ìœ„ì— ë”°ë¥¸ ë²ˆì—­ë¬¸ ë¶„í•  (ë‹¨ìˆœ ë°©ì‹)"""
    results = []
    cursor = 0
    total = len(tgt_text)
    for src_u in src_units:
        remaining = tgt_text[cursor:]
        end_len = find_target_span_end_simple(src_u, remaining)
        chunk = tgt_text[cursor:cursor+end_len]
        results.extend(split_inside_chunk(chunk))
        cursor += end_len
    if cursor < total:
        results.extend(split_inside_chunk(tgt_text[cursor:]))
    return results

def split_tgt_by_src_units_semantic(
    src_units: List[str], 
    tgt_text: str, 
    embed_func: Callable, 
    min_tokens: int = DEFAULT_MIN_TOKENS
) -> List[str]:
    """ì›ë¬¸ ë‹¨ìœ„ì— ë”°ë¥¸ ë²ˆì—­ë¬¸ ë¶„í•  (ì˜ë¯¸ ê¸°ë°˜, ì „ì—­ ë§¤ì¹­)"""
    
    # 1ë‹¨ê³„: ì „ê° ì½œë¡ ì„ í•˜ë“œ ê²½ê³„ë¡œ ì²˜ë¦¬
    if 'ï¼š' in tgt_text:
        colon_parts = tgt_text.split('ï¼š')
        if len(colon_parts) == 2:
            part1 = colon_parts[0].strip() + 'ï¼š'
            part2 = colon_parts[1].strip()
            result = [part1]
            if len(src_units) > 1:
                remaining_src = src_units[1:]
                remaining_parts = split_tgt_by_src_units_semantic(
                    remaining_src, part2, embed_func, min_tokens
                )
                result.extend(remaining_parts)
            else:
                result.append(part2)
            return result
    
    # 2ë‹¨ê³„: ë²ˆì—­ë¬¸ì„ ë¨¼ì € ìì—°ìŠ¤ëŸ¬ìš´ ë‹¨ìœ„ë¡œ ë¶„í• 
    tgt_chunks = split_inside_chunk(tgt_text)
    if not tgt_chunks or len(src_units) == 0:
        return tgt_chunks if tgt_chunks else []
    
    # 3ë‹¨ê³„: ì˜ë¯¸ ê¸°ë°˜ ì „ì—­ ë§¤ì¹­
    if len(src_units) == len(tgt_chunks):
        # 1:1 ëŒ€ì‘ì¸ ê²½ìš° ì˜ë¯¸ ìœ ì‚¬ë„ë¡œ ìµœì  ë§¤ì¹­ ì°¾ê¸°
        return _find_optimal_semantic_matching(src_units, tgt_chunks, embed_func)
    elif len(src_units) == 1:
        # ì›ë¬¸ì´ í•˜ë‚˜ì¸ ê²½ìš° - ë²ˆì—­ë¬¸ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê±°ë‚˜ DP ë§¤ì¹­ ì‚¬ìš©
        if len(tgt_chunks) <= 3:  # ì‘ì€ ê°œìˆ˜ë©´ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
            return [tgt_text.strip()]
        else:
            # ë§ì€ ê°œìˆ˜ë©´ DP ë§¤ì¹­ ì‚¬ìš©
            return _dp_semantic_matching(src_units, tgt_text, embed_func, min_tokens)
    elif len(tgt_chunks) == 1:
        # ë²ˆì—­ë¬¸ì´ í•˜ë‚˜ì¸ ê²½ìš° - ì›ë¬¸ ê°œìˆ˜ë§Œí¼ ë¶„í•  ì‹œë„
        return _split_single_target_to_multiple(src_units, tgt_chunks[0], embed_func)
    else:
        # ê°œìˆ˜ê°€ ë‹¤ë¥¸ ê²½ìš° DP ë§¤ì¹­ ì‚¬ìš©
        return _dp_semantic_matching(src_units, tgt_text, embed_func, min_tokens)

def _find_optimal_semantic_matching(src_units: List[str], tgt_chunks: List[str], embed_func: Callable) -> List[str]:
    """ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ ì²­í¬ ê°„ì˜ ìµœì  ì˜ë¯¸ ë§¤ì¹­ ì°¾ê¸° (ê°œì„ ëœ ë²„ì „)"""
    import itertools
    
    if len(src_units) != len(tgt_chunks):
        return tgt_chunks
    
    try:
        # ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ ì„ë² ë”© ê³„ì‚°
        normalized_src = [_normalize_for_embedding(src) for src in src_units]
        normalized_tgt = [_normalize_for_embedding(tgt) for tgt in tgt_chunks]
        
        src_embs = embed_func(normalized_src)
        tgt_embs = embed_func(normalized_tgt)
        
        # ëª¨ë“  ê°€ëŠ¥í•œ ë§¤ì¹­ì— ëŒ€í•´ ì¢…í•© ìœ ì‚¬ë„ ê³„ì‚°
        best_score = -1
        best_permutation = list(range(len(tgt_chunks)))
        
        for perm in itertools.permutations(range(len(tgt_chunks))):
            total_score = 0
            for i, j in enumerate(perm):
                # 1. ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
                sim = float(np.dot(src_embs[i], tgt_embs[j]) / 
                          (np.linalg.norm(src_embs[i]) * np.linalg.norm(tgt_embs[j]) + 1e-8))
                
                # 2. í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ (í•œì, ê³ ìœ ëª…ì‚¬ ë“±)
                keyword_bonus = _calculate_keyword_bonus(src_units[i], tgt_chunks[j])
                
                # 3. ë¬¸ë²•ì  ê²½ê³„ ë³´ë„ˆìŠ¤
                grammar_bonus = _calculate_grammar_bonus(tgt_chunks[j])
                
                # 4. êµ¬ë¬¸ êµ¬ì¡° ë§¤ì¹­ ë³´ë„ˆìŠ¤
                structure_bonus = _calculate_structure_bonus(src_units[i], tgt_chunks[j])
                
                # 5. ê¸¸ì´ ê· í˜• ë³´ë„ˆìŠ¤ (ë„ˆë¬´ ë¶ˆê· í˜•í•œ ë§¤ì¹­ ë°©ì§€)
                length_bonus = _calculate_length_balance_bonus(src_units[i], tgt_chunks[j])
                
                total_score += (sim * 1.0 + keyword_bonus * 0.8 + grammar_bonus * 0.6 + 
                               structure_bonus * 0.5 + length_bonus * 0.3)
            
            if total_score > best_score:
                best_score = total_score
                best_permutation = perm
        
        # ìµœì  ë§¤ì¹­ ìˆœì„œë¡œ ë°˜í™˜
        return [tgt_chunks[i] for i in best_permutation]
        
    except Exception as e:
        logger.warning(f"ì˜ë¯¸ ë§¤ì¹­ ì‹¤íŒ¨, ì›ë³¸ ìˆœì„œ ìœ ì§€: {e}")
        return tgt_chunks

def _calculate_keyword_bonus(src_unit: str, tgt_chunk: str) -> float:
    """í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ ê³„ì‚° - ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­ ê°œì„  (ìµœì‹  ë²„ì „)"""
    bonus = 0.0
    matches_found = []  # ë””ë²„ê¹…ìš© ë§¤ì¹­ ê¸°ë¡
    
    # 1. í•œì ì¶”ì¶œ (ê°œë³„ ë° ë³µí•©)
    src_hanja = regex.findall(r'\p{Han}+', src_unit)
    exact_hanja_matches = 0
    
    # 2. í•œì ì§ì ‘ ë§¤ì¹­
    for hanja in src_hanja:
        if hanja in tgt_chunk:
            exact_hanja_matches += 1
            bonus += 0.5  # í•œì ì§ì ‘ ë§¤ì¹­ (ì¦ê°€)
            matches_found.append(f"í•œìì§ì ‘:{hanja}")
            if len(hanja) >= 2:
                bonus += 0.3  # ê¸´ í•œìì–´ ë³´ë„ˆìŠ¤ (ì¦ê°€)
    
    # 3. í™•ì¥ëœ í•œì-í•œê¸€ ë§¤ì¹­ ì‚¬ì „ (ê°œë³„ ë° ë³µí•©)
    enhanced_hanja_to_hangul = {
        # í•µì‹¬ ë³µí•©ì–´ (ë†’ì€ ìš°ì„ ìˆœìœ„)
        'æ ¼ç‰©': ['ê²©ë¬¼', 'ì‚¬ë¬¼', 'ì´ì¹˜', 'ì‚¬ë¬¼ì— ì´ë¥¸ë‹¤'],
        'è‡´çŸ¥': ['ì¹˜ì§€', 'ì§€ì‹', 'ì•', 'ì•Œ'],
        'èª æ„': ['ì„±ì˜', 'ì„±ì‹¤', 'ì§„ì‹¤'],
        'æ­£å¿ƒ': ['ì •ì‹¬', 'ë§ˆìŒ', 'ì •ì‹ ', 'ë§ˆìŒì„ ë°”ë¥´ê²Œ'],
        'ä¿®èº«': ['ìˆ˜ì‹ ', 'ëª¸', 'ìˆ˜ì–‘'],
        'é½Šå®¶': ['ì œê°€', 'ê°€ì •', 'ì§‘ì•ˆ'],
        'æ²»åœ‹': ['ì¹˜êµ­', 'ë‚˜ë¼', 'ì •ì¹˜'],
        'å¹³å¤©ä¸‹': ['í‰ì²œí•˜', 'ì²œí•˜', 'ì„¸ìƒ'],
        
        # ê°œë³„ í•œì (ë³µí•©ì–´ ë§¤ì¹­ ì‹¤íŒ¨ì‹œ)
        'æ ¼': ['ê²©', 'ì‚¬ë¬¼', 'ì´ì¹˜'], 'ç‰©': ['ë¬¼', 'ì‚¬ë¬¼', 'ê²ƒ'],
        'è‡´': ['ì¹˜', 'ì´ë¥´', 'ë‹¬'], 'çŸ¥': ['ì§€', 'ì•Œ', 'ì•'],
        'èª ': ['ì„±', 'ì„±ì‹¤', 'ì§„ì‹¤'], 'æ„': ['ì˜', 'ëœ»', 'ë§ˆìŒ'],
        'æ­£': ['ì •', 'ë°”ë¥´', 'ì˜¬ë°”'], 'å¿ƒ': ['ì‹¬', 'ë§ˆìŒ', 'ì •ì‹ '],
        'ä¿®': ['ìˆ˜', 'ë‹¦', 'ìˆ˜ì–‘'], 'èº«': ['ì‹ ', 'ëª¸', 'ìì‹ '],
        'é½Š': ['ì œ', 'ê°€ì§€ëŸ°'], 'å®¶': ['ê°€', 'ì§‘', 'ê°€ì •'],
        'æ²»': ['ì¹˜', 'ë‹¤ìŠ¤', 'ì •ì¹˜'], 'åœ‹': ['êµ­', 'ë‚˜ë¼'],
        'å¹³': ['í‰', 'í‰í‰'], 'å¤©ä¸‹': ['ì²œí•˜', 'ì„¸ìƒ'],
        'è€…': ['ì', 'ê²ƒ', 'ë¼ëŠ” ê²ƒ'], 'ä¹Ÿ': ['ì•¼', 'ì´ë‹¤', 'ë‹¤']
    }
    
    phonetic_matches = 0
    
    # ë³µí•©ì–´ ìš°ì„  ë§¤ì¹­ (ë‹¤ì¤‘ í‚¤ì›Œë“œ ì§€ì›)
    for hanja in src_hanja:
        if hanja in enhanced_hanja_to_hangul:
            keywords = enhanced_hanja_to_hangul[hanja]
            for keyword in keywords:
                if keyword in tgt_chunk:
                    phonetic_matches += 1
                    # ì²« ë²ˆì§¸ í‚¤ì›Œë“œ(ì§ì ‘ ìŒë…)ëŠ” ë†’ì€ ì ìˆ˜, ì˜ë¯¸ì–´ëŠ” ì¤‘ê°„ ì ìˆ˜
                    if keyword == keywords[0]:
                        bonus += 0.4  # ì§ì ‘ ìŒë… (ì¦ê°€)
                    else:
                        bonus += 0.3  # ì˜ë¯¸ ë§¤ì¹­ (ì¦ê°€)
                    matches_found.append(f"ë³µí•©:{hanja}â†’{keyword}")
                    break
    
    # 4. í™•ì¥ëœ ì˜ë¯¸ í‚¤ì›Œë“œ ë§¤ì¹­
    extended_semantic_mappings = {
        # ë¬¸ë§¥ì  ì˜ë¯¸ í™•ì¥ (ë” êµ¬ì²´ì )
        'æ ¼ç‰©': ['ì´ë¥¸ë‹¤', 'ê¶êµ¬', 'ì‚¬ë¬¼ì˜ ì´ì¹˜', 'ì‚¬ë¬¼ì— ì´ë¥¸ë‹¤'],
        'è‡´çŸ¥': ['ì§€ì‹ì„ ì–»', 'ì•ì— ì´ë¥´', 'ì•Œê²Œ'],
        'èª æ„': ['ì„±ì˜ë¥¼ ë‹¤', 'ì§„ì‹¤', 'ì„±ì‹¤', 'ì„±ì˜ë¼ëŠ”'],
        'æ­£å¿ƒ': ['ë§ˆìŒì„ ë°”ë¥´ê²Œ', 'ì •ì‹ ì„ ë°”ë¡œ', 'ë§ˆìŒì„ ë‹¤ìŠ¤', 'ë°”ë¥´ê²Œ í•˜ëŠ”'],
        'ä¿®èº«': ['ëª¸ì„ ë‹¦', 'ìì‹ ì„ ìˆ˜ì–‘', 'ì¸ê²©ì„ ê¸°ë¥´'],
        'é½Šå®¶': ['ê°€ì •ì„ ë‹¤ìŠ¤', 'ì§‘ì•ˆì„ ë°”ë¡œ'],
        'è€…': ['ë¼ëŠ” ê²ƒ', 'ê²ƒ', 'ì', 'í•˜ëŠ” ì‚¬ëŒ', 'ë¼ëŠ” ê²ƒì€'],
        'ä¹Ÿ': ['ê²ƒì´ë‹¤', 'í•˜ëŠ” ê²ƒì´ë‹¤', 'ì´ë‹¤', 'ë‹¤']
    }
    
    semantic_matches = 0
    for hanja in src_hanja:
        if hanja in extended_semantic_mappings:
            for semantic_phrase in extended_semantic_mappings[hanja]:
                if semantic_phrase in tgt_chunk:
                    semantic_matches += 1
                    bonus += 0.35  # í™•ì¥ ì˜ë¯¸ ë§¤ì¹­ (ì¦ê°€)
                    matches_found.append(f"í™•ì¥ì˜ë¯¸:{hanja}â†’{semantic_phrase}")
                    break
    
    # 5. ê°•í™”ëœ ë¬¸ë²•ì  ëŒ€ì‘ (ì¡°ì‚¬/ì–´ë¯¸)
    enhanced_grammar_mappings = {
        'ì€': ['ëŠ”', 'ê²ƒì€', 'ë¼ëŠ” ê²ƒì€', 'ì´ë€', 'ë¼ëŠ”'],
        'ëŠ”': ['ì€', 'ê²ƒì€', 'ë¼ëŠ” ê²ƒì€', 'ì´ë€'],
        'è€…': ['ì', 'ê²ƒ', 'ë¼ëŠ” ê²ƒ', 'í•˜ëŠ” ê²ƒ', 'ë¼ëŠ”', 'ë¼ëŠ” ê²ƒì€'],
        'ä¹Ÿ': ['ì´ë‹¤', 'ë‹¤', 'ê²ƒì´ë‹¤', 'í•˜ëŠ” ê²ƒì´ë‹¤', 'ì¸ ê²ƒì´ë‹¤']
    }
    
    grammar_matches = 0
    for ending, targets in enhanced_grammar_mappings.items():
        if src_unit.endswith(ending) or ending in src_unit:
            for target in targets:
                if target in tgt_chunk:
                    grammar_matches += 1
                    bonus += 0.25  # ë¬¸ë²• ë§¤ì¹­ ì ìˆ˜ (ì¦ê°€)
                    matches_found.append(f"ë¬¸ë²•:{ending}â†’{target}")
                    break
    
    # 6. íŠ¹ìˆ˜ ë³µí•© êµ¬ë¬¸ íŒ¨í„´ ë§¤ì¹­ (í…ŒìŠ¤íŠ¸ì—ì„œ ì„±ê³µí•œ íŒ¨í„´ë“¤)
    special_patterns = [
        # (ì›ë¬¸íŒ¨í„´, ë²ˆì—­íŒ¨í„´, ë³´ë„ˆìŠ¤)
        ('æ ¼ç‰©ì€', 'ì‚¬ë¬¼ì— ì´ë¥¸ë‹¤', 0.6),
        ('æ ¼ç‰©ì€', 'ë¼ëŠ” ê²ƒì€', 0.5),
        ('èª æ„è€…', 'ì„±ì˜ë¼ëŠ”', 0.6),
        ('æ­£å¿ƒä¹Ÿ', 'ë§ˆìŒì„ ë°”ë¥´ê²Œ', 0.6),
        ('æ­£å¿ƒä¹Ÿ', 'í•˜ëŠ” ê²ƒì´ë‹¤', 0.5),
        ('è€…', 'ë¼ëŠ” ê²ƒ', 0.4),
        ('ä¹Ÿ', 'ê²ƒì´ë‹¤', 0.4)
    ]
    
    for src_pattern, tgt_pattern, pattern_bonus in special_patterns:
        if src_pattern in src_unit and tgt_pattern in tgt_chunk:
            bonus += pattern_bonus
            matches_found.append(f"íŠ¹ìˆ˜íŒ¨í„´:{src_pattern}â†’{tgt_pattern}")
    
    # 7. ì¶•ì•½ëœ í˜ë„í‹° ì‹œìŠ¤í…œ (ë” ê´€ëŒ€í•˜ê²Œ)
    penalty = 0.0
    
    # ë§¤ì¹­ì´ ì „í˜€ ì—†ëŠ” ê²½ìš°ë§Œ ì‘ì€ í˜ë„í‹°
    total_matches = exact_hanja_matches + phonetic_matches + semantic_matches + grammar_matches
    if total_matches == 0 and len(src_hanja) > 0:
        penalty += 0.1  # í˜ë„í‹° ì™„í™”
    
    final_bonus = max(0.0, bonus - penalty)
    
    # ë””ë²„ê¹… ì •ë³´ (ì„ íƒì )
    if matches_found and logger.isEnabledFor(logging.DEBUG):
        logger.debug(f"ë§¤ì¹­ '{src_unit}' â†’ '{tgt_chunk}': {matches_found} = {final_bonus:.3f}")
    
    return min(final_bonus, 2.5)  # ìµœëŒ€ê°’ì„ 2.5ë¡œ ì¦ê°€


def _calculate_structure_bonus(src_unit: str, tgt_chunk: str) -> float:
    """êµ¬ë¬¸ êµ¬ì¡° ë§¤ì¹­ ë³´ë„ˆìŠ¤ ê³„ì‚°"""
    bonus = 0.0
    
    # 1. êµ¬ë‘ì  íŒ¨í„´ ë§¤ì¹­
    src_punct = len(re.findall(r'[,ï¼Œ.ã€‚!ï¼?ï¼Ÿ:ï¼š;ï¼›]', src_unit))
    tgt_punct = len(re.findall(r'[,ï¼Œ.ã€‚!ï¼?ï¼Ÿ:ï¼š;ï¼›]', tgt_chunk))
    
    if src_punct == tgt_punct and src_punct > 0:
        bonus += 0.3  # êµ¬ë‘ì  ìˆ˜ê°€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
    
    # 2. ê´„í˜¸ êµ¬ì¡° ë§¤ì¹­
    src_parens = src_unit.count('(') + src_unit.count('ï¼ˆ')
    tgt_parens = tgt_chunk.count('(') + tgt_chunk.count('ï¼ˆ')
    
    if src_parens == tgt_parens and src_parens > 0:
        bonus += 0.2  # ê´„í˜¸ ìˆ˜ê°€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°
    
    # 3. ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ ë§¤ì¹­
    src_ends = any(src_unit.strip().endswith(end) for end in ['ë‹¤', 'ë¼', 'ìš”', 'ë‹ˆ', 'ê¹Œ'])
    tgt_ends = any(tgt_chunk.strip().endswith(end) for end in ['ë‹¤', 'ë¼', 'ìš”', 'ë‹ˆ', 'ê¹Œ'])
    
    if src_ends == tgt_ends:
        bonus += 0.1
    
    return bonus

def _calculate_length_balance_bonus(src_unit: str, tgt_chunk: str) -> float:
    """ê¸¸ì´ ê· í˜• ë³´ë„ˆìŠ¤ ê³„ì‚° (ë„ˆë¬´ ë¶ˆê· í˜•í•œ ë§¤ì¹­ ë°©ì§€)"""
    src_len = len(src_unit.strip())
    tgt_len = len(tgt_chunk.strip())
    
    if src_len == 0 or tgt_len == 0:
        return -0.5  # ë¹ˆ ë¬¸ìì—´ í˜ë„í‹°
    
    # ê¸¸ì´ ë¹„ìœ¨ ê³„ì‚°
    ratio = min(src_len, tgt_len) / max(src_len, tgt_len)
    
    # ì ì ˆí•œ ê¸¸ì´ ë¹„ìœ¨ì— ë³´ë„ˆìŠ¤ (0.3 ~ 1.0 ì‚¬ì´ê°€ ì ì ˆ)
    if ratio >= 0.5:
        return 0.2 * ratio  # ê· í˜• ì¡íŒ ê¸¸ì´ì— ë³´ë„ˆìŠ¤
    elif ratio >= 0.2:
        return 0.1 * ratio  # ì•½ê°„ ë¶ˆê· í˜•í•œ ê²½ìš° ì‘ì€ ë³´ë„ˆìŠ¤
    else:
        return -0.1  # ë„ˆë¬´ ë¶ˆê· í˜•í•œ ê²½ìš° í˜ë„í‹°

def _dp_semantic_matching(src_units: List[str], tgt_text: str, embed_func: Callable, min_tokens: int) -> List[str]:
    """DP ê¸°ë°˜ ì˜ë¯¸ ë§¤ì¹­ (ê¸°ì¡´ ë¡œì§)"""
    # ê¸°ì¡´ DP ë¡œì§ ìœ ì§€ (ë°±ì—…ìš©)
    tgt_tokens = tgt_text.replace('ï¼š', 'ï¼š ').split()
    N, T = len(src_units), len(tgt_tokens)
    if N == 0 or T == 0:
        return []

    dp = np.full((N+1, T+1), -np.inf)
    back = np.zeros((N+1, T+1), dtype=int)
    dp[0, 0] = 0.0

    # ì›ë¬¸ ë‹¨ìœ„ë“¤ì„ ì •ê·œí™”í•˜ì—¬ ì„ë² ë”© ê³„ì‚°
    normalized_src_units = [_normalize_for_embedding(unit) for unit in src_units]
    src_embs = embed_func(normalized_src_units)

    # ëª¨ë“  í›„ë³´ span ìˆ˜ì§‘
    span_map = {}
    all_spans = []
    for i in range(1, N+1):
        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):
            for k in range((i-1)*min_tokens, j-min_tokens+1):
                span = " ".join(tgt_tokens[k:j]).strip()
                key = (k, j)
                if span and key not in span_map:
                    span_map[key] = span
                    all_spans.append(span)
    
    all_spans = list(set(all_spans))

    # ë°°ì¹˜ ì„ë² ë”©
    def batch_embed(spans, batch_size=100):
        results = []
        for i in range(0, len(spans), batch_size):
            batch_spans = spans[i:i+batch_size]
            normalized_batch = [_normalize_for_embedding(span) for span in batch_spans]
            results.extend(embed_func(normalized_batch))
        return results
    
    span_embs = batch_embed(all_spans)
    span_emb_dict = {span: emb for span, emb in zip(all_spans, span_embs)}

    # DP ê³„ì‚° (ê°œì„ ëœ ì˜ë¯¸ ìœ ì‚¬ë„ + ë‹¤ì¤‘ ë³´ë„ˆìŠ¤)
    for i in range(1, N+1):
        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):
            for k in range((i-1)*min_tokens, j-min_tokens+1):
                span = span_map[(k, j)]
                tgt_emb = span_emb_dict[span]
                
                # 1. ê¸°ë³¸ ì˜ë¯¸ ìœ ì‚¬ì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
                sim = float(np.dot(src_embs[i-1], tgt_emb)/((np.linalg.norm(src_embs[i-1])*np.linalg.norm(tgt_emb))+1e-8))
                
                # 2. í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
                keyword_bonus = _calculate_keyword_bonus(src_units[i-1], span)
                
                # 3. ë¬¸ë²•ì  ê²½ê³„ ë³´ë„ˆìŠ¤
                grammar_bonus = _calculate_grammar_bonus(span)
                
                # 4. êµ¬ë¬¸ êµ¬ì¡° ë§¤ì¹­ ë³´ë„ˆìŠ¤
                structure_bonus = _calculate_structure_bonus(src_units[i-1], span)
                
                # 5. ê¸¸ì´ ê· í˜• ë³´ë„ˆìŠ¤
                length_bonus = _calculate_length_balance_bonus(src_units[i-1], span)
                
                # ê°€ì¤‘ì¹˜ ì ìš©í•œ ìµœì¢… ì ìˆ˜
                final_score = (sim * 1.0 + keyword_bonus * 0.8 + grammar_bonus * 0.6 + 
                              structure_bonus * 0.5 + length_bonus * 0.3)
                
                score = dp[i-1, k] + final_score
                
                if score > dp[i, j]:
                    dp[i, j] = score
                    back[i, j] = k

    # ì—­ì¶”ì 
    cuts = [T]
    curr = T
    for i in range(N, 0, -1):
        prev = int(back[i, curr])
        cuts.append(prev)
        curr = prev
    cuts = cuts[::-1]

    tgt_spans = []
    for i in range(N):
        span = " ".join(tgt_tokens[cuts[i]:cuts[i+1]]).strip()
        tgt_spans.append(span)
    return tgt_spans

def split_tgt_meaning_units(
    src_text: str,
    tgt_text: str,
    use_semantic: bool = True,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    embed_func: Callable = None
) -> List[str]:
    """ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
    # ì§€ì—° ì„í¬íŠ¸ë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
    if embed_func is None:
        from sa_embedders import compute_embeddings_with_cache  # ğŸ”§ ìˆ˜ì •
        embed_func = compute_embeddings_with_cache
        
    src_units = split_src_meaning_units(src_text, min_tokens, max_tokens)

    if use_semantic:
        return split_tgt_by_src_units_semantic(
            src_units,
            tgt_text,
            embed_func=embed_func,
            min_tokens=min_tokens
        )
    else:
        return split_tgt_by_src_units(src_units, tgt_text)

def tokenize_text(text):
    """í˜•íƒœì†Œ ë¶„ì„ ë° í† í°í™” - MeCab ì‚¬ìš©"""
    if mecab:
        result = mecab.parse(text)
        tokens = []
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 1:
                    tokens.append(parts[0])
        return tokens
    else:
        return text.split()

def pos_tag_text(text):
    """í’ˆì‚¬ íƒœê¹… - MeCab ì‚¬ìš©"""
    if mecab:
        result = mecab.parse(text)
        pos_tags = []
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    pos = parts[1].split(',')[0]
                    pos_tags.append((surface, pos))
        return pos_tags
    else:
        return [(word, 'UNKNOWN') for word in text.split()]

def sentence_split(text):
    """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
    return [s.strip() for s in sentences if s.strip()]

def normalize_for_embedding(text: str) -> str:
    """ì„ë² ë”© ê³„ì‚°ì„ ìœ„í•´ í…ìŠ¤íŠ¸ ì •ê·œí™” - ì „ê° ì½œë¡  ë“± êµ¬ë‘ì  ì œê±°"""
    # ì „ê° ì½œë¡ ê³¼ ê´„í˜¸ ë“±ì„ ì œê±°í•˜ì—¬ ì˜ë¯¸ ë§¤ì¹­ì— ì§‘ì¤‘
    normalized = text.replace('ï¼š', '').replace('(', '').replace(')', '')
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì •ë¦¬
    normalized = ' '.join(normalized.split())
    return normalized

def _normalize_for_embedding(text: str) -> str:
    """ì„ë² ë”© ê³„ì‚°ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •ê·œí™” - ì „ê° ì½œë¡  ì œê±°"""
    return text.replace('ï¼š', '').strip()

def _calculate_grammar_bonus(span: str) -> float:
    """ë¬¸ë²•ì  ê²½ê³„ì— ëŒ€í•œ ë³´ë„ˆìŠ¤ ì ìˆ˜ ê³„ì‚° - MeCab ê¸°ë°˜ ê°„ì†Œí™” ë²„ì „"""
    span = span.strip()
    bonus = 0.0
    
    # 1. ì „ê° ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš° ê°•í•œ ë³´ë„ˆìŠ¤
    if span.endswith('ï¼š'):
        return 0.8  # ë§¤ìš° ê°•í•œ ë¬¸ë²• ë³´ë„ˆìŠ¤
    
    # 2. MeCabì„ ì´ìš©í•œ ì •í™•í•œ ì–´ë¯¸/ì¡°ì‚¬ ë¶„ì„ì—ë§Œ ì˜ì¡´
    if mecab:
        try:
            result = mecab.parse(span)
            last_pos = None
            for line in result.split('\n'):
                if line and line != 'EOS':
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        pos_detail = parts[1].split(',')
                        last_pos = pos_detail[0]
            
            # í’ˆì‚¬ë³„ ë³´ë„ˆìŠ¤
            if last_pos == 'EF':  # ì¢…ê²°ì–´ë¯¸
                bonus = 0.5
            elif last_pos == 'EC':  # ì—°ê²°ì–´ë¯¸
                bonus = 0.4
            elif last_pos == 'JX':  # ë³´ì¡°ì‚¬
                bonus = 0.4
            elif last_pos in ['JKS', 'JKO', 'JKB', 'JKC']:  # ì£¼ìš” ì¡°ì‚¬
                bonus = 0.3
            elif last_pos in ['ETN', 'ETM']:  # ì „ì„±ì–´ë¯¸
                bonus = 0.3
        except:
            pass  # MeCab ì˜¤ë¥˜ ë¬´ì‹œ
    
    # 3. êµ¬ë‘ì ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš°
    if span.endswith(('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ')):
        bonus = max(bonus, 0.4)
    elif span.endswith((',', 'ï¼Œ', ';', 'ï¼›')):
        bonus = max(bonus, 0.2)
        
    return min(bonus, 1.0)  # ìµœëŒ€ ë³´ë„ˆìŠ¤ ì œí•œ

def _split_single_target_to_multiple(src_units: List[str], single_tgt: str, embed_func: Callable) -> List[str]:
    """ë‹¨ì¼ ë²ˆì—­ë¬¸ì„ ì—¬ëŸ¬ ì›ë¬¸ ë‹¨ìœ„ì— ë§ê²Œ ë¶„í• """
    # ì›ë¬¸ì´ ì—¬ëŸ¬ ê°œì´ê³  ë²ˆì—­ë¬¸ì´ í•˜ë‚˜ì¸ ê²½ìš°
    # ë²ˆì—­ë¬¸ì„ ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„ì—ì„œ ë¶„í• í•˜ì—¬ ì›ë¬¸ ê°œìˆ˜ì— ë§ì¶¤
    
    # ë¨¼ì € ìì—°ìŠ¤ëŸ¬ìš´ ë¶„í•  ì‹œë„
    natural_splits = split_inside_chunk(single_tgt)
    
    if len(natural_splits) >= len(src_units):
        # ìì—° ë¶„í• ì´ ì¶©ë¶„í•œ ê²½ìš°, ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ ì¡°í•© ì°¾ê¸°
        return _merge_splits_to_match_src_count(src_units, natural_splits, embed_func)
    else:
        # ìì—° ë¶„í• ì´ ë¶€ì¡±í•œ ê²½ìš°, ê°•ì œ ë¶„í• 
        return _force_split_by_semantic_boundaries(src_units, single_tgt, embed_func)

def _merge_splits_to_match_src_count(src_units: List[str], tgt_splits: List[str], embed_func: Callable) -> List[str]:
    """ë²ˆì—­ë¬¸ ë¶„í• ì„ ì›ë¬¸ ê°œìˆ˜ì— ë§ê²Œ ë³‘í•©"""
    if len(src_units) >= len(tgt_splits):
        return tgt_splits
    
    # ë„ˆë¬´ ë§ì´ splitëœ ê²½ìš° ì¼ë¶€ë¥¼ ë³‘í•©
    # ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ì¸ì ‘í•œ ë¶„í• ë“¤ì„ ë³‘í•©
    current_splits = tgt_splits[:]
    
    while len(current_splits) > len(src_units):
        # ì¸ì ‘í•œ ë¶„í• ë“¤ ì¤‘ ê°€ì¥ ì í•©í•œ ë³‘í•© í›„ë³´ ì°¾ê¸°
        best_merge_idx = 0
        best_score = -1
        
        for i in range(len(current_splits) - 1):
            merged = current_splits[i] + ' ' + current_splits[i + 1]
            # ë³‘í•©ëœ í…ìŠ¤íŠ¸ê°€ ì–´ë–¤ ì›ë¬¸ê³¼ ê°€ì¥ ì˜ ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸
            best_src_score = 0
            for src in src_units:
                score = _calculate_keyword_bonus(src, merged)
                best_src_score = max(best_src_score, score)
            
            if best_src_score > best_score:
                best_score = best_src_score
                best_merge_idx = i
        
        # ìµœì  ìœ„ì¹˜ì—ì„œ ë³‘í•©
        merged_text = current_splits[best_merge_idx] + ' ' + current_splits[best_merge_idx + 1]
        current_splits = (current_splits[:best_merge_idx] + 
                         [merged_text] + 
                         current_splits[best_merge_idx + 2:])
    
    return current_splits

def _force_split_by_semantic_boundaries(src_units: List[str], single_tgt: str, embed_func: Callable) -> List[str]:
    """ì˜ë¯¸ì  ê²½ê³„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°•ì œ ë¶„í• """
    # ë¬¸ì¥ì„ í† í° ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³  ì›ë¬¸ê³¼ì˜ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²½ê³„ ê²°ì •
    tokens = single_tgt.split()
    if len(tokens) <= len(src_units):
        return [single_tgt]  # í† í°ì´ ë¶€ì¡±í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    # ê° í† í° ìœ„ì¹˜ì—ì„œì˜ ëˆ„ì  í…ìŠ¤íŠ¸ì™€ ì›ë¬¸ë“¤ì˜ ìœ ì‚¬ë„ ê³„ì‚°í•˜ì—¬ ìµœì  ë¶„í• ì  ì°¾ê¸°
    boundaries = [0]
    src_idx = 0
    
    for i in range(1, len(tokens)):
        if src_idx >= len(src_units) - 1:
            break
            
        # í˜„ì¬ê¹Œì§€ì˜ í…ìŠ¤íŠ¸
        current_text = ' '.join(tokens[boundaries[-1]:i+1])
        # ë‹¤ìŒ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°
        next_text = ' '.join(tokens[i+1:min(i+10, len(tokens))])
        
        # í˜„ì¬ ì›ë¬¸ê³¼ì˜ ë§¤ì¹­ ì ìˆ˜
        current_score = _calculate_keyword_bonus(src_units[src_idx], current_text)
        
        # ë‹¤ìŒ ì›ë¬¸ê³¼ì˜ ë§¤ì¹­ ì ìˆ˜ (ìˆë‹¤ë©´)
        next_score = 0
        if src_idx + 1 < len(src_units) and next_text:
            next_score = _calculate_keyword_bonus(src_units[src_idx + 1], next_text)
        
        # ê²½ê³„ ê²°ì •: ë‹¤ìŒ ì›ë¬¸ê³¼ì˜ ë§¤ì¹­ì´ ë” ì¢‹ê³ , í˜„ì¬ í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„íˆ ê¸´ ê²½ìš°
        if (next_score > current_score * 0.7 and 
            len(current_text.strip()) >= 3 and 
            i - boundaries[-1] >= 2):
            boundaries.append(i)
            src_idx += 1
    
    boundaries.append(len(tokens))
    
    # ê²½ê³„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    result = []
    for i in range(len(boundaries) - 1):
        start, end = boundaries[i], boundaries[i + 1]
        if start < end:
            segment = ' '.join(tokens[start:end]).strip()
            if segment:
                result.append(segment)
    
    # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ ê²ƒì„ ë°˜í™˜
    if not result:
        result = [single_tgt]
    
    return result