import logging
import numpy as np
import regex
import re
from typing import List, Callable, Tuple
import jieba
import MeCab
import os

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •ê°’
DEFAULT_MIN_TOKENS = 1
DEFAULT_MAX_TOKENS = 50
DEFAULT_SIMILARITY_THRESHOLD = 0.7

# ì „ì—­ ë³€ìˆ˜ ì„ ì–¸
mecab = None
mecab_initialized = False

def initialize_mecab() -> MeCab.Tagger:
    """MeCab ì‹¤ì œ ì´ˆê¸°í™” ë¡œì§"""
    global mecab
    try:
        logger.debug("âš™ï¸ MeCab ì´ˆê¸°í™” ì‹œë„ ì¤‘...")
        dicdir = "C:/Users/junto/miniconda3/envs/myenv/Lib/site-packages/mecab_ko_dic/dicdir"
        required_files = [
            f"{dicdir}/mecabrc",
            f"{dicdir}/sys.dic",
            f"{dicdir}/matrix.bin",
            f"{dicdir}/user.dic",
        ]
        missing = [p for p in required_files if not os.path.exists(p)]
        if missing:
            logger.warning(f"âš ï¸ MeCab í•„ìˆ˜ íŒŒì¼ ëˆ„ë½: {len(missing)}ê°œ")
            for p in missing:
                logger.warning(f"  - {p}")
            return None
        # ê²½ë¡œ ì •ê·œí™”
        mecabrc_path = required_files[0].replace('\\', '/')
        dicdir_path = dicdir.replace('\\', '/')
        user_dic_path = required_files[3].replace('\\', '/')
        # ì‚¬ìš©ì ì‚¬ì „ ìƒì„±
        if os.path.getsize(user_dic_path) == 0:
            with open(user_dic_path, 'w', encoding='utf-8') as f:
                f.write("# ì‚¬ìš©ì ì‚¬ì „\n")
        # ì´ˆê¸°í™”
        mecab = MeCab.Tagger(f'-r "{mecabrc_path}" -d "{dicdir_path}" -u "{user_dic_path}"')
        # í…ŒìŠ¤íŠ¸: ê°œë³„ í† í° ì¡´ì¬ ì—¬ë¶€ë¡œ í™•ì¸
        test = mecab.parse("ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸")
        # MeCabì´ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ë¯€ë¡œ ê° surfaceë¥¼ ì¶”ì¶œ
        tokens = [line.split('\t')[0] for line in test.split('\n') if line and line != 'EOS']
        if {'ì´ˆê¸°', 'í™”', 'í…ŒìŠ¤íŠ¸'}.issubset(tokens):
            logger.info("âœ… MeCab ì •ìƒ ì´ˆê¸°í™”")
            logger.debug(f"í…ŒìŠ¤íŠ¸ í† í°: {tokens[:5]}")
            return mecab
        else:
            logger.error(f"âŒ MeCab í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨, í† í° ëˆ„ë½: {tokens}")
            return None
    except Exception:
        logger.exception("ğŸ”¥ MeCab ì´ˆê¸°í™” ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜")
        try:
            logger.debug(f"ë””ë ‰í† ë¦¬ ë‚´ìš©: {os.listdir(dicdir)}")
        except Exception:
            pass
        return None

def ensure_mecab_initialized() -> MeCab.Tagger:
    """ì•ˆì „í•œ MeCab ì´ˆê¸°í™” ë³´ì¥"""
    global mecab_initialized, mecab
    if not mecab_initialized:
        mecab = initialize_mecab()
        mecab_initialized = True
    return mecab

# ëª¨ë“ˆ ë¡œë“œ ì‹œ ìë™ ì´ˆê¸°í™”
ensure_mecab_initialized()

# ë¯¸ë¦¬ ì»´íŒŒì¼ëœ ì •ê·œì‹
hanja_re = regex.compile(r'\p{Han}+')
hangul_re = regex.compile(r'^\p{Hangul}+$')

def split_src_meaning_units(
    text: str,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    use_advanced: bool = True
) -> List[str]:
    """í•œë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  - jieba ë¶„ì„ ì°¸ê³ """
    words = text.replace('\n', ' ').replace('ï¼š', 'ï¼š ').split()
    if not words:
        return []
    jieba_tokens = list(jieba.cut(text))
    units = []
    i = 0
    while i < len(words):
        word = words[i]
        if hanja_re.search(word):
            units.append(word)
            i += 1
            continue
        if hangul_re.match(word):
            group = [word]
            j = i + 1
            while j < len(words) and hangul_re.match(words[j]):
                if _should_group_words_by_jieba(group + [words[j]], jieba_tokens):
                    group.append(words[j])
                    j += 1
                else:
                    break
            units.append(' '.join(group))
            i = j
            continue
        units.append(word)
        i += 1
    return units

def _should_group_words_by_jieba(word_group: List[str], jieba_tokens: List[str]) -> bool:
    """jieba ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ì„œ ì–´ì ˆë“¤ì„ ë¬¶ì„ì§€ ê²°ì •"""
    combined = ''.join(word_group)
    for token in jieba_tokens:
        if token.replace(' ', '') == combined.replace(' ', ''):
            return True
    if len(combined) > 10:
        return False
    return len(word_group) <= 3

def split_inside_chunk(chunk: str) -> List[str]:
    """ë²ˆì—­ë¬¸ ì²­í¬ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  - MeCab ë¶„ì„ ì°¸ê³ """
    m = ensure_mecab_initialized()
    if not chunk or not chunk.strip():
        return []
    words = chunk.split()
    morpheme_info = []
    if m:
        result = m.parse(chunk)
        for line in result.split('\n'):
            if line and line != 'EOS':
                surface, pos = line.split('\t')[0], line.split('\t')[1].split(',')[0]
                morpheme_info.append((surface, pos))
    delimiters = ['ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ',
                  'ì™€', 'ê³¼', 'ê³ ', 'ë©°', 'í•˜ê³ ', 'ë•Œ', 'ì˜', 'ë„', 'ë§Œ', 'ë•Œì—', 'ï¼š']
    units = []
    current_group = []
    for word in words:
        current_group.append(word)
        should_break = any(word.endswith(delimiter) for delimiter in delimiters)
        if morpheme_info and not should_break:
            should_break = _should_break_by_mecab(word, morpheme_info)
        if should_break:
            units.append(' '.join(current_group))
            current_group = []
    if current_group:
        units.append(' '.join(current_group))
    return [unit.strip() for unit in units if unit.strip()]

def _should_break_by_mecab(word: str, morpheme_info: List[tuple]) -> bool:
    """MeCab í’ˆì‚¬ë¥¼ ì°¸ê³ í•œ ê²½ê³„ ê²°ì •"""
    for surface, pos in morpheme_info:
        if surface in word:
            if pos in ['JKS', 'JKO', 'JKC', 'JX', 'EF', 'EC', 'ETN', 'SF', 'SP']:
                return True
            if pos in ['VV', 'VA', 'VX']:
                return True
    return False

def find_target_span_end_simple(src_unit: str, remaining_tgt: str) -> int:
    """ê°„ë‹¨í•œ íƒ€ê²Ÿ ìŠ¤íŒ¬ íƒìƒ‰"""
    hanja_chars = regex.findall(r'\p{Han}+', src_unit)
    if not hanja_chars:
        return 0
    last = hanja_chars[-1]
    idx = remaining_tgt.rfind(last)
    if idx == -1:
        return len(remaining_tgt)
    end = idx + len(last)
    next_space = remaining_tgt.find(' ', end)
    return next_space + 1 if next_space != -1 else len(remaining_tgt)

def find_target_span_end_semantic(
    src_unit: str,
    remaining_tgt: str,
    embed_func: Callable,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD
) -> int:
    """ìµœì í™”ëœ íƒ€ê²Ÿ ìŠ¤íŒ¬ íƒìƒ‰"""
    if not src_unit or not remaining_tgt:
        return 0
    try:
        src_emb = embed_func([src_unit])[0]
        tgt_tokens = remaining_tgt.split()
        if not tgt_tokens:
            return 0
        upper = min(len(tgt_tokens), max_tokens)
        cumulative_lengths = [0]
        current_length = 0
        for tok in tgt_tokens:
            current_length += len(tok) + 1
            cumulative_lengths.append(current_length)
        candidates, candidate_indices = [], []
        step_size = 1 if upper <= 10 else 2
        for end_i in range(min_tokens-1, upper, step_size):
            candidates.append(" ".join(tgt_tokens[:end_i+1]))
            candidate_indices.append(end_i)
        cand_embs = embed_func(candidates)
        best_score, best_end_idx = -1.0, cumulative_lengths[-1]
        for i, emb in enumerate(cand_embs):
            score = np.dot(src_emb, emb) / (np.linalg.norm(src_emb)*np.linalg.norm(emb) + 1e-8)
            end_i = candidate_indices[i]
            length_ratio = (end_i+1)/len(tgt_tokens)
            length_penalty = min(1.0, length_ratio*2)
            adjusted = score * length_penalty
            if adjusted > best_score and score >= similarity_threshold:
                best_score, best_end_idx = adjusted, cumulative_lengths[end_i+1]
        return best_end_idx
    except Exception as e:
        logger.warning(f"ì˜ë¯¸ ë§¤ì¹­ ì˜¤ë¥˜, ë‹¨ìˆœ ë§¤ì¹­ìœ¼ë¡œ ëŒ€ì²´: {e}")
        return find_target_span_end_simple(src_unit, remaining_tgt)

def split_tgt_by_src_units(src_units: List[str], tgt_text: str) -> List[str]:
    """ë‹¨ìˆœ ì›ë¬¸-ë²ˆì—­ ì—°ê²° ë°©ì‹"""
    results, cursor, total = [], 0, len(tgt_text)
    for src_u in src_units:
        remaining = tgt_text[cursor:]
        end_len = find_target_span_end_simple(src_u, remaining)
        chunk = tgt_text[cursor:cursor+end_len]
        results.extend(split_inside_chunk(chunk))
        cursor += end_len
    if cursor < total:
        results.extend(split_inside_chunk(tgt_text[cursor:]))
    return results

def split_tgt_by_src_units_semantic(
    src_units: List[str], tgt_text: str, embed_func: Callable, min_tokens: int = DEFAULT_MIN_TOKENS
) -> List[str]:
    """ì˜ë¯¸ ê¸°ë°˜ DP ë¶„í• """
    tgt_tokens, N, T = tgt_text.split(), len(src_units), len(tgt_text.split())
    if N==0 or T==0: return []
    dp = np.full((N+1,T+1), -np.inf); back = np.zeros((N+1,T+1),dtype=int); dp[0,0]=0.0
    src_embs = embed_func(src_units)
    span_map, all_spans = {}, []
    for i in range(1,N+1):
        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):
            for k in range((i-1)*min_tokens, j-min_tokens+1):
                span = " ".join(tgt_tokens[k:j]).strip()
                if span and (k,j) not in span_map:
                    span_map[(k,j)] = span; all_spans.append(span)
    all_spans = list(set(all_spans))
    def batch_embed(spans,bs=100):
        out=[]
        for s in range(0,len(spans),bs): out.extend(embed_func(spans[s:s+bs]))
        return out
    span_embs = batch_embed(all_spans)
    emb_dict = {sp:em for sp,em in zip(all_spans,span_embs)}
    for i in range(1,N+1):
        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):
            for k in range((i-1)*min_tokens, j-min_tokens+1):
                span = span_map[(k,j)]; sim = float(np.dot(src_embs[i-1],emb_dict[span])/(np.linalg.norm(src_embs[i-1])*np.linalg.norm(emb_dict[span])+1e-8))
                score = dp[i-1,k]+sim
                if score>dp[i,j]: dp[i,j]=score; back[i,j]=k
    cuts, curr = [T], T
    for i in range(N,0,-1): prev=back[i,curr]; cuts.append(prev); curr=prev
    cuts=cuts[::-1]
    assert cuts[0]==0 and cuts[-1]==T and len(cuts)==N+1
    return [" ".join(tgt_tokens[cuts[i]:cuts[i+1]]).strip() for i in range(N)]

def split_tgt_meaning_units(
    src_text: str,
    tgt_text: str,
    use_semantic: bool = True,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    embed_func: Callable = None
) -> List[str]:
    """í†µí•© ë¶„í•  ì¸í„°í˜ì´ìŠ¤"""
    if embed_func is None:
        from sa_embedders import compute_embeddings_with_cache
        embed_func = compute_embeddings_with_cache
    src_units = split_src_meaning_units(src_text,min_tokens,max_tokens)
    return (split_tgt_by_src_units_semantic(src_units,tgt_text,embed_func,min_tokens)
            if use_semantic else split_tgt_by_src_units(src_units,tgt_text))

def tokenize_text(text: str) -> List[str]:
    """í˜•íƒœì†Œ ë¶„ì„ ë° í† í°í™”"""
    m = ensure_mecab_initialized()
    if m:
        return [line.split('\t')[0] for line in m.parse(text).split('\n') if line and line!='EOS']
    return text.split()

def pos_tag_text(text: str) -> List[Tuple[str, str]]:
    """í’ˆì‚¬ íƒœê¹…"""
    m = ensure_mecab_initialized()
    if m:
        return [(line.split('\t')[0], line.split('\t')[1].split(',')[0])
                for line in m.parse(text).split('\n') if line and line!='EOS']
    return [(w,'UNKNOWN') for w in text.split()]

def sentence_split(text: str) -> List[str]:
    """ë¬¸ì¥ ë‹¨ìœ„ ë¶„ë¦¬"""
    return [s.strip() for s in re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text) if s.strip()]
