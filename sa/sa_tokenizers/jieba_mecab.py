"""ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ëª¨ë“ˆ - jieba & MeCab í™œìš©"""

import logging
import numpy as np
import regex
import re
from typing import List, Callable
import jieba
import MeCab
import os

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •ê°’
DEFAULT_MIN_TOKENS = 1
DEFAULT_MAX_TOKENS = 50
DEFAULT_SIMILARITY_THRESHOLD = 0.7

# jiebaì™€ MeCab ì´ˆê¸°í™”
try:
    # ì‚¬ìš©ì ì‚¬ì „ ê²½ë¡œë¥¼ .venv/Scripts/user.dicë¡œ ì§€ì •
    mecabrc_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir/mecabrc'
    dicdir_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir'
    userdic_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir/user.dic'
    mecab = MeCab.Tagger(f'-r {mecabrc_path} -d {dicdir_path} -u {userdic_path}')
    print("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ")
    logger.info("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ") # -dëŠ” ì‚¬ì „ ë””ë ‰í† ë¦¬, -uëŠ” ì‚¬ìš©ì ì‚¬ì „ ê²½ë¡œ
except Exception as e:
    print(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    logger.warning(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    mecab = None

# ë¯¸ë¦¬ ì»´íŒŒì¼ëœ ì •ê·œì‹
hanja_re = regex.compile(r'\p{Han}+')
hangul_re = regex.compile(r'^\p{Hangul}+$')

def split_src_meaning_units(
    text: str,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    by_space: bool = False,
    **kwargs
):
    """ì›ë¬¸(í•œë¬¸)ì€ ë¬´ì¡°ê±´ jiebaë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  (tokenizer ì¸ì ë¬´ì‹œ)"""
    # tokenizer ì¸ì ë¬´ì‹œ, ë¬´ì¡°ê±´ jieba ì‚¬ìš©
    
    # 1ë‹¨ê³„: ì–´ì ˆ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ì–´ì ˆ ë‚´ë¶€ëŠ” ì ˆëŒ€ ìª¼ê°œì§€ì§€ ì•ŠìŒ)
    words = text.replace('\n', ' ').replace('ï¼š', 'ï¼š ').split()
    if not words:
        return []
    
    # 2ë‹¨ê³„: jieba ë¶„ì„ ê²°ê³¼ ì°¸ê³ 
    jieba_tokens = list(jieba.cut(text))
    
    # 3ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì–´ì ˆë“¤ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
    units = []
    i = 0
    
    while i < len(words):
        word = words[i]
        
        # í•œì+ì¡°ì‚¬ íŒ¨í„´ (ë””í´íŠ¸ë¡œ í•­ìƒ ìˆ˜í–‰)
        if hanja_re.search(word):
            # í˜„ì¬ ì–´ì ˆì´ í•œìë¥¼ í¬í•¨í•˜ë©´ í•˜ë‚˜ì˜ ì˜ë¯¸ ë‹¨ìœ„
            units.append(word)
            i += 1
            continue
        
        # í•œê¸€ ì–´ì ˆë“¤ ì²˜ë¦¬ - jieba ë¶„ì„ ê²°ê³¼ ì°¸ê³ 
        if hangul_re.match(word):
            # jiebaê°€ ì œì•ˆí•˜ëŠ” ê²½ê³„ë¥¼ ì°¸ê³ í•´ì„œ ì˜ë¯¸ ë‹¨ìœ„ ê²°ì •
            group = [word]
            j = i + 1
            
            # ë‹¤ìŒ ì–´ì ˆë“¤ê³¼ ë¬¶ì„ì§€ jieba ê²°ê³¼ ì°¸ê³ í•´ì„œ ê²°ì •
            while j < len(words) and hangul_re.match(words[j]):
                # jieba í† í°ì—ì„œ ì—°ì†ì„± í™•ì¸
                should_group = _should_group_words_by_jieba(group + [words[j]], jieba_tokens)
                if should_group:
                    group.append(words[j])
                    j += 1
                else:
                    break
            
            units.append(' '.join(group))
            i = j
            continue
        
        # ê¸°íƒ€ ì–´ì ˆ (ìˆ«ì, êµ¬ë‘ì  ë“±)
        units.append(word)
        i += 1
    
    return units

def _should_group_words_by_jieba(word_group: List[str], jieba_tokens: List[str]) -> bool:
    """jieba ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ì„œ ì–´ì ˆë“¤ì„ ë¬¶ì„ì§€ ê²°ì •"""
    combined = ''.join(word_group)
    
    # jieba í† í° ì¤‘ì—ì„œ í˜„ì¬ ì¡°í•©ê³¼ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ìˆìœ¼ë©´ ë¬¶ê¸°
    for token in jieba_tokens:
        if token.replace(' ', '') == combined.replace(' ', ''):
            return True
    
    # ê¸¸ì´ ì œí•œ
    if len(combined) > 10:
        return False
    
    return len(word_group) <= 3

def split_inside_chunk(chunk: str) -> List[str]:
    """ë²ˆì—­ë¬¸ ì²­í¬ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  - MeCab ë¶„ì„ ì°¸ê³ """
    
    if not chunk or not chunk.strip():
        return []
    
    # 1ë‹¨ê³„: ì–´ì ˆ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ì–´ì ˆ ë‚´ë¶€ëŠ” ì ˆëŒ€ ìª¼ê°œì§€ì§€ ì•ŠìŒ)
    words = chunk.split()
    if not words:
        return []
    
    # 2ë‹¨ê³„: MeCab ë¶„ì„ ê²°ê³¼ ì°¸ê³ 
    morpheme_info = []
    if mecab:
        result = mecab.parse(chunk)
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    pos = parts[1].split(',')[0]
                    morpheme_info.append((surface, pos))
    
    # 3ë‹¨ê³„: ê¸°ë³¸ íŒ¨í„´ìœ¼ë¡œ ì–´ì ˆë“¤ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
    delimiters = ['ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ìœ¼ë‹ˆ', 'ì´ë‹ˆ', 'í•˜ë‹ˆ', 'ë˜ë‹ˆ', 'ëŠ”ë°', 'ì¸ë°',
                  'ì™€', 'ê³¼', 'ë©°', 'í•˜ê³ ', 'ì´ê³ ', 'ë•Œ', 'ì˜', 'ë„', 'ë§Œ', 'ë•Œì—', 'ê²ƒì€', 'ì´ëŠ”', 'ì´ë©´', 'í•˜ë©´', 'ë˜ë©´', 'ìœ¼ë©´', 'ï¼š']
    
    units = []
    current_group = []
    
    for word in words:
        current_group.append(word)
        
        # ê¸°ë³¸ íŒ¨í„´: ì¡°ì‚¬/ì–´ë¯¸ë¡œ ëë‚˜ë©´ ì˜ë¯¸ ë‹¨ìœ„ ì™„ì„±
        should_break = any(word.endswith(delimiter) for delimiter in delimiters)
        
        # MeCab ë¶„ì„ ê²°ê³¼ ì°¸ê³ : í’ˆì‚¬ ì •ë³´ë¡œ ê²½ê³„ ì¡°ì •
        if morpheme_info and not should_break:
            should_break = _should_break_by_mecab(word, morpheme_info)
        
        if should_break and current_group:
            units.append(' '.join(current_group))
            current_group = []
    
    if current_group:
        units.append(' '.join(current_group))
    
    return [unit.strip() for unit in units if unit.strip()]

def _should_break_by_mecab(word: str, morpheme_info: List[tuple]) -> bool:
    """MeCab ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ì„œ ì˜ë¯¸ ë‹¨ìœ„ ê²½ê³„ ê²°ì •"""
    
    # wordì— í•´ë‹¹í•˜ëŠ” í˜•íƒœì†Œë“¤ì˜ í’ˆì‚¬ í™•ì¸
    for surface, pos in morpheme_info:
        if surface in word:
            # ì¡°ì‚¬, ì–´ë¯¸, êµ¬ë‘ì ì—ì„œ ê²½ê³„
            if pos in ['JKS', 'JKO', 'JKC', 'JX', 'EF', 'EC', 'ETN', 'SF', 'SP']:
                return True
            # ë™ì‚¬, í˜•ìš©ì‚¬ ì–´ê°„ ë‹¤ìŒì—ì„œ ê²½ê³„  
            if pos in ['VV', 'VA', 'VX']:
                return True
    
    return False

def find_target_span_end_simple(src_unit: str, remaining_tgt: str) -> int:
    """ê°„ë‹¨í•œ íƒ€ê²Ÿ ìŠ¤íŒ¬ íƒìƒ‰"""
    hanja_chars = regex.findall(r'\p{Han}+', src_unit)
    if not hanja_chars:
        return 0
    last = hanja_chars[-1]
    idx = remaining_tgt.rfind(last)
    if idx == -1:
        return len(remaining_tgt)
    end = idx + len(last)
    next_space = remaining_tgt.find(' ', end)
    return next_space + 1 if next_space != -1 else len(remaining_tgt)

def find_target_span_end_semantic(
    src_unit: str,
    remaining_tgt: str,
    embed_func: Callable,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD
) -> int:
    """ìµœì í™”ëœ íƒ€ê²Ÿ ìŠ¤íŒ¬ íƒìƒ‰ í•¨ìˆ˜"""
    if not src_unit or not remaining_tgt:
        return 0
        
    try:
        # 1) ì›ë¬¸ ì„ë² ë”© (ë‹¨ì¼ ê³„ì‚°)
        src_emb = embed_func([src_unit])[0]
        
        # 2) ë²ˆì—­ë¬¸ í† í° ë¶„ë¦¬ ë° ëˆ„ì  ê¸¸ì´ ê³„ì‚°
        tgt_tokens = remaining_tgt.split()
        if not tgt_tokens:
            return 0
            
        upper = min(len(tgt_tokens), max_tokens)
        cumulative_lengths = [0]
        current_length = 0
        
        for tok in tgt_tokens:
            current_length += len(tok) + 1
            cumulative_lengths.append(current_length)
            
        # 3) í›„ë³´ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        candidates = []
        candidate_indices = []
        
        step_size = 1 if upper <= 10 else 2
        
        for end_i in range(min_tokens-1, upper, step_size):
            cand = " ".join(tgt_tokens[:end_i+1])
            candidates.append(cand)
            candidate_indices.append(end_i)
            
        # 4) ë°°ì¹˜ ì„ë² ë”©
        cand_embs = embed_func(candidates)
        
        # 5) ìµœì  ë§¤ì¹­ íƒìƒ‰
        best_score = -1.0
        best_end_idx = cumulative_lengths[-1]
        
        for i, emb in enumerate(cand_embs):
            score = np.dot(src_emb, emb) / (np.linalg.norm(src_emb) * np.linalg.norm(emb) + 1e-8)
            
            end_i = candidate_indices[i]
            length_ratio = (end_i + 1) / len(tgt_tokens)
            length_penalty = min(1.0, length_ratio * 2)
            
            adjusted_score = score * length_penalty
            
            if adjusted_score > best_score and score >= similarity_threshold:
                best_score = adjusted_score
                best_end_idx = cumulative_lengths[end_i + 1]
                
        return best_end_idx
        
    except Exception as e:
        logger.warning(f"ì˜ë¯¸ ë§¤ì¹­ ì˜¤ë¥˜, ë‹¨ìˆœ ë§¤ì¹­ìœ¼ë¡œ ëŒ€ì²´: {e}")
        return find_target_span_end_simple(src_unit, remaining_tgt)

def split_tgt_by_src_units(src_units: List[str], tgt_text: str) -> List[str]:
    """ì›ë¬¸ ë‹¨ìœ„ì— ë”°ë¥¸ ë²ˆì—­ë¬¸ ë¶„í•  (ë‹¨ìˆœ ë°©ì‹)"""
    results = []
    cursor = 0
    total = len(tgt_text)
    for src_u in src_units:
        remaining = tgt_text[cursor:]
        end_len = find_target_span_end_simple(src_u, remaining)
        chunk = tgt_text[cursor:cursor+end_len]
        results.extend(split_inside_chunk(chunk))
        cursor += end_len
    if cursor < total:
        results.extend(split_inside_chunk(tgt_text[cursor:]))
    return results

def split_tgt_by_src_units_semantic(
    src_units: List[str], 
    tgt_text: str, 
    embed_func: Callable, 
    min_tokens: int = DEFAULT_MIN_TOKENS
) -> List[str]:
    """ì›ë¬¸ ë‹¨ìœ„ì— ë”°ë¥¸ ë²ˆì—­ë¬¸ ë¶„í•  (ì˜ë¯¸ ê¸°ë°˜, ì„ë² ë”© batch ì²˜ë¦¬)"""
    tgt_tokens = tgt_text.split()
    N, T = len(src_units), len(tgt_tokens)
    if N == 0 or T == 0:
        return []

    dp = np.full((N+1, T+1), -np.inf)
    back = np.zeros((N+1, T+1), dtype=int)
    dp[0, 0] = 0.0

    src_embs = embed_func(src_units)

    # 1. ëª¨ë“  í›„ë³´ spanì„ ë¯¸ë¦¬ ìˆ˜ì§‘ (strip, ì¤‘ë³µ ì™„ì „ ì œê±°)
    span_map = {}  # (k, j) -> span string
    all_spans = []
    for i in range(1, N+1):
        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):
            for k in range((i-1)*min_tokens, j-min_tokens+1):
                span = " ".join(tgt_tokens[k:j]).strip()
                key = (k, j)
                if span and key not in span_map:
                    span_map[key] = span
                    all_spans.append(span)
    # í•„ìš”ì‹œ ì™„ì „ ì¤‘ë³µ ì œê±°
    all_spans = list(set(all_spans))

    # 2. batch 100ê°œ ì œí•œ ì²˜ë¦¬ (embed_funcê°€ ë‚´ë¶€ì ìœ¼ë¡œ ì§€ì›í•˜ì§€ ì•Šìœ¼ë©´)
    def batch_embed(spans, batch_size=100):
        results = []
        for i in range(0, len(spans), batch_size):
            results.extend(embed_func(spans[i:i+batch_size]))
        return results
    span_embs = batch_embed(all_spans)

    span_emb_dict = {span: emb for span, emb in zip(all_spans, span_embs)}

    # 3. DP ê³„ì‚° (ì„ë² ë”© ì¬ì‚¬ìš©)
    for i in range(1, N+1):
        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):
            for k in range((i-1)*min_tokens, j-min_tokens+1):
                span = span_map[(k, j)]
                tgt_emb = span_emb_dict[span]
                sim = float(np.dot(src_embs[i-1], tgt_emb)/((np.linalg.norm(src_embs[i-1])*np.linalg.norm(tgt_emb))+1e-8))
                score = dp[i-1, k] + sim
                if score > dp[i, j]:
                    dp[i, j] = score
                    back[i, j] = k

    cuts = [T]
    curr = T
    for i in range(N, 0, -1):
        prev = int(back[i, curr])
        cuts.append(prev)
        curr = prev
    cuts = cuts[::-1]
    assert cuts[0] == 0 and cuts[-1] == T and len(cuts) == N + 1

    tgt_spans = []
    for i in range(N):
        span = " ".join(tgt_tokens[cuts[i]:cuts[i+1]]).strip()
        tgt_spans.append(span)
    return tgt_spans

def split_tgt_meaning_units(
    src_text: str,
    tgt_text: str,
    use_semantic: bool = True,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    embed_func: Callable = None
) -> List[str]:
    """ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
    # ì§€ì—° ì„í¬íŠ¸ë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
    if embed_func is None:
        from sa_embedders import compute_embeddings_with_cache  # ğŸ”§ ìˆ˜ì •
        embed_func = compute_embeddings_with_cache
        
    src_units = split_src_meaning_units(src_text, min_tokens, max_tokens)

    if use_semantic:
        return split_tgt_by_src_units_semantic(
            src_units,
            tgt_text,
            embed_func=embed_func,
            min_tokens=min_tokens
        )
    else:
        return split_tgt_by_src_units(src_units, tgt_text)

def tokenize_text(text):
    """í˜•íƒœì†Œ ë¶„ì„ ë° í† í°í™” - MeCab ì‚¬ìš©"""
    if mecab:
        result = mecab.parse(text)
        tokens = []
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 1:
                    tokens.append(parts[0])
        return tokens
    else:
        return text.split()

def pos_tag_text(text):
    """í’ˆì‚¬ íƒœê¹… - MeCab ì‚¬ìš©"""
    if mecab:
        result = mecab.parse(text)
        pos_tags = []
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    pos = parts[1].split(',')[0]
                    pos_tags.append((surface, pos))
        return pos_tags
    else:
        return [(word, 'UNKNOWN') for word in text.split()]

def sentence_split(text):
    """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
    return [s.strip() for s in sentences if s.strip()]