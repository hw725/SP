"""ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ëª¨ë“ˆ - jieba & MeCab í™œìš©"""

import logging
import numpy as np
import regex
import re
import itertools
from typing import List, Callable
import jieba
import MeCab
import os

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •ê°’
DEFAULT_MIN_TOKENS = 1
DEFAULT_MAX_TOKENS = 50
DEFAULT_SIMILARITY_THRESHOLD = 0.7

# jiebaì™€ MeCab ì´ˆê¸°í™”
try:
    # ì‚¬ìš©ì ì‚¬ì „ ê²½ë¡œë¥¼ .venv/Scripts/user.dicë¡œ ì§€ì •
    mecabrc_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir/mecabrc'
    dicdir_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir'
    userdic_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir/user.dic'
    mecab = MeCab.Tagger(f'-r {mecabrc_path} -d {dicdir_path} -u {userdic_path}')
    print("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ")
    logger.info("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ") # -dëŠ” ì‚¬ì „ ë””ë ‰í† ë¦¬, -uëŠ” ì‚¬ìš©ì ì‚¬ì „ ê²½ë¡œ
except Exception as e:
    print(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    logger.warning(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    mecab = None

# ë¯¸ë¦¬ ì»´íŒŒì¼ëœ ì •ê·œì‹
hanja_re = regex.compile(r'\p{Han}+')
hangul_re = regex.compile(r'^\p{Hangul}+$')

# ë¬¸ë²•ì  ê²½ê³„ í‘œì§€ (ê°„ê²°í•œ ì •ì˜)
BOUNDARY_MARKERS = {
    # ì¤‘ì„¸êµ­ì–´ ì–´ë¯¸ (ì›ë¬¸ ì „ìš© - í‘œì  ê¸°ëŠ¥)
    'í˜¸ë˜': ['boundary', 'quotative', 'split_after', 'src_only'],    # ì¸ìš© í‘œì§€ (ì›ë¬¸ í‘œì  ê¸°ëŠ¥)
    'ë¼': ['boundary', 'imperative', 'src_only'],                    # ëª…ë ¹/ì²­ìœ  í‘œì§€ (ì›ë¬¸ ì „ìš©)
    'ë‹ˆ': ['boundary', 'causal', 'src_only'],                        # ì›ì¸/ì´ìœ  í‘œì§€ (ì›ë¬¸ ì „ìš©)
    'ë˜': ['boundary', 'adversative', 'src_only'],                   # ëŒ€ë¦½/ì „í™˜ í‘œì§€ (ì›ë¬¸ ì „ìš©)
    'ë‹ˆë¼': ['boundary', 'declarative', 'src_only'],                 # ì„œìˆ  í‘œì§€ (ì›ë¬¸ ì „ìš©)
    'ë‚˜': ['boundary', 'interrogative', 'src_only'],                 # ì˜ë¬¸ í‘œì§€ (ì›ë¬¸ ì „ìš©)
    
    # í˜„ëŒ€ì–´ ì—°ê²° í‘œì§€ (ë²ˆì—­ë¬¸ì—ì„œ ì‚¬ìš©)
    'ë¯€ë¡œ': ['boundary', 'causal'],                                  # ì¸ê³¼ ê´€ê³„
    'ì„œ': ['boundary', 'causal'],                                    # ì¸ê³¼ ê´€ê³„ (ë‹¨ì¶•)
    'ë©´ì„œ': ['boundary', 'simultaneous'],                            # ë™ì‹œ ê´€ê³„
}

# ì½¤ë§ˆ ê²½ê³„ íŒ¨í„´ (ë³‘ë ¬ ì œì™¸)
COMMA_BOUNDARY_PATTERNS = [
    r'(?:í•˜ê³ |í•˜ë©°|í•˜ë©´ì„œ),',          # ìˆœì°¨ì  ì—°ê²°
    r'(?:ë¯€ë¡œ|ì„œ|ë‹ˆ),',                # ì¸ê³¼ ê´€ê³„  
    r'(?:í›„|ì „|ë•Œ),',                  # ì‹œê°„ ê´€ê³„
    r'(?:ë©´|ê±°ë“ |ë©´ì„œ)',               # ì¡°ê±´/ë™ì‹œ ê´€ê³„
]

# ì• êµ¬ì— ë¶™ì–´ì•¼ í•˜ëŠ” í‘œí˜„ë“¤ (ìƒˆë¡œìš´ êµ¬ë¡œ ì‹œì‘í•˜ë©´ ì•ˆ ë¨)
ATTACH_TO_PREVIOUS_PATTERNS = [
    # ì¸ìš© í‘œì§€ (ë¼ê³  + ìš©ì–¸)
    r'^ë¼ê³ \s+(?:í•˜[ë‹¤ì‹œë©°ë©´ì—ˆì„]|ë§í•˜[ë‹¤ì‹œë©°ë©´ì•˜ì„]|ìƒê°í•˜[ë‹¤ì‹œë©°ë©´ì—ˆì„])',  # ë¼ê³  í•˜ë‹¤/ë§í•˜ë‹¤/ìƒê°í•˜ë‹¤
    r'^ë¼ê³ \s+(?:í•œë‹¤|í–ˆë‹¤|í•˜ë©´|í•˜ë©°|í•˜ëŠ”|í• )',                      # ë¼ê³  + ìš©ì–¸ í™œìš©
    
    # ê´€í˜• í‘œì§€ (ë¼ëŠ”/ë¼ë˜ + ëª…ì‚¬êµ¬)
    r'^ë¼ëŠ”\s+(?:ê²ƒì€|ì‚¬ì‹¤ì€|ë§ì€|ì ì€|ë¶€ë¶„ì€)',                     # ë¼ëŠ” ê²ƒì€/ì‚¬ì‹¤ì€/ë§ì€ ë“±
    r'^ë¼ë˜\s+(?:ê²ƒì€|ë§ì€|ì ì€)',                                  # ë¼ë˜ ê²ƒì€/ë§ì€ ë“±
    
    # ê¸°íƒ€ ì—°ê²° í‘œí˜„
    r'^ë¼ë©°\s+',                                                   # ë¼ë©°
    r'^ë¼ë©´ì„œ\s+',                                                 # ë¼ë©´ì„œ
    r'^ë¼ê³ ë„\s+',                                                 # ë¼ê³ ë„
    
    # ê²©ì¡°ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” í‘œí˜„ë“¤
    r'^(?:ì´ë¼|ë¼)ëŠ”\s+',                                          # ì´ë¼ëŠ”/ë¼ëŠ”
    r'^(?:ì´ë¼|ë¼)ê³ \s+',                                          # ì´ë¼ê³ /ë¼ê³ 
]

# ì¤‘ì„¸êµ­ì–´ ì–´ë¯¸ ìƒì„¸ ì •ì˜ (detect_middle_korean_ending í•¨ìˆ˜ìš©)
MIDDLE_KOREAN_ENDINGS = {
    'í˜¸ë˜': {'type': 'connective', 'meaning': 'ì—­ì ‘/ì¸ìš©', 'split_after': True},
    'ë¼': {'type': 'imperative', 'meaning': 'ëª…ë ¹/ì²­ìœ ', 'split_after': True},
    'ë‹ˆ': {'type': 'connective', 'meaning': 'ì¸ê³¼', 'split_after': True},
    'ë˜': {'type': 'connective', 'meaning': 'ì—­ì ‘', 'split_after': True},
    'ë‹ˆë¼': {'type': 'declarative', 'meaning': 'ì„œìˆ ', 'split_after': True},
    'ë‚˜': {'type': 'interrogative', 'meaning': 'ì˜ë¬¸', 'split_after': True},
    
    # í˜„ëŒ€ì–´ ì—°ê²° í‘œì§€ë„ í¬í•¨
    'ë¯€ë¡œ': {'type': 'connective', 'meaning': 'ì¸ê³¼', 'split_after': True},
    'ì„œ': {'type': 'connective', 'meaning': 'ì¸ê³¼', 'split_after': True},
    'ë©´ì„œ': {'type': 'connective', 'meaning': 'ë™ì‹œ', 'split_after': True},
    'ë©´': {'type': 'connective', 'meaning': 'ì¡°ê±´', 'split_after': False},
    'ì•¼': {'type': 'connective', 'meaning': 'ì—°ì†', 'split_after': False},
    
    # ì¡°ì‚¬ (í•œë¬¸ ë’¤ì— ë¶™ëŠ” ê²½ìš°)
    'ì€': {'type': 'particle', 'meaning': 'ì£¼ì œ/ëŒ€ì¡°', 'split_after': False},
    'ëŠ”': {'type': 'particle', 'meaning': 'ì£¼ì œ/ëŒ€ì¡°', 'split_after': False},
    'ì´': {'type': 'particle', 'meaning': 'ì£¼ê²©', 'split_after': False},
    'ê°€': {'type': 'particle', 'meaning': 'ì£¼ê²©', 'split_after': False},
    'ì„': {'type': 'particle', 'meaning': 'ëª©ì ê²©', 'split_after': False},
    'ë¥¼': {'type': 'particle', 'meaning': 'ëª©ì ê²©', 'split_after': False},
    'ë„': {'type': 'particle', 'meaning': 'ë³´ì¡°ì‚¬', 'split_after': False},
    'ë§Œ': {'type': 'particle', 'meaning': 'ë³´ì¡°ì‚¬', 'split_after': False},
}

# [REMOVED] ì¤‘ì„¸êµ­ì–´ ì–´ë¯¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤ - ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
# - detect_middle_korean_ending
# - split_hanja_with_korean_ending
# - enhance_src_split_with_korean_endings

def split_src_meaning_units(
    text: str,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    by_space: bool = False,
    **kwargs
):
    """ì›ë¬¸(í•œë¬¸+í•œê¸€)ì„ jiebaì™€ MeCabìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• """
    
    # ë¬´ê²°ì„± ê²€ì¦: ì…ë ¥ í…ìŠ¤íŠ¸ ë³´ì¡´
    if not text or not text.strip():
        return []
    
    original_text = text
    
    # 1ë‹¨ê³„: ê³µë°± ì •ê·œí™” ë° ì „ê° ì½œë¡  ì²˜ë¦¬ (ë¬´ê²°ì„± ë³´ì¥)
    # ì—°ì† ê³µë°±ì„ ë‹¨ì¼ ê³µë°±ë¡œ ì •ê·œí™”í•˜ê³ , ì „ê° ì½œë¡  ë’¤ì— ê³µë°± ì¶”ê°€
    normalized_text = re.sub(r'\s+', ' ', text.replace('\n', ' ')).strip()
    normalized_text = normalized_text.replace('ï¼š', 'ï¼š ')
    
    words = normalized_text.split()
    if not words:
        return []
    
    # 2ë‹¨ê³„: jiebaì™€ MeCab ë¶„ì„ ê²°ê³¼ ì¤€ë¹„ (ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ì¤€)
    jieba_tokens = list(jieba.cut(original_text))
    
    # MeCab ë¶„ì„ (í•œê¸€ ë¶€ë¶„ìš©)
    morpheme_info = []
    if mecab:
        result = mecab.parse(original_text)
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    pos = parts[1].split(',')[0]
                    morpheme_info.append((surface, pos))
    
    # 3ë‹¨ê³„: ì–´ì ˆë“¤ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™” (jieba + MeCab ì •ë³´ ì ê·¹ í™œìš©)
    units = []
    i = 0
    
    while i < len(words):
        word = words[i]
        
        # ì „ê° ì½œë¡  ì²˜ë¦¬ - í•˜ë“œ ê²½ê³„
        if word.endswith('ï¼š'):
            units.append(word)
            i += 1
            continue
        
        # í•œì í¬í•¨ ì–´ì ˆ ì²˜ë¦¬
        if hanja_re.search(word):
            # í˜„ì¬ ì–´ì ˆì´ í•œìë¥¼ í¬í•¨í•˜ë©´ í•˜ë‚˜ì˜ ì˜ë¯¸ ë‹¨ìœ„
            units.append(word)
            i += 1
            continue
        
        # í•œê¸€ ì–´ì ˆë“¤ ì²˜ë¦¬ - jiebaì™€ MeCab ë¶„ì„ ê²°ê³¼ ì ê·¹ í™œìš©
        if hangul_re.match(word):
            group = [word]
            j = i + 1
            
            # ì¤‘ì„¸êµ­ì–´ ì–´ë¯¸ë‚˜ ë¬¸ë²• í‘œì§€ë¡œ ê²½ê³„ íŒë‹¨ (ì›ë¬¸ìš©)
            should_break_here = _should_break_by_mecab_src(word, morpheme_info) if morpheme_info else False
            
            # jieba í† í° ì—°ì†ì„±ë„ ì ê·¹ í™•ì¸ (ê²½ê³„ ì‹ í˜¸ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
            if not should_break_here:
                while j < len(words) and hangul_re.match(words[j]):
                    should_group = _should_group_words_by_jieba(group + [words[j]], jieba_tokens)
                    if should_group:
                        group.append(words[j])
                        j += 1
                    else:
                        break
            
            units.append(' '.join(group))
            i = j
            continue
        
        # ê¸°íƒ€ ì–´ì ˆ (ìˆ«ì, êµ¬ë‘ì  ë“±)
        units.append(word)
        i += 1
    
    # ë¬´ê²°ì„± ê²€ì¦: í† í° ìˆœì„œ ë° ë‚´ìš© ë³´ì¡´ í™•ì¸
    reconstructed = ' '.join(units).replace(' ', '')
    original_clean = original_text.replace(' ', '').replace('\n', '')
    
    if reconstructed != original_clean:
        logger.warning(f"ì›ë¬¸ ë¶„í•  ë¬´ê²°ì„± ê²½ê³ : ì›ë³¸ê³¼ ë³µì› ê²°ê³¼ ë¶ˆì¼ì¹˜")
        logger.warning(f"ì›ë³¸: {original_clean[:100]}...")
        logger.warning(f"ë³µì›: {reconstructed[:100]}...")
    
    return units

def _should_group_words_by_jieba(word_group: List[str], jieba_tokens: List[str]) -> bool:
    """jieba ë¶„ì„ ê²°ê³¼ë¥¼ ì ê·¹ ì°¸ê³ í•´ì„œ ì–´ì ˆë“¤ì„ ë¬¶ì„ì§€ ê²°ì • (ê°•í™” ë²„ì „)"""
    combined = ''.join(word_group)
    
    # 1. jieba í† í°ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš° (ìš°ì„ ìˆœìœ„ 1)
    for token in jieba_tokens:
        clean_token = token.replace(' ', '').replace('\n', '')
        clean_combined = combined.replace(' ', '')
        if clean_token == clean_combined and len(clean_token) > 1:
            return True
    
    # 2. jieba í† í°ì— í¬í•¨ë˜ëŠ” ë¶€ë¶„ ë¬¸ìì—´ì¸ ê²½ìš° (ìš°ì„ ìˆœìœ„ 2)
    for token in jieba_tokens:
        clean_token = token.replace(' ', '').replace('\n', '')
        clean_combined = combined.replace(' ', '')
        if len(clean_combined) > 1 and clean_combined in clean_token:
            return True
    
    # 3. ê¸¸ì´ ì œí•œ ë° ê¸°ë³¸ íœ´ë¦¬ìŠ¤í‹±
    if len(combined) > 15:  # ë„ˆë¬´ ê¸´ ì¡°í•© ë°©ì§€
        return False
    
    if len(word_group) > 4:  # ë„ˆë¬´ ë§ì€ ì–´ì ˆ ì¡°í•© ë°©ì§€
        return False
    
    # 4. ë‹¨ìŒì ˆ ì–´ì ˆë“¤ì˜ ì¡°í•©ì€ ì‹ ì¤‘í•˜ê²Œ (ê¸¸ì´ 3 ì´í•˜)
    if all(len(word) <= 1 for word in word_group) and len(word_group) > 2:
        return False
    
    return len(word_group) <= 3

def split_inside_chunk(chunk: str) -> List[str]:
    """ë²ˆì—­ë¬¸ ì²­í¬ë¥¼ ê²½ê³„ í‘œì§€ ê¸°ë°˜ìœ¼ë¡œ ë¶„í•  (ì „ê° ì½œë¡  ìš°ì„  ì²˜ë¦¬, ë¬´ê²°ì„± ë³´ì¥)"""
    
    if not chunk or not chunk.strip():
        return []
    
    original_chunk = chunk
    
    # 1ë‹¨ê³„: ì „ê° ì½œë¡  ìš°ì„  ë¶„í•  (ì¬ê·€ì  ì²˜ë¦¬)
    if 'ï¼š' in chunk:
        colon_parts = chunk.split('ï¼š')
        if len(colon_parts) == 2:
            part1 = colon_parts[0].strip() + 'ï¼š'
            part2 = colon_parts[1].strip()
            result = []
            if part1.strip():
                result.append(part1)
            if part2.strip():
                result.extend(split_inside_chunk(part2))  # ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
            
            # ë¬´ê²°ì„± ê²€ì¦
            reconstructed = ''.join(result).replace(' ', '')
            original_clean = original_chunk.replace(' ', '')
            if reconstructed != original_clean:
                logger.warning(f"ì²­í¬ ë¶„í•  ë¬´ê²°ì„± ê²½ê³ : {original_chunk} -> {result}")
                return [original_chunk]  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
            
            return result
    
    # 2ë‹¨ê³„: ê³µë°± ì •ê·œí™” (ì—°ì† ê³µë°± ì œê±°)
    normalized_chunk = re.sub(r'\s+', ' ', chunk.strip())
    words = normalized_chunk.split()
    if not words:
        return []
    
    # 3ë‹¨ê³„: ê²½ê³„ í‘œì§€ ê¸°ë°˜ ë¶„í• 
    units = []
    current_group = []
    
    for word in words:
        current_group.append(word)
        
        # ê²½ê³„ í‘œì§€ í™•ì¸ (ë¬¸ë²•ì  í‘œì§€ + ì½¤ë§ˆ) - ë²ˆì—­ë¬¸ ì²˜ë¦¬
        if is_boundary_marker(word, is_source=False):
            units.append(' '.join(current_group))
            current_group = []
            continue
    
    # ë§ˆì§€ë§‰ ê·¸ë£¹ ì²˜ë¦¬
    if current_group:
        units.append(' '.join(current_group))
    
    # 4ë‹¨ê³„: "ë¼ê³ /ë¼ëŠ”" í‘œí˜„ í›„ì²˜ë¦¬ - ì• êµ¬ì— ë³‘í•©
    if len(units) > 1:
        merged_units = []
        i = 0
        
        while i < len(units):
            current_unit = units[i]
            
            # ë‹¤ìŒ ë‹¨ìœ„ê°€ "ë¼ê³ /ë¼ëŠ”" ê³„ì—´ë¡œ ì‹œì‘í•˜ë©´ í˜„ì¬ ë‹¨ìœ„ì— ë³‘í•©
            if i + 1 < len(units) and should_attach_to_previous(units[i + 1]):
                merged_unit = current_unit + ' ' + units[i + 1]
                merged_units.append(merged_unit.strip())
                i += 2  # ë‘ ë‹¨ìœ„ë¥¼ ëª¨ë‘ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ 2 ì¦ê°€
            else:
                merged_units.append(current_unit.strip())
                i += 1
        
        units = merged_units
    
    result = [unit.strip() for unit in units if unit.strip()]
    
    # ë¬´ê²°ì„± ê²€ì¦: ì „ì²´ ë‚´ìš© ë³´ì¡´ í™•ì¸
    reconstructed = ''.join(result).replace(' ', '')
    original_clean = original_chunk.replace(' ', '')
    
    if reconstructed != original_clean:
        logger.warning(f"ì²­í¬ ë¶„í•  ë¬´ê²°ì„± ê²½ê³ : ë‚´ìš© ë¶ˆì¼ì¹˜")
        logger.warning(f"ì›ë³¸: {original_clean}")
        logger.warning(f"ë³µì›: {reconstructed}")
        return [original_chunk]  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    return result if result else [original_chunk]

def _should_break_by_mecab(word: str, morpheme_info: List[tuple]) -> bool:
    """MeCab ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ì„œ ì˜ë¯¸ ë‹¨ìœ„ ê²½ê³„ ê²°ì • - ë³´ì¡°ì‚¬(JX) ê°•í™”"""
    
    # MeCab ë¶„ì„ ê²°ê³¼ í™•ì¸
    for surface, pos in morpheme_info:
        # ë‹¨ì–´ê°€ í•´ë‹¹ í˜•íƒœì†Œë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸ (ë” ì •í™•í•œ ë§¤ì¹­)
        if word.endswith(surface):
            # ê°•í•œ ê²½ê³„ ì‹ í˜¸ - ì¢…ê²°ì–´ë¯¸, êµ¬ë‘ì 
            if pos in ['EF', 'SF', 'SP']:
                return True
            
            # ë³´ì¡°ì‚¬(JX) - ë§¤ìš° ì¤‘ìš”í•œ ë¬¸ë²•ì  í‘œì§€ë¡œ ê°•í™” ì²˜ë¦¬
            if pos == 'JX':
                return True  # ëª¨ë“  ë³´ì¡°ì‚¬ì—ì„œ ë¶„í• 
            
            # ì£¼ìš” ì¡°ì‚¬ë“¤ - ì˜ë¯¸ ë‹¨ìœ„ ê²½ê³„
            if pos in ['JKS', 'JKO', 'JKC', 'JKB', 'JKG', 'JKV', 'JKQ']:
                return True  # ëª¨ë“  ì¡°ì‚¬ì—ì„œ ë¶„í• 
            
            # ì—°ê²°ì–´ë¯¸(EC) - ë¬¸ì¥ ì—°ê²°
            if pos == 'EC':
                return True  # ëª¨ë“  ì—°ê²°ì–´ë¯¸ì—ì„œ ë¶„í• 
            
            # ëª…ì‚¬í˜• ì „ì„±ì–´ë¯¸(ETN) - ëª…ì‚¬í™”
            if pos == 'ETN':
                return True
            
            # ê´€í˜•í˜• ì „ì„±ì–´ë¯¸(ETM) - ê´€í˜•ì–´í™”
            if pos == 'ETM':
                return True
            
            # ë™ì‚¬, í˜•ìš©ì‚¬ ì–´ê°„ ë‹¤ìŒì—ì„œ ê²½ê³„  
            if pos in ['VV', 'VA', 'VX']:
                return len(surface) >= 1  # ê¸¸ì´ 1 ì´ìƒì¸ ìš©ì–¸ ì–´ê°„ì—ì„œ ë¶„í• 
            
            # ì¤‘ìš”í•œ ë¶€ì‚¬ì—ì„œ ë¶„í•  (MAG, MAJ)
            if pos in ['MAG', 'MAJ'] and len(surface) >= 2:
                return True  # ê¸¸ì´ 2 ì´ìƒì¸ ë¶€ì‚¬ì—ì„œ ë¶„í• 
    
    return False

def find_target_span_end_simple(src_unit: str, remaining_tgt: str) -> int:
    """ê°„ë‹¨í•œ íƒ€ê²Ÿ ìŠ¤íŒ¬ íƒìƒ‰"""
    hanja_chars = regex.findall(r'\p{Han}+', src_unit)
    if not hanja_chars:
        return 0
    last = hanja_chars[-1]
    idx = remaining_tgt.rfind(last)
    if idx == -1:
        return len(remaining_tgt)
    end = idx + len(last)
    next_space = remaining_tgt.find(' ', end)
    return next_space + 1 if next_space != -1 else len(remaining_tgt)

# [REMOVED] find_target_span_end_semantic - ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ

def split_tgt_by_src_units(src_units: List[str], tgt_text: str) -> List[str]:
    """ì›ë¬¸ ë‹¨ìœ„ì— ë”°ë¥¸ ë²ˆì—­ë¬¸ ë¶„í•  (ë‹¨ìˆœ ë°©ì‹)"""
    results = []
    cursor = 0
    total = len(tgt_text)
    for src_u in src_units:
        remaining = tgt_text[cursor:]
        end_len = find_target_span_end_simple(src_u, remaining)
        chunk = tgt_text[cursor:cursor+end_len]
        results.extend(split_inside_chunk(chunk))
        cursor += end_len
    if cursor < total:
        results.extend(split_inside_chunk(tgt_text[cursor:]))
    return results

def split_tgt_by_src_units_semantic(
    src_units: List[str], 
    tgt_text: str, 
    embed_func: Callable = None, 
    min_tokens: int = DEFAULT_MIN_TOKENS
) -> List[str]:
    """ì›ë¬¸ ë‹¨ìœ„ì— ë”°ë¥¸ ë²ˆì—­ë¬¸ ë¶„í•  - ì „ê° ì½œë¡  ìš°ì„  ì²˜ë¦¬"""
    
    # 1ë‹¨ê³„: ì „ê° ì½œë¡ ì„ í•˜ë“œ ê²½ê³„ë¡œ ì²˜ë¦¬
    if 'ï¼š' in tgt_text:
        colon_parts = tgt_text.split('ï¼š')
        if len(colon_parts) == 2:
            part1 = colon_parts[0].strip() + 'ï¼š'
            part2 = colon_parts[1].strip()
            result = [part1]
            if len(src_units) > 1:
                remaining_src = src_units[1:]
                remaining_parts = split_tgt_by_src_units_semantic(
                    remaining_src, part2, embed_func, min_tokens
                )
                result.extend(remaining_parts)
            else:
                result.append(part2)
            return result
    
    # 2ë‹¨ê³„: ìˆœì°¨ ë°©ì‹ìœ¼ë¡œ ë¶„í• 
    return split_tgt_meaning_units_sequential(
        ' '.join(src_units), tgt_text, min_tokens, min_tokens * 3
    )

# [REMOVED] Complex semantic matching functions - ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
# - _find_optimal_semantic_matching
# - _calculate_keyword_bonus  
# - _calculate_structure_bonus
# - _calculate_length_balance_bonus
# - _dp_semantic_matching

def split_tgt_meaning_units(
    src_text: str,
    tgt_text: str,
    use_semantic: bool = False,  # ê¸°ë³¸ê°’ì„ Falseë¡œ ë³€ê²½ (ìˆœì°¨ ëª¨ë“œ ìš°ì„ )
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    embed_func: Callable = None
) -> List[str]:
    """ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  - ìˆœì°¨ ë°©ì‹ ìš°ì„ """
    
    # ê¸°ë³¸ì ìœ¼ë¡œ ìˆœì°¨ ë¶„í•  ì‚¬ìš© (ìˆœì„œ ë³´ì¥)
    if not use_semantic:
        return split_tgt_meaning_units_sequential(
            src_text, tgt_text, min_tokens, max_tokens
        )
    
    # ê¸°ì¡´ ì˜ë¯¸ ê¸°ë°˜ ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ìš©)
    if embed_func is None:
        from sa_embedders import compute_embeddings_with_cache
        embed_func = compute_embeddings_with_cache
        
    src_units = split_src_meaning_units(src_text, min_tokens, max_tokens)
    return split_tgt_by_src_units_semantic(
        src_units, tgt_text, embed_func=embed_func, min_tokens=min_tokens
    )

def tokenize_text(text):
    """í˜•íƒœì†Œ ë¶„ì„ ë° í† í°í™” - MeCab ì‚¬ìš©"""
    if mecab:
        result = mecab.parse(text)
        tokens = []
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 1:
                    tokens.append(parts[0])
        return tokens
    else:
        return text.split()

def pos_tag_text(text):
    """í’ˆì‚¬ íƒœê¹… - MeCab ì‚¬ìš©"""
    if mecab:
        result = mecab.parse(text)
        pos_tags = []
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    pos = parts[1].split(',')[0]
                    pos_tags.append((surface, pos))
        return pos_tags
    else:
        return [(word, 'UNKNOWN') for word in text.split()]

def sentence_split(text):
    """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
    return [s.strip() for s in sentences if s.strip()]

# [REMOVED] ì„ë² ë”© ì •ê·œí™” í•¨ìˆ˜ë“¤ - ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
# - normalize_for_embedding  
# - _normalize_for_embedding

def _calculate_grammar_bonus(span: str) -> float:
    """ë¬¸ë²•ì  ê²½ê³„ì— ëŒ€í•œ ë³´ë„ˆìŠ¤ ì ìˆ˜ ê³„ì‚° - MeCab ê¸°ë°˜ ë‹¨ìˆœí™” ë²„ì „"""
    span = span.strip()
    bonus = 0.0
    
    # 1. ì „ê° ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš° ê°•í•œ ë³´ë„ˆìŠ¤
    if span.endswith('ï¼š'):
        return 0.8
    
    # 2. MeCabì„ ì´ìš©í•œ ì •í™•í•œ ì–´ë¯¸/ì¡°ì‚¬ ë¶„ì„
    if mecab:
        try:
            result = mecab.parse(span)
            last_pos = None
            for line in result.split('\n'):
                if line and line != 'EOS':
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        pos_detail = parts[1].split(',')
                        last_pos = pos_detail[0]
            
            # í’ˆì‚¬ë³„ ë³´ë„ˆìŠ¤ - ë‹¨ìˆœí™”
            if last_pos == 'EF':  # ì¢…ê²°ì–´ë¯¸
                bonus = 0.5
            elif last_pos == 'EC':  # ì—°ê²°ì–´ë¯¸
                bonus = 0.4
            elif last_pos == 'JX':  # ë³´ì¡°ì‚¬
                bonus = 0.4
            elif last_pos in ['JKS', 'JKO', 'JKB', 'JKC']:  # ì£¼ìš” ì¡°ì‚¬
                bonus = 0.3
            elif last_pos in ['ETN', 'ETM']:  # ì „ì„±ì–´ë¯¸
                bonus = 0.3
        except:
            pass
    
    # 3. ê¸°ë³¸ êµ¬ë‘ì  ì²˜ë¦¬
    if span.endswith(('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ')):
        bonus = max(bonus, 0.4)
    elif span.endswith((',', 'ï¼Œ', ';', 'ï¼›')):
        bonus = max(bonus, 0.2)
        
    return min(bonus, 1.0)

# [REMOVED] ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” í•¨ìˆ˜ë“¤ - ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŒ
# - _split_single_target_to_multiple
# - _merge_splits_to_match_src_count  
# - _force_split_by_semantic_boundaries

def _should_break_by_mecab_src(word: str, morpheme_info: List[tuple]) -> bool:
    """ì›ë¬¸ìš© - MeCab ë¶„ì„ ê²°ê³¼ + ì¤‘ì„¸êµ­ì–´ ì–´ë¯¸ íŒ¨í„´ìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ ê²½ê³„ ê²°ì •"""
    
    # 1. ì¤‘ì„¸êµ­ì–´ ì–´ë¯¸ íŒ¨í„´ í™•ì¸ (ì›ë¬¸ì—ë§Œ ì ìš©)
    middle_korean_endings = [
        'ë‹ˆë¼', 'ë…¸ë¼', 'ë„ë‹¤', 'ë¡œë‹¤', 'ê°€ë‹¤', 'ê±°ë‹¤',  # ì¢…ê²°ì–´ë¯¸
        'ë ¤ë‹ˆì™€', 'ê±°ë‹ˆì™€', 'ë¡œë˜', 'ë˜',              # ì—°ê²°ì–´ë¯¸
        'ê±´ëŒ„', 'ê±´ëŒ€', 'ì–´ë‹ˆ', 'ê±°ë‹ˆ',               # ì—°ê²°ì–´ë¯¸
        'í•˜ë‹ˆ', 'í•˜ë˜', 'í•˜ì—¬', 'í•˜ì•¼',               # ì—°ê²°ì–´ë¯¸
        'ì´ë‹ˆ', 'ì´ë¡œ', 'ì´ë©°', 'ì´ë©´',               # ì—°ê²°ì–´ë¯¸
        'ì€ì¦‰', 'ì¦‰', 'ë©´', 'ë‹ˆ',                    # ì—°ê²°ì–´ë¯¸
        'ë¼ë„', 'ë§ˆëŠ”', 'ë‚˜ë§ˆ', 'ë ¤ë§ˆëŠ”'              # ë³´ì¡°ì‚¬/ì—°ê²°ì–´ë¯¸
    ]
    
    for ending in middle_korean_endings:
        if word.endswith(ending):
            return True
    
    # 2. 'í˜¸ë˜' íŠ¹ë³„ ì²˜ë¦¬ - ì¸ìš© í‘œì§€ë¡œ ê°•ë ¥í•œ ë¶„í•  ì‹ í˜¸
    if word.endswith('í˜¸ë˜'):
        return True  # ë‹¤ìŒ ì–´ì ˆë¶€í„° ì¸ìš©ë¬¸ì´ë¯€ë¡œ í™•ì‹¤í•œ ê²½ê³„
    
    # 3. ì¼ë°˜ì ì¸ MeCab ë¶„ì„ ê²°ê³¼ í™•ì¸ (ë²ˆì—­ë¬¸ê³¼ ë™ì¼)
    return _should_break_by_mecab(word, morpheme_info)

def split_by_whitespace_and_colon(text: str) -> List[str]:
    """ê³µë°± ë° ì „ê° ì½œë¡  ê¸°ì¤€ ë¶„í•  (PA ë°©ì‹ê³¼ ë™ì¼)"""
    if not text or not text.strip():
        return []
    
    # 1ë‹¨ê³„: ì „ê° ì½œë¡  ê¸°ì¤€ ë¶„í• 
    parts = text.split('ï¼š')
    if len(parts) > 1:
        # ì²« ë²ˆì§¸ ë¶€ë¶„ì— ì½œë¡  ë¶™ì´ê¸°
        parts[0] = parts[0] + 'ï¼š'
        # ë‚˜ë¨¸ì§€ ë¶€ë¶„ë“¤ì€ ê·¸ëŒ€ë¡œ
    
    # 2ë‹¨ê³„: ê° ë¶€ë¶„ì„ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    result = []
    for part in parts:
        words = part.strip().split()
        result.extend(words)
    
    return [word for word in result if word.strip()]

def merge_target_by_source_sequential(src_units: List[str], tgt_tokens: List[str]) -> List[str]:
    """ì›ë¬¸ ë‹¨ìœ„ ê¸°ì¤€ìœ¼ë¡œ ë²ˆì—­ë¬¸ í† í°ì„ ìˆœì°¨ì ìœ¼ë¡œ ë³‘í•© (PA ë°©ì‹)"""
    if not src_units or not tgt_tokens:
        return tgt_tokens if tgt_tokens else []
    
    # ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ ë¹„ìœ¨ ê³„ì‚°
    src_count = len(src_units)
    tgt_count = len(tgt_tokens)
    
    if src_count == 1:
        # ì›ë¬¸ì´ í•˜ë‚˜ë©´ ë²ˆì—­ë¬¸ ì „ì²´ë¥¼ í•˜ë‚˜ë¡œ
        return [' '.join(tgt_tokens)]
    
    # ë²ˆì—­ë¬¸ í† í°ì„ ì›ë¬¸ ê°œìˆ˜ë§Œí¼ ë¶„í• 
    tokens_per_unit = tgt_count // src_count
    remainder = tgt_count % src_count
    
    result = []
    start_idx = 0
    
    for i in range(src_count):
        # ë‚˜ë¨¸ì§€ê°€ ìˆìœ¼ë©´ ì•ìª½ ë‹¨ìœ„ë“¤ì— í•˜ë‚˜ì”© ë” ë°°ë¶„
        current_size = tokens_per_unit + (1 if i < remainder else 0)
        end_idx = start_idx + current_size
        
        if end_idx > tgt_count:
            end_idx = tgt_count
        
        if start_idx < end_idx:
            unit_tokens = tgt_tokens[start_idx:end_idx]
            result.append(' '.join(unit_tokens))
        
        start_idx = end_idx
    
    return result

def split_tgt_meaning_units_sequential(
    src_text: str,
    tgt_text: str,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    embed_func: Callable = None,
    use_grammar: bool = True,  # ğŸ†• ë¬¸ë²•ì  í‘œì§€ ì‚¬ìš© ì˜µì…˜
    **kwargs
) -> List[str]:
    """ì˜ë¯¸ ê¸°ë°˜ ìˆœì°¨ì  ë¶„í•  - ì„ë² ë”©ê³¼ ì½¤ë§ˆ ë¶„í• , ë¬¸ë²•ì  í‘œì§€ë¥¼ ì ê·¹ í™œìš© (ë¬´ê²°ì„± ë³´ì¥)"""
    
    # ë¬´ê²°ì„± ê²€ì¦: ì…ë ¥ ê²€ì‚¬
    if not tgt_text or not tgt_text.strip():
        return []
    
    original_tgt = tgt_text
    
    # ì „ê° ì½œë¡ ì„ í¬í•¨í•œ ê²½ìš° ìš°ì„  ì²˜ë¦¬ (í•˜ë“œ ê²½ê³„)
    if 'ï¼š' in tgt_text:
        colon_parts = tgt_text.split('ï¼š')
        if len(colon_parts) == 2:
            part1 = colon_parts[0].strip() + 'ï¼š'
            part2 = colon_parts[1].strip()
            result = []
            if part1.strip():
                result.append(part1)
            if part2.strip():
                # ë‚˜ë¨¸ì§€ ë¶€ë¶„ì„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
                remaining_parts = split_tgt_meaning_units_sequential(
                    src_text, part2, min_tokens, max_tokens, embed_func, use_grammar, **kwargs
                )
                result.extend(remaining_parts)
            
            # ë¬´ê²°ì„± ê²€ì¦
            reconstructed = ''.join(result).replace(' ', '')
            original_clean = original_tgt.replace(' ', '')
            if reconstructed != original_clean:
                logger.warning(f"íƒ€ê²Ÿ ë¶„í•  ë¬´ê²°ì„± ê²½ê³  (ì½œë¡ ): ë‚´ìš© ë¶ˆì¼ì¹˜")
                return [original_tgt]
            
            return result
    
    # ì„ë² ë”© í•¨ìˆ˜ ì¤€ë¹„
    if embed_func is None:
        try:
            from sa_embedders import compute_embeddings_with_cache
            embed_func = compute_embeddings_with_cache
        except ImportError:
            # ì„ë² ë”© ì—†ìœ¼ë©´ ê¸°ë³¸ ìˆœì°¨ ë¶„í• 
            return _fallback_sequential_split(src_text, tgt_text, min_tokens, max_tokens, use_grammar)
    
    # 1ë‹¨ê³„: ì›ë¬¸ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• 
    src_units = split_src_meaning_units(src_text, min_tokens, max_tokens)
    
    # 2ë‹¨ê³„: ë²ˆì—­ë¬¸ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í•  (ì½¤ë§ˆ ìš°ì„ , MeCab ì°¸ê³ )
    tgt_chunks = split_inside_chunk(tgt_text)
    
    # 3ë‹¨ê³„: ì˜ë¯¸ ê¸°ë°˜ ìˆœì°¨ ë§¤ì¹­ (ê°„ê²°í™”)
    result = _semantic_sequential_matching(src_units, tgt_chunks, embed_func)
    
    # ë¬´ê²°ì„± ê²€ì¦: ìµœì¢… ê²°ê³¼ í™•ì¸
    reconstructed = ''.join(result).replace(' ', '')
    original_clean = original_tgt.replace(' ', '')
    
    if reconstructed != original_clean:
        logger.warning(f"ìˆœì°¨ ë¶„í•  ë¬´ê²°ì„± ê²½ê³ : ë‚´ìš© ë¶ˆì¼ì¹˜")
        logger.warning(f"ì›ë³¸: {original_clean[:100]}...")
        logger.warning(f"ë³µì›: {reconstructed[:100]}...")
        # ì‹¤íŒ¨ ì‹œ í´ë°± ë¶„í•  ì‹œë„
        return _fallback_sequential_split(src_text, tgt_text, min_tokens, max_tokens, use_grammar)
    
    return result

def _semantic_sequential_matching(
    src_units: List[str], 
    tgt_chunks: List[str], 
    embed_func: Callable
) -> List[str]:
    """ì˜ë¯¸ ê¸°ë°˜ ìˆœì°¨ ë§¤ì¹­ - ìˆœì„œ ë³´ì¥í•˜ë©´ì„œ ì˜ë¯¸ì ìœ¼ë¡œ ìµœì í™”"""
    
    if not src_units or not tgt_chunks:
        return tgt_chunks if tgt_chunks else [' '.join(src_units)]
    
    # ì„ë² ë”© ê³„ì‚°
    try:
        src_embeddings = embed_func(src_units)
        tgt_embeddings = embed_func(tgt_chunks)
        
        # ê¸°ë³¸ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
        embed_similarity = _calculate_similarity_matrix(src_embeddings, tgt_embeddings)
        
        # ê°•í™”ëœ ìœ ì‚¬ë„ í–‰ë ¬ (í‚¤ì›Œë“œ + êµ¬ì¡° ì •ë³´ ê²°í•©)
        enhanced_similarity = _enhanced_similarity_matrix(src_units, tgt_chunks, embed_similarity)
        
        # êµ¬ì¡°ì  ë³´ë„ˆìŠ¤ ì¶”ê°€
        structure_bonus = _calculate_structure_bonus(src_units, tgt_chunks)
        
        # ìˆœì°¨ì  ìµœì  ë§¤ì¹­ (ê°•í™”ëœ ìœ ì‚¬ë„ ì‚¬ìš©)
        return _optimal_sequential_assignment(
            src_units, tgt_chunks, enhanced_similarity, structure_bonus
        )
        
    except Exception as e:
        logger.warning(f"ì„ë² ë”© ê¸°ë°˜ ë§¤ì¹­ ì‹¤íŒ¨, í´ë°±: {e}")
        return _fallback_sequential_split(' '.join(src_units), ' '.join(tgt_chunks), 1, 50)

def _calculate_similarity_matrix(src_embeddings, tgt_embeddings):
    """ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°"""
    import numpy as np
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    src_norm = src_embeddings / (np.linalg.norm(src_embeddings, axis=1, keepdims=True) + 1e-8)
    tgt_norm = tgt_embeddings / (np.linalg.norm(tgt_embeddings, axis=1, keepdims=True) + 1e-8)
    
    similarity = np.dot(src_norm, tgt_norm.T)
    return similarity

def _optimal_sequential_assignment(
    src_units: List[str], 
    tgt_chunks: List[str], 
    similarity_matrix,
    structure_bonus: float = 0.0
) -> List[str]:
    """ìˆœì°¨ì  ìµœì  í• ë‹¹ - ìˆœì„œ ë³´ì¥í•˜ë©´ì„œ ì˜ë¯¸ ìœ ì‚¬ë„ ìµœëŒ€í™”"""
    
    n_src = len(src_units)
    n_tgt = len(tgt_chunks)
    
    # 1:1 ë§¤ì¹­ì¸ ê²½ìš°
    if n_src == n_tgt:
        return tgt_chunks
    
    # ì›ë¬¸ì´ ë” ì ì€ ê²½ìš° - ë²ˆì—­ë¬¸ ì²­í¬ ë³‘í•© (êµ¬ì¡° ë³´ë„ˆìŠ¤ í¬í•¨)
    if n_src < n_tgt:
        return _merge_chunks_by_similarity(tgt_chunks, similarity_matrix, n_src, structure_bonus)
    
    # ì›ë¬¸ì´ ë” ë§ì€ ê²½ìš° - ë²ˆì—­ë¬¸ ì²­í¬ ë¶„í•  (êµ¬ì¡° ë³´ë„ˆìŠ¤ í¬í•¨)
    else:
        return _split_chunks_by_similarity(src_units, tgt_chunks, similarity_matrix, n_src, structure_bonus)

def _merge_chunks_by_similarity(
    tgt_chunks: List[str], 
    similarity_matrix, 
    target_count: int,
    structure_bonus: float = 0.0
) -> List[str]:
    """ìœ ì‚¬ë„ ê¸°ë°˜ ì²­í¬ ë³‘í•© - ìˆœì„œ ë³´ì¥"""
    
    if len(tgt_chunks) <= target_count:
        return tgt_chunks
    
    import numpy as np
    
    # í˜„ì¬ ì²­í¬ë“¤
    current_chunks = tgt_chunks[:]
    
    # target_countê°œê°€ ë  ë•Œê¹Œì§€ ë³‘í•©
    while len(current_chunks) > target_count:
        
        # ì¸ì ‘í•œ ì²­í¬ë“¤ ì¤‘ ë³‘í•© ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ ìŒ ì°¾ê¸°
        best_merge_idx = -1
        best_score = -1
        
        for i in range(len(current_chunks) - 1):
            # ë³‘í•© ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ + ê¸¸ì´ ê· í˜• + ì½¤ë§ˆ ë¶„í•  ë³´ì¡´ + êµ¬ì¡° ë³´ë„ˆìŠ¤)
            merge_score = _calculate_merge_score(
                current_chunks[i], 
                current_chunks[i + 1],
                i, 
                similarity_matrix,
                len(current_chunks),
                structure_bonus
            )
            
            if merge_score > best_score:
                best_score = merge_score
                best_merge_idx = i
        
        # ìµœì  ìŒ ë³‘í•©
        if best_merge_idx >= 0:
            merged_chunk = current_chunks[best_merge_idx] + ' ' + current_chunks[best_merge_idx + 1]
            current_chunks = (current_chunks[:best_merge_idx] + 
                            [merged_chunk] + 
                            current_chunks[best_merge_idx + 2:])
        else:
            # ë” ì´ìƒ ë³‘í•©í•  ìˆ˜ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ë‘ ì²­í¬ ë³‘í•©
            if len(current_chunks) >= 2:
                merged = current_chunks[-2] + ' ' + current_chunks[-1]
                current_chunks = current_chunks[:-2] + [merged]
            else:
                break
    
    return current_chunks

def _calculate_merge_score(
    chunk1: str, 
    chunk2: str, 
    position: int, 
    similarity_matrix, 
    total_chunks: int,
    structure_bonus: float = 0.0
) -> float:
    """ì²­í¬ ë³‘í•© ì ìˆ˜ ê³„ì‚° - ë¬¸ë²•ì  í‘œì§€ ê³ ë ¤"""
    
    # ê¸°ë³¸ ì ìˆ˜ (ê¸¸ì´ ê· í˜•)
    len1, len2 = len(chunk1), len(chunk2)
    length_balance = 1.0 - abs(len1 - len2) / max(len1 + len2, 1)
    
    # ì½¤ë§ˆ ë¶„í•  ë³´ì¡´ ì ìˆ˜ (ì½¤ë§ˆê°€ ìˆëŠ” ì²­í¬ëŠ” ë³‘í•©í•˜ì§€ ì•ŠìŒ)
    comma_penalty = 0.0
    if (',' in chunk1 and chunk1.endswith(',')) or ('ï¼Œ' in chunk1 and chunk1.endswith('ï¼Œ')):
        comma_penalty = -0.5  # ì½¤ë§ˆë¡œ ëë‚˜ëŠ” ì²­í¬ëŠ” ë³‘í•© í˜ë„í‹°
    if (',' in chunk2 and chunk2.endswith(',')) or ('ï¼Œ' in chunk2 and chunk2.endswith('ï¼Œ')):
        comma_penalty = -0.5
    
    # ğŸ†• ë¬¸ë²•ì  í‘œì§€ ë³´ì¡´ ì ìˆ˜
    grammar_bonus = _calculate_grammar_preservation_score(chunk1, chunk2)
    
    # ì˜ë¯¸ì  ìœ ì‚¬ë„ (ì„ë² ë”© ê¸°ë°˜) - ê°•í™”ëœ ê³„ì‚°
    semantic_bonus = 0.0
    try:
        if similarity_matrix is not None and position < similarity_matrix.shape[0] - 1:
            # ì¸ì ‘í•œ ì›ë¬¸ ë‹¨ìœ„ë“¤ê³¼ì˜ ìœ ì‚¬ë„ í‰ê·  (ë” ì •êµí•œ ê³„ì‚°)
            if position < similarity_matrix.shape[1] - 1:
                semantic_bonus = float(similarity_matrix[position, position]) * 0.4
                # êµì°¨ ìœ ì‚¬ë„ë„ ê³ ë ¤ (ì˜ë¯¸ì  ì—°ê´€ì„±)
                cross_similarity = float(similarity_matrix[position, position + 1]) * 0.2
                semantic_bonus += cross_similarity
    except:
        pass
    
    # ìœ„ì¹˜ ê¸°ë°˜ ë³´ë„ˆìŠ¤ (ì•ìª½ ì²­í¬ë“¤ ìš°ì„  ë³‘í•©)
    position_bonus = (total_chunks - position) / total_chunks * 0.2
    
    # êµ¬ì¡°ì  ë³´ë„ˆìŠ¤ ì ìš©
    final_structure_bonus = structure_bonus * 0.1  # ì „ì²´ ì ìˆ˜ì˜ 10%
    
    return length_balance + semantic_bonus + position_bonus + comma_penalty + grammar_bonus + final_structure_bonus

def _calculate_grammar_preservation_score(chunk1: str, chunk2: str) -> float:
    """ë¬¸ë²•ì  í‘œì§€ ë³´ì¡´ ì ìˆ˜ ê³„ì‚°"""
    
    # ê¸¸ì´ ê· í˜• ì ìˆ˜ (ê°„ê²°í™”)
    len1, len2 = len(chunk1), len(chunk2)
    length_balance = 1.0 - abs(len1 - len2) / max(len1 + len2, 1)
    
    # ì½¤ë§ˆ ë¶„í•  ë³´ì¡´ ì ìˆ˜ (ê²½ê³„ í‘œì§€ í™œìš©)
    comma_penalty = 0.0
    if is_boundary_marker(chunk1, is_source=False) or is_boundary_marker(chunk2, is_source=False):
        comma_penalty = -0.5  # ê²½ê³„ í‘œì§€ê°€ ìˆëŠ” ì²­í¬ëŠ” ë³‘í•© í˜ë„í‹°
    
    # ê²½ê³„ ê°•ë„ ê¸°ë°˜ ì ìˆ˜
    boundary_bonus = (get_boundary_strength(chunk1, is_source=False) + get_boundary_strength(chunk2, is_source=False)) * 0.2
    
    return length_balance + comma_penalty + boundary_bonus

def _split_chunks_by_similarity(
    src_units: List[str],
    tgt_chunks: List[str], 
    similarity_matrix,
    target_count: int,
    structure_bonus: float = 0.0
) -> List[str]:
    """ìœ ì‚¬ë„ ê¸°ë°˜ ì²­í¬ ë¶„í• """
    
    if len(tgt_chunks) >= target_count:
        return tgt_chunks[:target_count]  # ì˜ë¼ì„œ ë°˜í™˜
    
    # ê°€ì¥ ê¸´ ì²­í¬ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ë¶„í• 
    result_chunks = tgt_chunks[:]
    
    while len(result_chunks) < target_count:
        # ê°€ì¥ ê¸´ ì²­í¬ ì°¾ê¸°
        longest_idx = max(range(len(result_chunks)), key=lambda i: len(result_chunks[i]))
        longest_chunk = result_chunks[longest_idx]
        
        # MeCab ê¸°ë°˜ìœ¼ë¡œ ë¶„í•  ì‹œë„
        split_attempts = split_inside_chunk(longest_chunk)
        
        if len(split_attempts) > 1:
            # ë¶„í•  ì„±ê³µ
            result_chunks = (result_chunks[:longest_idx] + 
                           split_attempts + 
                           result_chunks[longest_idx + 1:])
        else:
            # ë¶„í•  ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ í† í° ë¶„í• 
            tokens = longest_chunk.split()
            if len(tokens) >= 2:
                mid = len(tokens) // 2
                part1 = ' '.join(tokens[:mid])
                part2 = ' '.join(tokens[mid:])
                result_chunks = (result_chunks[:longest_idx] + 
                               [part1, part2] + 
                               result_chunks[longest_idx + 1:])
            else:
                break  # ë” ì´ìƒ ë¶„í• í•  ìˆ˜ ì—†ìŒ
    
    return result_chunks[:target_count]

def _fallback_sequential_split(
    src_text: str, 
    tgt_text: str, 
    min_tokens: int, 
    max_tokens: int,
    use_grammar: bool = True
) -> List[str]:
    """ì„ë² ë”© ì—†ì„ ë•Œ í´ë°± ë¶„í•  - ë¬¸ë²•ì  í‘œì§€ ê³ ë ¤"""
    
    # ì›ë¬¸ ë¶„í• 
    src_units = split_src_meaning_units(src_text, min_tokens, max_tokens)
    
    # ë²ˆì—­ë¬¸ ì½¤ë§ˆ ê¸°ë°˜ ë¶„í• 
    tgt_chunks = split_inside_chunk(tgt_text)
    # ê°„ë‹¨í•œ ê¸¸ì´ ê¸°ë°˜ ì¡°ì • (ê°„ê²°í™”)
    if len(src_units) == len(tgt_chunks):
        return tgt_chunks
    elif len(src_units) == 1:
        # ì½¤ë§ˆ ë¶„í•  ë³´ì¡´ (ê²½ê³„ í‘œì§€ ê¸°ë°˜)
        if len(tgt_chunks) > 1 and any(is_boundary_marker(c, is_source=False) for c in tgt_chunks):
            return tgt_chunks
        else:
            return [tgt_text]
    elif len(tgt_chunks) > len(src_units):
        # ê°„ë‹¨í•œ ë³‘í•©
        return _simple_merge_chunks(tgt_chunks, len(src_units))
    else:
        # ê°„ë‹¨í•œ ë¶„í• 
        return _simple_split_by_tokens(tgt_text, len(src_units))

def _simple_merge_chunks(chunks: List[str], target_count: int) -> List[str]:
    """ê°„ë‹¨í•œ ì²­í¬ ë³‘í•©"""
    if len(chunks) <= target_count:
        return chunks
    
    result = chunks[:]
    while len(result) > target_count:
        # ê°€ì¥ ì§§ì€ ì¸ì ‘ ìŒ ë³‘í•©
        min_len = float('inf')
        merge_idx = 0
        
        for i in range(len(result) - 1):
            combined_len = len(result[i]) + len(result[i + 1])
            # ì½¤ë§ˆë¡œ ëë‚˜ëŠ” ì²­í¬ëŠ” ë³‘í•© ìš°ì„ ìˆœìœ„ ë‚®ì¶¤
            if result[i].endswith(',') or result[i].endswith('ï¼Œ'):
                combined_len += 100  # í˜ë„í‹°
            
            if combined_len < min_len:
                min_len = combined_len
                merge_idx = i
        
        # ë³‘í•© ì‹¤í–‰
        merged = result[merge_idx] + ' ' + result[merge_idx + 1]
        result = result[:merge_idx] + [merged] + result[merge_idx + 2:]
    
    return result

def _simple_split_by_tokens(text: str, target_count: int) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ í† í° ê¸°ì¤€ìœ¼ë¡œ ë‹¨ìˆœ ë¶„í• """
    tokens = text.split()
    if len(tokens) <= target_count:
        return [text]
    
    tokens_per_unit = len(tokens) // target_count
    remainder = len(tokens) % target_count
    
    result = []
    start = 0
    
    for i in range(target_count):
        current_size = tokens_per_unit + (1 if i < remainder else 0)
        end = start + current_size
        
        if start < len(tokens):
            segment = ' '.join(tokens[start:end]).strip()
            if segment:
                result.append(segment)
        start = end
    
    return result

def _merge_target_chunks_sequential(chunks: List[str], target_count: int) -> List[str]:
    """ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ì„± ìœ ì§€"""
    return _simple_merge_chunks(chunks, target_count)

# ë¬¸ë²•ì  ê²½ê³„ í‘œì§€ì™€ ì½¤ë§ˆë¥¼ ê°„ê²°í•˜ê²Œ ì¸ì‹í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€
def is_boundary_marker(text: str, is_source: bool = False) -> bool:
    """ë¬¸ë²•ì  ê²½ê³„ í‘œì§€ ë˜ëŠ” ì½¤ë§ˆ ê²½ê³„ì¸ì§€ í™•ì¸ (ì›ë¬¸/ë²ˆì—­ë¬¸ êµ¬ë¶„, ë°œí™”ë™ì‚¬ íƒì§€ í†µí•©)"""
    
    # 1. ë¬¸ë²•ì  í‘œì§€ í™•ì¸ (ê¸°ì¡´ ë¡œì§)
    for marker, functions in BOUNDARY_MARKERS.items():
        if text.endswith(marker):
            # ì›ë¬¸ ì „ìš© í‘œì§€ì¸ì§€ í™•ì¸
            if 'src_only' in functions:
                if is_source:
                    # 'í˜¸ë˜'ëŠ” ì›ë¬¸ì—ì„œ í‘œì  ê¸°ëŠ¥ - ê°•ë ¥í•œ ë¶„í•  ì‹ í˜¸
                    if marker == 'í˜¸ë˜':
                        return True  # ì¸ìš© ê²½ê³„ë¡œ í‘œì  ê¸°ëŠ¥
                    return True
                else:
                    # ë²ˆì—­ë¬¸ì—ì„œëŠ” ì›ë¬¸ ì „ìš© í‘œì§€ ë¬´ì‹œ
                    continue
            else:
                # ì¼ë°˜ í‘œì§€ (ë²ˆì—­ë¬¸ì—ì„œ ì‚¬ìš©)
                return True
    
    # 2. ğŸ†• ë°œí™”ë™ì‚¬ ê¸°ë°˜ ì¸ìš© ê²½ê³„ íƒì§€ (ë²ˆì—­ë¬¸ì—ì„œë§Œ)
    if not is_source and is_quotative_end_pattern(text):
        return True
    
    # 3. ì½¤ë§ˆ ê²½ê³„ í™•ì¸ (ë²ˆì—­ë¬¸ì—ì„œë§Œ, ë³‘ë ¬ ì œì™¸)
    if not is_source:
        import re
        for pattern in COMMA_BOUNDARY_PATTERNS:
            if re.search(pattern, text):
                return True
        
        # 4. ë‹¨ìˆœ ì½¤ë§ˆ (ë³‘ë ¬ì´ ì•„ë‹Œ ê²½ìš°)
        if text.endswith(',') or text.endswith('ï¼Œ'):
            # ë³‘ë ¬ ì œì™¸ ë¡œì§ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            if not any(conj in text for conj in ['ê³¼', 'ì™€', 'ë°', 'ë˜ëŠ”', 'ì´ë‚˜']):
                return True
    
    return False

def get_boundary_strength(text: str, is_source: bool = False) -> float:
    """ê²½ê³„ ê°•ë„ ê³„ì‚° (0.0-1.0) - ì›ë¬¸/ë²ˆì—­ë¬¸ êµ¬ë¶„, 'í˜¸ë˜' íŠ¹ë³„ ì²˜ë¦¬, ë°œí™”ë™ì‚¬ ê³ ë ¤"""
    
    # 1. ì›ë¬¸ ì „ìš© í‘œì§€ ì²˜ë¦¬
    if is_source:
        # 'í˜¸ë˜'ëŠ” ì›ë¬¸ì—ì„œ í‘œì  ê¸°ëŠ¥ - ë§¤ìš° ê°•í•œ ê²½ê³„
        if text.endswith('í˜¸ë˜'):
            return 0.95  # ì¸ìš© í‘œì§€ë¡œ í‘œì  ê¸°ëŠ¥, ê±°ì˜ ì ˆëŒ€ì  ë¶„í• ì 
        
        # ê¸°íƒ€ ì¤‘ì„¸êµ­ì–´ ì–´ë¯¸ë“¤
        for marker, functions in BOUNDARY_MARKERS.items():
            if 'src_only' in functions and text.endswith(marker):
                return 0.8  # ì›ë¬¸ ì „ìš© ì–´ë¯¸ë“¤ì€ ê°•í•œ ê²½ê³„
    
    # 2. ì „ê° ì½œë¡  (ì›ë¬¸/ë²ˆì—­ë¬¸ ê³µí†µ)
    if text.endswith('ï¼š'):
        return 0.95  # 'í˜¸ë˜'ì™€ ë™ë“±í•œ ê°•ë„
    
    # 3. ğŸ†• ë°œí™”ë™ì‚¬ ê¸°ë°˜ ì¸ìš© ê²½ê³„ (ë²ˆì—­ë¬¸ì—ì„œë§Œ)
    if not is_source and is_quotative_end_pattern(text):
        return 0.85  # ë°œí™”ë™ì‚¬ ë’¤ëŠ” ê°•í•œ ë¶„í• ì  (ì¸ìš©ë¬¸ ê²½ê³„)
    
    # 4. ì¼ë°˜ ë¬¸ë²•ì  í‘œì§€ (ë²ˆì—­ë¬¸ì—ì„œ ì£¼ë¡œ ì‚¬ìš©)
    for marker, functions in BOUNDARY_MARKERS.items():
        if 'src_only' not in functions and text.endswith(marker):
            if 'boundary' in functions:
                return 0.8  # ì¼ë°˜ì ì¸ ê²½ê³„ í‘œì§€
            return 0.6
    
    # 5. ì½¤ë§ˆ ê²½ê³„ (ë²ˆì—­ë¬¸ì—ì„œë§Œ)
    if not is_source and (text.endswith(',') or text.endswith('ï¼Œ')):
        return 0.5
    
    return 0.0

def _calculate_keyword_similarity(src_unit: str, tgt_chunk: str) -> float:
    """í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚° - í•œë¬¸-í•œê¸€ ë§¤ì¹­ ê°•í™” (ê´„í˜¸ í•œì í¬í•¨)"""
    
    # ğŸ†• 'í˜¸ë˜'-ì½œë¡  íŠ¹ë³„ ë§¤ì¹­ (ìš°ì„ ìˆœìœ„ 1)
    if src_unit.endswith('í˜¸ë˜') and tgt_chunk.endswith('ï¼š'):
        # í˜¸ë˜ë¡œ ëë‚˜ëŠ” ì›ë¬¸ê³¼ ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ë²ˆì—­ë¬¸ì€ ê°•í•œ ìœ ì‚¬ì„±
        base_score = 0.85  # ë†’ì€ ê¸°ë³¸ ì ìˆ˜
        
        # ë‚˜ë¨¸ì§€ ë¶€ë¶„ì˜ í•œì ë§¤ì¹­ë„ í™•ì¸
        src_without_ending = src_unit[:-2]  # 'í˜¸ë˜' ì œê±°
        tgt_without_colon = tgt_chunk[:-1]  # 'ï¼š' ì œê±°
        
        if src_without_ending and tgt_without_colon:
            content_similarity = _calculate_content_similarity(src_without_ending, tgt_without_colon)
            return min(base_score + (content_similarity * 0.15), 1.0)
        else:
            return base_score
    
    # ğŸ†• ì—­ë°©í–¥ ë§¤ì¹­: ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ì›ë¬¸ê³¼ í˜¸ë˜ë¡œ ëë‚˜ëŠ” ë²ˆì—­ë¬¸
    elif src_unit.endswith('ï¼š') and tgt_chunk.endswith('í˜¸ë˜'):
        base_score = 0.85
        src_without_colon = src_unit[:-1]
        tgt_without_ending = tgt_chunk[:-2]
        
        if src_without_colon and tgt_without_ending:
            content_similarity = _calculate_content_similarity(src_without_colon, tgt_without_ending)
            return min(base_score + (content_similarity * 0.15), 1.0)
        else:
            return base_score
    
    # í•œì í‚¤ì›Œë“œ ì¶”ì¶œ (ì›ë¬¸ì—ì„œ)
    src_hanja = regex.findall(r'\p{Han}+', src_unit)
    
    if not src_hanja:
        return 0.0
    
    # ğŸ†• ë²ˆì—­ë¬¸ì—ì„œ í•œì ì¶”ì¶œ - ê´„í˜¸ ì•ˆ í•œì ìš°ì„  ê³ ë ¤
    # 1. ê´„í˜¸ ì•ˆì˜ í•œì ì¶”ì¶œ (ê°€ì¥ ì¤‘ìš”)
    bracket_hanja = regex.findall(r'[ï¼ˆ(]\s*(\p{Han}+)\s*[ï¼‰)]', tgt_chunk)
    
    # 2. ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ í•œì ì¶”ì¶œ (ë³´ì¡°ì )
    direct_hanja = regex.findall(r'\p{Han}+', tgt_chunk)
    
    # ê´„í˜¸ ì•ˆ í•œìë¥¼ ìš°ì„ ì ìœ¼ë¡œ ê³ ë ¤í•˜ë˜, ì¼ë°˜ í•œìë„ í¬í•¨
    all_tgt_hanja = bracket_hanja + direct_hanja
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (ê´„í˜¸ í•œìì— ê°€ì¤‘ì¹˜ ë¶€ì—¬)
    keyword_matches = 0.0
    total_keywords = len(src_hanja)
    
    for src_word in src_hanja:
        best_match_score = 0.0
        
        # 1. ê´„í˜¸ ì•ˆ í•œìì™€ì˜ ì™„ì „ ì¼ì¹˜ (ìµœê³  ì ìˆ˜ + ë³´ë„ˆìŠ¤)
        if src_word in bracket_hanja:
            best_match_score = 1.2  # ê´„í˜¸ í•œì ì™„ì „ ì¼ì¹˜ ë³´ë„ˆìŠ¤
        
        # 2. ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œì˜ ì™„ì „ ì¼ì¹˜
        elif src_word in tgt_chunk:
            best_match_score = 1.0
            
        # 3. ê´„í˜¸ í•œìì™€ì˜ ë¶€ë¶„ ì¼ì¹˜ (ë†’ì€ ì ìˆ˜)
        else:
            for bracket_word in bracket_hanja:
                if src_word in bracket_word or bracket_word in src_word:
                    partial_score = len(set(src_word) & set(bracket_word)) / len(set(src_word) | set(bracket_word))
                    best_match_score = max(best_match_score, partial_score * 1.0)  # ê´„í˜¸ ë‚´ ë¶€ë¶„ ì¼ì¹˜ ë³´ë„ˆìŠ¤
            
            # 4. ì¼ë°˜ í•œìì™€ì˜ ë¶€ë¶„ ì¼ì¹˜ (ì¤‘ê°„ ì ìˆ˜)
            if best_match_score == 0.0:
                for tgt_word in direct_hanja:
                    if src_word in tgt_word or tgt_word in src_word:
                        partial_score = len(set(src_word) & set(tgt_word)) / len(set(src_word) | set(tgt_word))
                        best_match_score = max(best_match_score, partial_score * 0.8)
            
            # 5. ê°œë³„ í•œì ë§¤ì¹­ (ê°€ì¥ ë‚®ì€ ì ìˆ˜)
            if best_match_score == 0.0:
                char_matches = sum(1 for char in src_word if char in tgt_chunk)
                if char_matches > 0:
                    best_match_score = (char_matches / len(src_word)) * 0.3
        
        keyword_matches += min(best_match_score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ (ê´„í˜¸ í•œì ë³´ë„ˆìŠ¤ ë°˜ì˜)
    keyword_ratio = keyword_matches / max(total_keywords, 1)
    
    # ğŸ†• ê´„í˜¸ í•œì ì¡´ì¬ ì‹œ ì¶”ê°€ ë³´ë„ˆìŠ¤
    bracket_bonus = 0.0
    if bracket_hanja and any(src_word in bracket_hanja for src_word in src_hanja):
        # ì›ë¬¸ í•œìì™€ ê´„í˜¸ í•œìì˜ ì¼ì¹˜ ë¹„ìœ¨ì— ë”°ë¥¸ ë³´ë„ˆìŠ¤
        matching_bracket_count = sum(1 for src_word in src_hanja if src_word in bracket_hanja)
        bracket_bonus = (matching_bracket_count / len(src_hanja)) * 0.2
    
    # ê¸¸ì´ ê¸°ë°˜ ë³´ì • (ê°œì„ ëœ ê³µì‹)
    src_len = len(src_unit.replace(' ', ''))
    tgt_len = len(tgt_chunk.replace(' ', ''))
    
    if src_len > 0 and tgt_len > 0:
        length_ratio = min(tgt_len / src_len, src_len / tgt_len)
        # ì ì • ê¸¸ì´ ë¹„ìœ¨ (0.5 ~ 2.0)ì—ì„œ ìµœê³  ì ìˆ˜
        if 0.5 <= length_ratio <= 2.0:
            length_factor = 1.0
        else:
            length_factor = max(0.3, length_ratio if length_ratio < 0.5 else 1.0 / length_ratio)
    else:
        length_factor = 0.1
    
    # êµ¬ë‘ì  ì¼ì¹˜ ë³´ë„ˆìŠ¤
    punctuation_bonus = 0.0
    src_punct = set(char for char in src_unit if char in 'ï¼Œã€‚ï¼›ï¼ï¼Ÿï¼š')
    tgt_punct = set(char for char in tgt_chunk if char in 'ï¼Œã€‚ï¼›ï¼ï¼Ÿï¼š,.')
    if src_punct and tgt_punct:
        punctuation_bonus = len(src_punct & tgt_punct) / max(len(src_punct | tgt_punct), 1) * 0.1
    
    final_score = keyword_ratio * length_factor + bracket_bonus + punctuation_bonus
    return min(final_score, 1.0)

def _calculate_content_similarity(src_content: str, tgt_content: str) -> float:
    """'í˜¸ë˜'ì™€ 'ï¼š'ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë‚´ìš©ì˜ ìœ ì‚¬ë„ ê³„ì‚° - ê´„í˜¸ í•œì ê³ ë ¤"""
    
    # í•œì í‚¤ì›Œë“œ ì¶”ì¶œ (ì›ë¬¸)
    src_hanja = regex.findall(r'\p{Han}+', src_content)
    
    if not src_hanja:
        return 0.0
    
    # ğŸ†• ë²ˆì—­ë¬¸ì—ì„œ ê´„í˜¸ í•œì ìš°ì„  ì¶”ì¶œ
    bracket_hanja = regex.findall(r'[ï¼ˆ(]\s*(\p{Han}+)\s*[ï¼‰)]', tgt_content)
    direct_hanja = regex.findall(r'\p{Han}+', tgt_content)
    
    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (ê´„í˜¸ í•œìì— ê°€ì¤‘ì¹˜)
    keyword_matches = 0.0
    total_keywords = len(src_hanja)
    
    for src_word in src_hanja:
        # 1. ê´„í˜¸ ì•ˆ í•œìì™€ì˜ ì™„ì „ ì¼ì¹˜ (ë³´ë„ˆìŠ¤ ì ìˆ˜)
        if src_word in bracket_hanja:
            keyword_matches += 1.2
        # 2. ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œì˜ ì™„ì „ ì¼ì¹˜
        elif src_word in tgt_content:
            keyword_matches += 1.0
        else:
            # 3. ë¶€ë¶„ ë§¤ì¹­ (ê°œë³„ í•œì)
            char_matches = sum(1 for char in src_word if char in tgt_content)
            if char_matches > 0:
                keyword_matches += (char_matches / len(src_word)) * 0.5
    
    # ê²°ê³¼ë¥¼ [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
    return min(keyword_matches / max(total_keywords, 1), 1.0)

def _enhanced_similarity_matrix(src_units: List[str], tgt_chunks: List[str], embed_similarity) -> np.ndarray:
    """ì„ë² ë”© ìœ ì‚¬ë„ì™€ í‚¤ì›Œë“œ ìœ ì‚¬ë„ë¥¼ ê²°í•©í•œ ê°•í™”ëœ ìœ ì‚¬ë„ í–‰ë ¬"""
    
    n_src = len(src_units)
    n_tgt = len(tgt_chunks)
    
    # í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
    keyword_similarity = np.zeros((n_src, n_tgt))
    
    for i, src_unit in enumerate(src_units):
        for j, tgt_chunk in enumerate(tgt_chunks):
            keyword_similarity[i, j] = _calculate_keyword_similarity(src_unit, tgt_chunk)
    
    # ì„ë² ë”© ìœ ì‚¬ë„ì™€ í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê²°í•© (ê°œì„ ëœ ê°€ì¤‘ì¹˜)
    # í•œë¬¸-í•œê¸€ ë²ˆì—­ì—ì„œëŠ” í‚¤ì›Œë“œ ë§¤ì¹­ì´ ë” ì¤‘ìš”í•  ìˆ˜ ìˆìŒ
    combined_similarity = (
        embed_similarity * 0.55 +      # ì„ë² ë”© ìœ ì‚¬ë„ 55%
        keyword_similarity * 0.45      # í‚¤ì›Œë“œ ìœ ì‚¬ë„ 45% (ì¦ê°€)
    )
    
    return combined_similarity

def _calculate_structure_bonus(src_units: List[str], tgt_chunks: List[str]) -> float:
    """êµ¬ì¡°ì  ì¼ì¹˜ë„ ë³´ë„ˆìŠ¤ ê³„ì‚° (ê°œì„  ë²„ì „ - 'í˜¸ë˜'-ì½œë¡  ë§¤ì¹­ ê³ ë ¤)"""
    
    total_bonus = 0.0
    
    # ğŸ†• 'í˜¸ë˜'-ì½œë¡  êµ¬ì¡°ì  ë§¤ì¹­ ë³´ë„ˆìŠ¤ (ìš°ì„ ìˆœìœ„ 1)
    hodeok_colon_bonus = 0.0
    for i, src_unit in enumerate(src_units):
        if src_unit.endswith('í˜¸ë˜'):
            # ê°™ì€ ìœ„ì¹˜ë‚˜ ì¸ì ‘ ìœ„ì¹˜ì˜ íƒ€ê²Ÿì—ì„œ ì½œë¡  ì°¾ê¸°
            for j in range(max(0, i-1), min(len(tgt_chunks), i+2)):
                if j < len(tgt_chunks) and tgt_chunks[j].endswith('ï¼š'):
                    position_match = 1.0 - abs(i - j) / max(len(src_units), len(tgt_chunks), 1)
                    hodeok_colon_bonus = max(hodeok_colon_bonus, position_match * 0.6)
                    break
    
    total_bonus += hodeok_colon_bonus
    
    # 1. ì „ê° ì½œë¡  ìœ„ì¹˜ ì¼ì¹˜ë„ (ê¸°ì¡´ ë¡œì§, ê°€ì¤‘ì¹˜ ì¡°ì •)
    src_colon_positions = [i for i, unit in enumerate(src_units) if 'ï¼š' in unit]
    tgt_colon_positions = [i for i, chunk in enumerate(tgt_chunks) if 'ï¼š' in chunk]
    
    if src_colon_positions and tgt_colon_positions:
        # ì²« ë²ˆì§¸ ì½œë¡  ìœ„ì¹˜ ë¹„ìœ¨ ë¹„êµ
        src_ratio = src_colon_positions[0] / max(len(src_units) - 1, 1)
        tgt_ratio = tgt_colon_positions[0] / max(len(tgt_chunks) - 1, 1)
        position_similarity = 1.0 - abs(src_ratio - tgt_ratio)
        total_bonus += position_similarity * 0.3  # í˜¸ë˜-ì½œë¡  ë§¤ì¹­ì´ ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜ ê°ì†Œ
    
    # 2. ë‹¨ìœ„ ìˆ˜ ì¼ì¹˜ë„
    count_similarity = 1.0 - abs(len(src_units) - len(tgt_chunks)) / max(len(src_units), len(tgt_chunks), 1)
    total_bonus += count_similarity * 0.2
    
    # 3. ê¸¸ì´ ë¶„í¬ ì¼ì¹˜ë„ (ê°œì„ ëœ ê³„ì‚°)
    if len(src_units) > 1 and len(tgt_chunks) > 1:
        src_lengths = [len(unit.replace(' ', '')) for unit in src_units]
        tgt_lengths = [len(chunk.replace(' ', '')) for chunk in tgt_chunks]
        
        # ê°™ì€ ê¸¸ì´ì¸ ê²½ìš° ìƒê´€ê´€ê³„ ê³„ì‚°
        if len(src_lengths) == len(tgt_lengths):
            try:
                correlation = np.corrcoef(src_lengths, tgt_lengths)[0, 1]
                if not np.isnan(correlation):
                    length_bonus = max(correlation, 0) * 0.25
                else:
                    length_bonus = 0.0
            except:
                length_bonus = 0.0
        else:
            # ê¸¸ì´ê°€ ë‹¤ë¥¸ ê²½ìš° ë¶„í¬ì˜ ìœ ì‚¬ì„± ê³„ì‚°
            src_avg = np.mean(src_lengths) if src_lengths else 0
            tgt_avg = np.mean(tgt_lengths) if tgt_lengths else 0
            avg_similarity = 1.0 - abs(src_avg - tgt_avg) / max(src_avg + tgt_avg, 1)
            length_bonus = avg_similarity * 0.15
        
        total_bonus += length_bonus
    
    # 4. êµ¬ë‘ì  íŒ¨í„´ ì¼ì¹˜ë„
    src_punct_pattern = ''.join([char for unit in src_units for char in unit if char in 'ï¼Œã€‚ï¼›ï¼ï¼Ÿ'])
    tgt_punct_pattern = ''.join([char for chunk in tgt_chunks for char in chunk if char in 'ï¼Œã€‚ï¼›ï¼ï¼Ÿï¼š,.'])
    
    if src_punct_pattern and tgt_punct_pattern:
        # êµ¬ë‘ì  ìˆœì„œì™€ ì¢…ë¥˜ì˜ ì¼ì¹˜ë„
        punct_similarity = len(set(src_punct_pattern) & set(tgt_punct_pattern)) / len(set(src_punct_pattern) | set(tgt_punct_pattern))
        total_bonus += punct_similarity * 0.15
    
    return min(total_bonus, 1.0)

def _smart_colon_split(src_text: str, tgt_text: str) -> tuple:
    """ì „ê° ì½œë¡  ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë¶„í•  - ì˜ë¯¸ ëŒ€ì‘ ê³ ë ¤ ('í˜¸ë˜' íŠ¹ë³„ ì²˜ë¦¬)"""
    
    # ğŸ†• 'í˜¸ë˜'ë¡œ ëë‚˜ëŠ” ì›ë¬¸ê³¼ ì½œë¡ ì´ ìˆëŠ” ë²ˆì—­ë¬¸ ë§¤ì¹­
    if src_text.endswith('í˜¸ë˜') and 'ï¼š' in tgt_text:
        tgt_parts = tgt_text.split('ï¼š')
        if len(tgt_parts) == 2:
            # 'í˜¸ë˜'ë¡œ ëë‚˜ëŠ” ì›ë¬¸ì€ ì½œë¡  ì•ë¶€ë¶„ê³¼ ë§¤ì¹­
            src_parts = [src_text, '']  # í˜¸ë˜ëŠ” ì²« ë²ˆì§¸ ë¶€ë¶„ìœ¼ë¡œ ì²˜ë¦¬
            tgt_result = [tgt_parts[0] + 'ï¼š', tgt_parts[1].strip()]
            return ([src_text], tgt_result)
    
    # ğŸ†• ì½œë¡ ì´ ìˆëŠ” ì›ë¬¸ê³¼ 'í˜¸ë˜'ë¡œ ëë‚˜ëŠ” ë²ˆì—­ë¬¸ ë§¤ì¹­ (ì—­ë°©í–¥)
    elif 'ï¼š' in src_text and tgt_text.endswith('í˜¸ë˜'):
        src_parts = src_text.split('ï¼š')
        if len(src_parts) == 2:
            src_result = [src_parts[0] + 'ï¼š', src_parts[1].strip()]
            return (src_result, [tgt_text])
    
    # ê¸°ì¡´ ë¡œì§ ê³„ì† ì‹¤í–‰
    if 'ï¼š' not in src_text and 'ï¼š' not in tgt_text:
        return None, None
    
    # ì†ŒìŠ¤ì—ë§Œ ì½œë¡ ì´ ìˆëŠ” ê²½ìš°
    if 'ï¼š' in src_text and 'ï¼š' not in tgt_text:
        src_parts = src_text.split('ï¼š')
        if len(src_parts) == 2:
            # ë²ˆì—­ë¬¸ì—ì„œ í•´ë‹¹ ìœ„ì¹˜ ì¶”ì •
            src_ratio = len(src_parts[0]) / len(src_text)
            split_point = int(len(tgt_text) * src_ratio)
            
            # ì–´ì ˆ ê²½ê³„ì—ì„œ ë¶„í• ì  ì¡°ì •
            words = tgt_text[:split_point + 20].split()  # ì—¬ìœ ë¶„ í¬í•¨
            if len(words) > 1:
                adjusted_split = len(' '.join(words[:-1])) + 1
                part1 = tgt_text[:adjusted_split].strip() + 'ï¼š'
                part2 = tgt_text[adjusted_split:].strip()
                return (src_parts[0] + 'ï¼š', src_parts[1]), (part1, part2)
    
    # íƒ€ê²Ÿì—ë§Œ ì½œë¡ ì´ ìˆëŠ” ê²½ìš°
    elif 'ï¼š' not in src_text and 'ï¼š' in tgt_text:
        tgt_parts = tgt_text.split('ï¼š')
        if len(tgt_parts) == 2:
            # ì›ë¬¸ì—ì„œ í•´ë‹¹ ìœ„ì¹˜ ì¶”ì •
            tgt_ratio = len(tgt_parts[0]) / len(tgt_text)
            split_point = int(len(src_text) * tgt_ratio)
            
            # í•œì ê²½ê³„ì—ì„œ ë¶„í• ì  ì¡°ì •
            adjusted_split = split_point
            while adjusted_split < len(src_text) and src_text[adjusted_split] not in 'ï¼Œã€‚ï¼›':
                adjusted_split += 1
            
            if adjusted_split < len(src_text):
                part1 = src_text[:adjusted_split + 1]
                part2 = src_text[adjusted_split + 1:]
                return (part1, part2), (tgt_parts[0] + 'ï¼š', tgt_parts[1])
    
    # ë‘˜ ë‹¤ ì½œë¡ ì´ ìˆëŠ” ê²½ìš° (ê¸°ë³¸ ì²˜ë¦¬)
    elif 'ï¼š' in src_text and 'ï¼š' in tgt_text:
        src_parts = src_text.split('ï¼š')
        tgt_parts = tgt_text.split('ï¼š')
        if len(src_parts) == 2 and len(tgt_parts) == 2:
            return (src_parts[0] + 'ï¼š', src_parts[1]), (tgt_parts[0] + 'ï¼š', tgt_parts[1])
    
    return None, None

def should_attach_to_previous(text: str) -> bool:
    """ì• êµ¬ì— ë¶™ì–´ì•¼ í•˜ëŠ” í‘œí˜„ì¸ì§€ í™•ì¸ (ë‹¨ìˆœí™”ëœ íŒ¨í„´ + MeCab í™œìš©)"""
    
    if not text or not text.strip():
        return False
    
    text = text.strip()
    
    # 1. í•µì‹¬ íŒ¨í„´ë“¤ë§Œ í™•ì¸ (í•˜ë“œì½”ë”© ìµœì†Œí™”)
    if text.startswith(('ë¼ê³  ', 'ë¼ëŠ” ', 'ë¼ë©°', 'ë¼ë©´ì„œ')):
        return True
    
    # 2. MeCabì„ ì´ìš©í•œ í’ˆì‚¬ ê¸°ë°˜ íŒë‹¨
    if mecab:
        try:
            result = mecab.parse(text)
            morphemes = []
            
            for line in result.split('\n'):
                if line and line != 'EOS':
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        surface = parts[0]
                        pos = parts[1].split(',')[0]
                        morphemes.append((surface, pos))
            
            if morphemes:
                # ì²« ë²ˆì§¸ í˜•íƒœì†Œê°€ ì¸ìš© ì¡°ì‚¬ì¸ ê²½ìš°
                first_surface, first_pos = morphemes[0]
                if first_pos == 'JKQ' and first_surface in ['ë¼ê³ ', 'ë¼ëŠ”']:  # ì¸ìš©ê²©ì¡°ì‚¬
                    return True
                
                # ì—°ê²°ì–´ë¯¸ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
                if first_pos == 'EC' and first_surface in ['ë¼ë©°', 'ë¼ë©´ì„œ']:
                    return True
        
        except Exception as e:
            logger.debug(f"MeCab ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return False

# ë°œí™”ë™ì‚¬ ì›í˜•ë“¤ (í•µì‹¬ë§Œ, ì‹œì œëŠ” MeCabìœ¼ë¡œ ì²˜ë¦¬)
# í•µì‹¬ ë°œí™”ë™ì‚¬ ì›í˜• (ìµœì†Œí•œìœ¼ë¡œ ì¶•ì†Œ)
CORE_QUOTATIVE_LEMMAS = {
    'ë§í•˜ë‹¤', 'ë¬»ë‹¤', 'ë‹µí•˜ë‹¤', 'ì´ë¥´ë‹¤'  # ê°€ì¥ ê¸°ë³¸ì ì¸ ë°œí™”ë™ì‚¬ë§Œ
}

# MeCab ì˜ë¯¸ ë¶„ë¥˜ë¥¼ í™œìš©í•œ ë°œí™”ë™ì‚¬ íƒì§€ìš© í‚¤ì›Œë“œ
COMMUNICATION_KEYWORDS = {
    'ë§', 'ì–¸', 'ë‹µ', 'ë¬¸', 'ë¬¼', 'ì´ì•¼ê¸°', 'ì–˜ê¸°', 'ë…¼', 'ì„¤ëª…', 'í‘œí˜„', 
    'ì§„ìˆ ', 'ì„œìˆ ', 'ë°œì–¸', 'ì–¸ê¸‰', 'í‰', 'ì¹­', 'ë¶€ë¥´'
}

def detect_quotative_boundary_advanced(text: str) -> bool:
    """MeCab ì›í˜• ë³µì› ë° ì˜ë¯¸ ì¶”ë¡ ì„ í™œìš©í•œ ë°œí™”ë™ì‚¬ íƒì§€"""
    
    if not mecab or not text.strip():
        return False
    
    try:
        # MeCab ë¶„ì„ (ì›í˜• ì •ë³´ í¬í•¨)
        result = mecab.parse(text.strip())
        morphemes = []
        
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    features = parts[1].split(',')
                    pos = features[0]
                    
                    # ì›í˜• ì •ë³´ ì¶”ì¶œ (MeCab ì¶œë ¥ êµ¬ì¡°ì— ë”°ë¼)
                    lemma = features[6] if len(features) > 6 and features[6] != '*' else surface
                    
                    morphemes.append({
                        'surface': surface,
                        'pos': pos,
                        'lemma': lemma,
                        'features': features
                    })
        
        if not morphemes:
            return False
        
        # ë¬¸ì¥ ëë¶€ë¶„ì—ì„œ ë°œí™”ë™ì‚¬ íŒ¨í„´ ì°¾ê¸°
        for i in range(max(0, len(morphemes) - 4), len(morphemes)):
            morph = morphemes[i]
            
            # 1. ë™ì‚¬ì¸ì§€ í™•ì¸
            if morph['pos'] in ['VV', 'VX']:
                lemma = morph['lemma']
                surface = morph['surface']
                
                # 2-1. í•µì‹¬ ë°œí™”ë™ì‚¬ ì›í˜• í™•ì¸ (í™•ì‹¤í•œ ê²½ìš°)
                if lemma in CORE_QUOTATIVE_LEMMAS:
                    return True
                
                # 2-2. ì˜ë¯¸ì  ì¶”ë¡ : ì†Œí†µ/ëŒ€í™” ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
                for keyword in COMMUNICATION_KEYWORDS:
                    if keyword in lemma or keyword in surface:
                        # 3. ì¢…ê²°ì–´ë¯¸ë‚˜ ì—°ê²°ì–´ë¯¸ê°€ ë’¤ë”°ë¥´ëŠ”ì§€ í™•ì¸
                        if i + 1 < len(morphemes):
                            next_morph = morphemes[i + 1]
                            if next_morph['pos'] in ['EF', 'EC']:  # ì¢…ê²°ì–´ë¯¸ or ì—°ê²°ì–´ë¯¸
                                return True
                        return True  # ë™ì‚¬ ìì²´ë¡œë„ íŒë‹¨
                
                # 2-3. ì–´ê°„ íŒ¨í„´ ë¶„ì„ (ë³´ì¡°ì )
                verb_stem = lemma[:-1] if lemma.endswith('ë‹¤') else lemma
                if len(verb_stem) >= 2:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                    for keyword in COMMUNICATION_KEYWORDS:
                        if verb_stem.startswith(keyword) or verb_stem.endswith(keyword):
                            return True
        
        return False
        
    except Exception as e:
        logger.debug(f"ë°œí™”ë™ì‚¬ íƒì§€ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def detect_sentence_ending_type(text: str) -> str:
    """ë¬¸ì¥ ì¢…ê²° ìœ í˜• íƒì§€ - ë°œí™”ë™ì‚¬ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´"""
    
    if not mecab or not text.strip():
        return 'unknown'
    
    try:
        result = mecab.parse(text.strip())
        
        # ë§ˆì§€ë§‰ ëª‡ ê°œ í˜•íƒœì†Œ í™•ì¸
        morphemes = []
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    features = parts[1].split(',')
                    morphemes.append((surface, features[0], features))
        
        if not morphemes:
            return 'unknown'
        
        # ì¢…ê²°ì–´ë¯¸ í™•ì¸
        for surface, pos, features in reversed(morphemes[-3:]):
            if pos == 'EF':  # ì¢…ê²°ì–´ë¯¸
                if len(features) > 1:
                    # ì¢…ê²°ì–´ë¯¸ ì„¸ë¶€ ìœ í˜•
                    ending_type = features[1]
                    if ending_type in ['í‰ì„œ', 'ì˜ë¬¸', 'ëª…ë ¹', 'ì²­ìœ ']:
                        return ending_type
                return 'declarative'  # ê¸°ë³¸ê°’
        
        return 'unknown'
        
    except Exception as e:
        logger.debug(f"ë¬¸ì¥ ì¢…ê²° ìœ í˜• íƒì§€ ì¤‘ ì˜¤ë¥˜: {e}")
        return 'unknown'

def is_quotative_end_pattern(text: str) -> bool:
    """ë°œí™”ë™ì‚¬ ê¸°ë°˜ ì¸ìš© ë íƒì§€ - ì›í˜• ë³µì› í™œìš©"""
    
    # 1. ê¸°ë³¸ì ì¸ ë¬¸ì¥ ì¢…ê²° í™•ì¸
    if not text.strip().endswith(('.', 'ã€‚', '?', 'ï¼Ÿ', '!', 'ï¼')):
        return False
    
    # 2. MeCab ê¸°ë°˜ ë°œí™”ë™ì‚¬ íƒì§€ (ê°œì„ ëœ ë²„ì „)
    return detect_quotative_boundary_advanced(text)

def is_discourse_marker(text: str) -> bool:
    """ë‹´í™” í‘œì§€ íƒì§€ - MeCab í’ˆì‚¬ ì •ë³´ ì¤‘ì‹¬ì˜ ì¼ë°˜í™”ëœ ì ‘ê·¼"""
    
    if not mecab or not text.strip():
        return False
    
    # 1. ë°œí™”ë™ì‚¬ë„ ë‹´í™” í‘œì§€ì˜ ì¼ì¢…
    if detect_quotative_boundary_advanced(text):
        return True
    
    # 2. MeCabìœ¼ë¡œ ë‹´í™” ê¸°ëŠ¥ í’ˆì‚¬ í™•ì¸
    try:
        result = mecab.parse(text.strip())
        
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    features = parts[1].split(',')
                    pos = features[0]
                    
                    # ë‹´í™” ê¸°ëŠ¥ì„ í•˜ëŠ” í’ˆì‚¬ë“¤
                    if pos in ['MAJ', 'IC', 'JC'] and len(surface) >= 2:  # ì ‘ì†ë¶€ì‚¬, ê°íƒ„ì‚¬, ì ‘ì†ì¡°ì‚¬
                        return True
                    
                    # ì ‘ì† ì˜ë¯¸ì˜ ë¶€ì‚¬
                    if pos == 'MAG' and len(surface) >= 3:  # ì¼ë°˜ë¶€ì‚¬ ì¤‘ ê¸´ ê²ƒë“¤ (ì ‘ì† ê¸°ëŠ¥ ê°€ëŠ¥ì„±)
                        # ì ‘ì† ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
                        connection_hints = ['ê·¸ëŸ¬', 'í•˜ì§€', 'ë”°ë¼', 'ê·¸ë˜', 'ì¦‰', 'ë˜', 'ê²Œë‹¤']
                        if any(hint in surface for hint in connection_hints):
                            return True
                            
    except Exception as e:
        logger.debug(f"ë‹´í™” í‘œì§€ íƒì§€ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return False