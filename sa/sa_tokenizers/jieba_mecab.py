"""ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ëª¨ë“ˆ - jieba & MeCab í™œìš©"""

import logging
import numpy as np
import regex
import re
import itertools
from typing import List, Callable
import jieba
import MeCab
import os

logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •ê°’
DEFAULT_MIN_TOKENS = 1
DEFAULT_MAX_TOKENS = 50
DEFAULT_SIMILARITY_THRESHOLD = 0.7

# jiebaì™€ MeCab ì´ˆê¸°í™”
try:
    # ì‚¬ìš©ì ì‚¬ì „ ê²½ë¡œë¥¼ .venv/Scripts/user.dicë¡œ ì§€ì •
    mecabrc_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir/mecabrc'
    dicdir_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir'
    userdic_path = 'C:/Users/junto/Downloads/head-repo/CSP/.venv/Lib/site-packages/mecab_ko_dic/dicdir/user.dic'
    mecab = MeCab.Tagger(f'-r {mecabrc_path} -d {dicdir_path} -u {userdic_path}')
    print("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ")
    logger.info("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ") # -dëŠ” ì‚¬ì „ ë””ë ‰í† ë¦¬, -uëŠ” ì‚¬ìš©ì ì‚¬ì „ ê²½ë¡œ
except Exception as e:
    print(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    logger.warning(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    mecab = None

# ë¯¸ë¦¬ ì»´íŒŒì¼ëœ ì •ê·œì‹
hanja_re = regex.compile(r'\p{Han}+')
hangul_re = regex.compile(r'^\p{Hangul}+$')

def split_src_meaning_units(
    text: str,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    by_space: bool = False,
    **kwargs
):
    """ì›ë¬¸(í•œë¬¸+í•œê¸€)ì„ jiebaì™€ MeCabìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• """
    
    # 1ë‹¨ê³„: ì–´ì ˆ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ì–´ì ˆ ë‚´ë¶€ëŠ” ì ˆëŒ€ ìª¼ê°œì§€ì§€ ì•ŠìŒ)
    # ì „ê° ì½œë¡  ë’¤ì—ë§Œ ê³µë°±ì„ ì¶”ê°€í•˜ì—¬ "ì „ìš´(ç®‹äº‘)ï¼š" + "ê°ˆëŒ€ëŠ”" í˜•íƒœë¡œ ë¶„í• 
    words = text.replace('\n', ' ').replace('ï¼š', 'ï¼š ').split()
    if not words:
        return []
    
    # 2ë‹¨ê³„: jiebaì™€ MeCab ë¶„ì„ ê²°ê³¼ ì¤€ë¹„
    jieba_tokens = list(jieba.cut(text))
    
    # MeCab ë¶„ì„ (í•œê¸€ ë¶€ë¶„ìš©)
    morpheme_info = []
    if mecab:
        result = mecab.parse(text)
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    pos = parts[1].split(',')[0]
                    morpheme_info.append((surface, pos))
    
    # 3ë‹¨ê³„: ì–´ì ˆë“¤ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™” (jieba + MeCab ì •ë³´ í™œìš©)
    units = []
    i = 0
    
    while i < len(words):
        word = words[i]
        
        # í•œì í¬í•¨ ì–´ì ˆ ì²˜ë¦¬
        if hanja_re.search(word):
            # í˜„ì¬ ì–´ì ˆì´ í•œìë¥¼ í¬í•¨í•˜ë©´ í•˜ë‚˜ì˜ ì˜ë¯¸ ë‹¨ìœ„
            units.append(word)
            i += 1
            continue
        
        # í•œê¸€ ì–´ì ˆë“¤ ì²˜ë¦¬ - jiebaì™€ MeCab ë¶„ì„ ê²°ê³¼ ëª¨ë‘ ì°¸ê³ 
        if hangul_re.match(word):
            group = [word]
            j = i + 1
            
            # ì¤‘ì„¸êµ­ì–´ ì–´ë¯¸ë‚˜ ë¬¸ë²• í‘œì§€ë¡œ ê²½ê³„ íŒë‹¨ (ì›ë¬¸ìš©)
            should_break_here = _should_break_by_mecab_src(word, morpheme_info) if morpheme_info else False
            
            # jieba í† í° ì—°ì†ì„±ë„ í™•ì¸ (ê²½ê³„ ì‹ í˜¸ê°€ ì—†ëŠ” ê²½ìš°ë§Œ)
            if not should_break_here:
                while j < len(words) and hangul_re.match(words[j]):
                    should_group = _should_group_words_by_jieba(group + [words[j]], jieba_tokens)
                    if should_group:
                        group.append(words[j])
                        j += 1
                    else:
                        break
            
            units.append(' '.join(group))
            i = j
            continue
        
        # ê¸°íƒ€ ì–´ì ˆ (ìˆ«ì, êµ¬ë‘ì  ë“±)
        units.append(word)
        i += 1
    
    return units

def _should_group_words_by_jieba(word_group: List[str], jieba_tokens: List[str]) -> bool:
    """jieba ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ì„œ ì–´ì ˆë“¤ì„ ë¬¶ì„ì§€ ê²°ì •"""
    combined = ''.join(word_group)
    
    # jieba í† í° ì¤‘ì—ì„œ í˜„ì¬ ì¡°í•©ê³¼ ì¼ì¹˜í•˜ëŠ” ê²ƒì´ ìˆìœ¼ë©´ ë¬¶ê¸°
    for token in jieba_tokens:
        if token.replace(' ', '') == combined.replace(' ', ''):
            return True
    
    # ê¸¸ì´ ì œí•œ
    if len(combined) > 10:
        return False
    
    return len(word_group) <= 3

def split_inside_chunk(chunk: str) -> List[str]:
    """ë²ˆì—­ë¬¸ ì²­í¬ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  - MeCab ë¶„ì„ ì°¸ê³  (ê°œì„ ëœ ë²„ì „)"""
    
    if not chunk or not chunk.strip():
        return []
    
    # 1ë‹¨ê³„: ì–´ì ˆ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ì–´ì ˆ ë‚´ë¶€ëŠ” ì ˆëŒ€ ìª¼ê°œì§€ì§€ ì•ŠìŒ)
    # ì „ê° ì½œë¡  ë’¤ì—ë§Œ ê³µë°±ì„ ì¶”ê°€í•˜ì—¬ "ì „ìš´(ç®‹äº‘)ï¼š" + "ê°ˆëŒ€ëŠ”" í˜•íƒœë¡œ ë¶„í• 
    words = chunk.replace('ï¼š', 'ï¼š ').split()
    if not words:
        return []
    
    # 2ë‹¨ê³„: MeCab ë¶„ì„ ê²°ê³¼ ì°¸ê³ 
    morpheme_info = []
    if mecab:
        result = mecab.parse(chunk)
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    pos = parts[1].split(',')[0]
                    morpheme_info.append((surface, pos))
    
    # 3ë‹¨ê³„: MeCab ë¶„ì„ ê²°ê³¼ë¥¼ í™œìš©í•œ ì˜ë¯¸ ë‹¨ìœ„ ê·¸ë£¹í™”
    units = []
    current_group = []
    
    for word in words:
        current_group.append(word)
        
        # ì „ê° ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ë‹¨ì–´ëŠ” ì¦‰ì‹œ ë‹¨ìœ„ ì™„ì„± (í•˜ë“œ ê²½ê³„)
        if word.endswith('ï¼š') or word == 'ï¼š':
            units.append(' '.join(current_group))
            current_group = []
            continue
        
        # MeCab ë¶„ì„ ê²°ê³¼ë¡œ ê²½ê³„ íŒë‹¨ (í’ˆì‚¬ ì •ë³´ í™œìš©)
        should_break = _should_break_by_mecab(word, morpheme_info) if morpheme_info else False
        
        if should_break and current_group:
            units.append(' '.join(current_group))
            current_group = []
    
    if current_group:
        units.append(' '.join(current_group))
    
    return [unit.strip() for unit in units if unit.strip()]

def _should_break_by_mecab(word: str, morpheme_info: List[tuple]) -> bool:
    """MeCab ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ì„œ ì˜ë¯¸ ë‹¨ìœ„ ê²½ê³„ ê²°ì • - ë³´ì¡°ì‚¬(JX) ê°•í™”"""
    
    # MeCab ë¶„ì„ ê²°ê³¼ í™•ì¸
    for surface, pos in morpheme_info:
        # ë‹¨ì–´ê°€ í•´ë‹¹ í˜•íƒœì†Œë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸ (ë” ì •í™•í•œ ë§¤ì¹­)
        if word.endswith(surface):
            # ê°•í•œ ê²½ê³„ ì‹ í˜¸ - ì¢…ê²°ì–´ë¯¸, êµ¬ë‘ì 
            if pos in ['EF', 'SF', 'SP']:
                return True
            
            # ë³´ì¡°ì‚¬(JX) - ë§¤ìš° ì¤‘ìš”í•œ ë¬¸ë²•ì  í‘œì§€ë¡œ ê°•í™” ì²˜ë¦¬
            if pos == 'JX':
                return True  # ëª¨ë“  ë³´ì¡°ì‚¬ì—ì„œ ë¶„í• 
            
            # ì£¼ìš” ì¡°ì‚¬ë“¤ - ì˜ë¯¸ ë‹¨ìœ„ ê²½ê³„
            if pos in ['JKS', 'JKO', 'JKC', 'JKB', 'JKG', 'JKV', 'JKQ']:
                return True  # ëª¨ë“  ì¡°ì‚¬ì—ì„œ ë¶„í• 
            
            # ì—°ê²°ì–´ë¯¸(EC) - ë¬¸ì¥ ì—°ê²°
            if pos == 'EC':
                return True  # ëª¨ë“  ì—°ê²°ì–´ë¯¸ì—ì„œ ë¶„í• 
            
            # ëª…ì‚¬í˜• ì „ì„±ì–´ë¯¸(ETN) - ëª…ì‚¬í™”
            if pos == 'ETN':
                return True
            
            # ê´€í˜•í˜• ì „ì„±ì–´ë¯¸(ETM) - ê´€í˜•ì–´í™”
            if pos == 'ETM':
                return True
            
            # ë™ì‚¬, í˜•ìš©ì‚¬ ì–´ê°„ ë‹¤ìŒì—ì„œ ê²½ê³„  
            if pos in ['VV', 'VA', 'VX']:
                return len(surface) >= 1  # ê¸¸ì´ 1 ì´ìƒì¸ ìš©ì–¸ ì–´ê°„ì—ì„œ ë¶„í• 
            
            # ì¤‘ìš”í•œ ë¶€ì‚¬ì—ì„œ ë¶„í•  (MAG, MAJ)
            if pos in ['MAG', 'MAJ'] and len(surface) >= 2:
                return True  # ê¸¸ì´ 2 ì´ìƒì¸ ë¶€ì‚¬ì—ì„œ ë¶„í• 
    
    return False

def find_target_span_end_simple(src_unit: str, remaining_tgt: str) -> int:
    """ê°„ë‹¨í•œ íƒ€ê²Ÿ ìŠ¤íŒ¬ íƒìƒ‰"""
    hanja_chars = regex.findall(r'\p{Han}+', src_unit)
    if not hanja_chars:
        return 0
    last = hanja_chars[-1]
    idx = remaining_tgt.rfind(last)
    if idx == -1:
        return len(remaining_tgt)
    end = idx + len(last)
    next_space = remaining_tgt.find(' ', end)
    return next_space + 1 if next_space != -1 else len(remaining_tgt)

def find_target_span_end_semantic(
    src_unit: str,
    remaining_tgt: str,
    embed_func: Callable,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD
) -> int:
    """ìµœì í™”ëœ íƒ€ê²Ÿ ìŠ¤íŒ¬ íƒìƒ‰ í•¨ìˆ˜"""
    if not src_unit or not remaining_tgt:
        return 0
        
    try:
        # 1) ì›ë¬¸ ì„ë² ë”© (ë‹¨ì¼ ê³„ì‚°)
        src_emb = embed_func([src_unit])[0]
        
        # 2) ë²ˆì—­ë¬¸ í† í° ë¶„ë¦¬ ë° ëˆ„ì  ê¸¸ì´ ê³„ì‚°
        tgt_tokens = remaining_tgt.split()
        if not tgt_tokens:
            return 0
            
        upper = min(len(tgt_tokens), max_tokens)
        cumulative_lengths = [0]
        current_length = 0
        
        for tok in tgt_tokens:
            current_length += len(tok) + 1
            cumulative_lengths.append(current_length)
            
        # 3) í›„ë³´ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„±
        candidates = []
        candidate_indices = []
        
        step_size = 1 if upper <= 10 else 2
        
        for end_i in range(min_tokens-1, upper, step_size):
            cand = " ".join(tgt_tokens[:end_i+1])
            candidates.append(cand)
            candidate_indices.append(end_i)
            
        # 4) ë°°ì¹˜ ì„ë² ë”©
        cand_embs = embed_func(candidates)
        
        # 5) ìµœì  ë§¤ì¹­ íƒìƒ‰
        best_score = -1.0
        best_end_idx = cumulative_lengths[-1]
        
        for i, emb in enumerate(cand_embs):
            score = np.dot(src_emb, emb) / (np.linalg.norm(src_emb) * np.linalg.norm(emb) + 1e-8)
            
            end_i = candidate_indices[i]
            length_ratio = (end_i + 1) / len(tgt_tokens)
            length_penalty = min(1.0, length_ratio * 2)
            
            adjusted_score = score * length_penalty
            
            if adjusted_score > best_score and score >= similarity_threshold:
                best_score = adjusted_score
                best_end_idx = cumulative_lengths[end_i + 1]
                
        return best_end_idx
        
    except Exception as e:
        logger.warning(f"ì˜ë¯¸ ë§¤ì¹­ ì˜¤ë¥˜, ë‹¨ìˆœ ë§¤ì¹­ìœ¼ë¡œ ëŒ€ì²´: {e}")
        return find_target_span_end_simple(src_unit, remaining_tgt)

def split_tgt_by_src_units(src_units: List[str], tgt_text: str) -> List[str]:
    """ì›ë¬¸ ë‹¨ìœ„ì— ë”°ë¥¸ ë²ˆì—­ë¬¸ ë¶„í•  (ë‹¨ìˆœ ë°©ì‹)"""
    results = []
    cursor = 0
    total = len(tgt_text)
    for src_u in src_units:
        remaining = tgt_text[cursor:]
        end_len = find_target_span_end_simple(src_u, remaining)
        chunk = tgt_text[cursor:cursor+end_len]
        results.extend(split_inside_chunk(chunk))
        cursor += end_len
    if cursor < total:
        results.extend(split_inside_chunk(tgt_text[cursor:]))
    return results

def split_tgt_by_src_units_semantic(
    src_units: List[str], 
    tgt_text: str, 
    embed_func: Callable, 
    min_tokens: int = DEFAULT_MIN_TOKENS
) -> List[str]:
    """ì›ë¬¸ ë‹¨ìœ„ì— ë”°ë¥¸ ë²ˆì—­ë¬¸ ë¶„í•  (ì˜ë¯¸ ê¸°ë°˜, ì „ì—­ ë§¤ì¹­)"""
    
    # 1ë‹¨ê³„: ì „ê° ì½œë¡ ì„ í•˜ë“œ ê²½ê³„ë¡œ ì²˜ë¦¬
    if 'ï¼š' in tgt_text:
        colon_parts = tgt_text.split('ï¼š')
        if len(colon_parts) == 2:
            part1 = colon_parts[0].strip() + 'ï¼š'
            part2 = colon_parts[1].strip()
            result = [part1]
            if len(src_units) > 1:
                remaining_src = src_units[1:]
                remaining_parts = split_tgt_by_src_units_semantic(
                    remaining_src, part2, embed_func, min_tokens
                )
                result.extend(remaining_parts)
            else:
                result.append(part2)
            return result
    
    # 2ë‹¨ê³„: ë²ˆì—­ë¬¸ì„ ë¨¼ì € ìì—°ìŠ¤ëŸ¬ìš´ ë‹¨ìœ„ë¡œ ë¶„í• 
    tgt_chunks = split_inside_chunk(tgt_text)
    if not tgt_chunks or len(src_units) == 0:
        return tgt_chunks if tgt_chunks else []
    
    # 3ë‹¨ê³„: ì˜ë¯¸ ê¸°ë°˜ ì „ì—­ ë§¤ì¹­
    if len(src_units) == len(tgt_chunks):
        # 1:1 ëŒ€ì‘ì¸ ê²½ìš° ì˜ë¯¸ ìœ ì‚¬ë„ë¡œ ìµœì  ë§¤ì¹­ ì°¾ê¸°
        return _find_optimal_semantic_matching(src_units, tgt_chunks, embed_func)
    elif len(src_units) == 1:
        # ì›ë¬¸ì´ í•˜ë‚˜ì¸ ê²½ìš° - ë²ˆì—­ë¬¸ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê±°ë‚˜ DP ë§¤ì¹­ ì‚¬ìš©
        if len(tgt_chunks) <= 3:  # ì‘ì€ ê°œìˆ˜ë©´ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
            return [tgt_text.strip()]
        else:
            # ë§ì€ ê°œìˆ˜ë©´ DP ë§¤ì¹­ ì‚¬ìš©
            return _dp_semantic_matching(src_units, tgt_text, embed_func, min_tokens)
    elif len(tgt_chunks) == 1:
        # ë²ˆì—­ë¬¸ì´ í•˜ë‚˜ì¸ ê²½ìš° - ì›ë¬¸ ê°œìˆ˜ë§Œí¼ ë¶„í•  ì‹œë„
        return _split_single_target_to_multiple(src_units, tgt_chunks[0], embed_func)
    else:
        # ê°œìˆ˜ê°€ ë‹¤ë¥¸ ê²½ìš° DP ë§¤ì¹­ ì‚¬ìš©
        return _dp_semantic_matching(src_units, tgt_text, embed_func, min_tokens)

def _find_optimal_semantic_matching(src_units: List[str], tgt_chunks: List[str], embed_func: Callable) -> List[str]:
    """ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ ì²­í¬ ê°„ì˜ ìµœì  ì˜ë¯¸ ë§¤ì¹­ ì°¾ê¸° (ê°œì„ ëœ ë²„ì „)"""
    import itertools
    
    if len(src_units) != len(tgt_chunks):
        return tgt_chunks
    
    try:
        # ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ ì„ë² ë”© ê³„ì‚°
        normalized_src = [_normalize_for_embedding(src) for src in src_units]
        normalized_tgt = [_normalize_for_embedding(tgt) for tgt in tgt_chunks]
        
        src_embs = embed_func(normalized_src)
        tgt_embs = embed_func(normalized_tgt)
        
        # ëª¨ë“  ê°€ëŠ¥í•œ ë§¤ì¹­ì— ëŒ€í•´ ì¢…í•© ìœ ì‚¬ë„ ê³„ì‚°
        best_score = -1
        best_permutation = list(range(len(tgt_chunks)))
        
        for perm in itertools.permutations(range(len(tgt_chunks))):
            total_score = 0
            for i, j in enumerate(perm):
                # 1. ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚° (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
                sim = float(np.dot(src_embs[i], tgt_embs[j]) / 
                          (np.linalg.norm(src_embs[i]) * np.linalg.norm(tgt_embs[j]) + 1e-8))
                
                # 2. í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ (í•œì, ê³ ìœ ëª…ì‚¬ ë“±)
                keyword_bonus = _calculate_keyword_bonus(src_units[i], tgt_chunks[j])
                
                # 3. ë¬¸ë²•ì  ê²½ê³„ ë³´ë„ˆìŠ¤
                grammar_bonus = _calculate_grammar_bonus(tgt_chunks[j])
                
                # 4. êµ¬ë¬¸ êµ¬ì¡° ë§¤ì¹­ ë³´ë„ˆìŠ¤
                structure_bonus = _calculate_structure_bonus(src_units[i], tgt_chunks[j])
                
                # 5. ê¸¸ì´ ê· í˜• ë³´ë„ˆìŠ¤ (ë„ˆë¬´ ë¶ˆê· í˜•í•œ ë§¤ì¹­ ë°©ì§€)
                length_bonus = _calculate_length_balance_bonus(src_units[i], tgt_chunks[j])
                
                total_score += (sim * 1.0 + keyword_bonus * 0.8 + grammar_bonus * 0.6 + 
                               structure_bonus * 0.5 + length_bonus * 0.3)
            
            if total_score > best_score:
                best_score = total_score
                best_permutation = perm
        
        # ìµœì  ë§¤ì¹­ ìˆœì„œë¡œ ë°˜í™˜
        return [tgt_chunks[i] for i in best_permutation]
        
    except Exception as e:
        logger.warning(f"ì˜ë¯¸ ë§¤ì¹­ ì‹¤íŒ¨, ì›ë³¸ ìˆœì„œ ìœ ì§€: {e}")
        return tgt_chunks

def _calculate_keyword_bonus(src_unit: str, tgt_chunk: str) -> float:
    """í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤ ê³„ì‚° - ë‹¨ìˆœí™”ëœ ë²„ì „"""
    bonus = 0.0
    
    # 1. í•œì ì¶”ì¶œ
    src_hanja = regex.findall(r'\p{Han}+', src_unit)
    
    # 2. í•œì ì§ì ‘ ë§¤ì¹­
    for hanja in src_hanja:
        if hanja in tgt_chunk:
            bonus += 0.5  # í•œì ì§ì ‘ ë§¤ì¹­
            if len(hanja) >= 2:
                bonus += 0.2  # ê¸´ í•œìì–´ ë³´ë„ˆìŠ¤
    
    # 3. ê¸°ë³¸ì ì¸ ë¬¸ë²• í‘œì§€ ë§¤ì¹­ë§Œ ìœ ì§€
    if 'è€…' in src_unit and any(marker in tgt_chunk for marker in ['ê²ƒ', 'ì', 'ë¼ëŠ”']):
        bonus += 0.3
    
    if 'ä¹Ÿ' in src_unit and any(marker in tgt_chunk for marker in ['ë‹¤', 'ì´ë‹¤', 'ê²ƒì´ë‹¤']):
        bonus += 0.3
    
    return min(bonus, 1.5)  # ìµœëŒ€ê°’ ì œí•œ


def _calculate_structure_bonus(src_unit: str, tgt_chunk: str) -> float:
    """êµ¬ë¬¸ êµ¬ì¡° ë§¤ì¹­ ë³´ë„ˆìŠ¤ ê³„ì‚° - ë‹¨ìˆœí™”ëœ ë²„ì „"""
    bonus = 0.0
    
    # 1. êµ¬ë‘ì  ìˆ˜ ë§¤ì¹­ë§Œ ìœ ì§€
    src_punct = len(re.findall(r'[,ï¼Œ.ã€‚!ï¼?ï¼Ÿ:ï¼š;ï¼›]', src_unit))
    tgt_punct = len(re.findall(r'[,ï¼Œ.ã€‚!ï¼?ï¼Ÿ:ï¼š;ï¼›]', tgt_chunk))
    
    if src_punct == tgt_punct and src_punct > 0:
        bonus += 0.2
    
    # 2. ê´„í˜¸ êµ¬ì¡° ë§¤ì¹­
    src_parens = src_unit.count('(') + src_unit.count('ï¼ˆ')
    tgt_parens = tgt_chunk.count('(') + tgt_chunk.count('ï¼ˆ')
    
    if src_parens == tgt_parens and src_parens > 0:
        bonus += 0.1
    
    return bonus

def _calculate_length_balance_bonus(src_unit: str, tgt_chunk: str) -> float:
    """ê¸¸ì´ ê· í˜• ë³´ë„ˆìŠ¤ ê³„ì‚° (ë„ˆë¬´ ë¶ˆê· í˜•í•œ ë§¤ì¹­ ë°©ì§€)"""
    src_len = len(src_unit.strip())
    tgt_len = len(tgt_chunk.strip())
    
    if src_len == 0 or tgt_len == 0:
        return -0.5  # ë¹ˆ ë¬¸ìì—´ í˜ë„í‹°
    
    # ê¸¸ì´ ë¹„ìœ¨ ê³„ì‚°
    ratio = min(src_len, tgt_len) / max(src_len, tgt_len)
    
    # ì ì ˆí•œ ê¸¸ì´ ë¹„ìœ¨ì— ë³´ë„ˆìŠ¤ (0.3 ~ 1.0 ì‚¬ì´ê°€ ì ì ˆ)
    if ratio >= 0.5:
        return 0.2 * ratio  # ê· í˜• ì¡íŒ ê¸¸ì´ì— ë³´ë„ˆìŠ¤
    elif ratio >= 0.2:
        return 0.1 * ratio  # ì•½ê°„ ë¶ˆê· í˜•í•œ ê²½ìš° ì‘ì€ ë³´ë„ˆìŠ¤
    else:
        return -0.1  # ë„ˆë¬´ ë¶ˆê· í˜•í•œ ê²½ìš° í˜ë„í‹°

def _dp_semantic_matching(src_units: List[str], tgt_text: str, embed_func: Callable, min_tokens: int) -> List[str]:
    """DP ê¸°ë°˜ ì˜ë¯¸ ë§¤ì¹­ (ê¸°ì¡´ ë¡œì§)"""
    # ê¸°ì¡´ DP ë¡œì§ ìœ ì§€ (ë°±ì—…ìš©)
    tgt_tokens = tgt_text.replace('ï¼š', 'ï¼š ').split()
    N, T = len(src_units), len(tgt_tokens)
    if N == 0 or T == 0:
        return []

    dp = np.full((N+1, T+1), -np.inf)
    back = np.zeros((N+1, T+1), dtype=int)
    dp[0, 0] = 0.0

    # ì›ë¬¸ ë‹¨ìœ„ë“¤ì„ ì •ê·œí™”í•˜ì—¬ ì„ë² ë”© ê³„ì‚°
    normalized_src_units = [_normalize_for_embedding(unit) for unit in src_units]
    src_embs = embed_func(normalized_src_units)

    # ëª¨ë“  í›„ë³´ span ìˆ˜ì§‘
    span_map = {}
    all_spans = []
    for i in range(1, N+1):
        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):
            for k in range((i-1)*min_tokens, j-min_tokens+1):
                span = " ".join(tgt_tokens[k:j]).strip()
                key = (k, j)
                if span and key not in span_map:
                    span_map[key] = span
                    all_spans.append(span)
    
    all_spans = list(set(all_spans))

    # ë°°ì¹˜ ì„ë² ë”©
    def batch_embed(spans, batch_size=100):
        results = []
        for i in range(0, len(spans), batch_size):
            batch_spans = spans[i:i+batch_size]
            normalized_batch = [_normalize_for_embedding(span) for span in batch_spans]
            results.extend(embed_func(normalized_batch))
        return results
    
    span_embs = batch_embed(all_spans)
    span_emb_dict = {span: emb for span, emb in zip(all_spans, span_embs)}

    # DP ê³„ì‚° (ê°œì„ ëœ ì˜ë¯¸ ìœ ì‚¬ë„ + ë‹¤ì¤‘ ë³´ë„ˆìŠ¤)
    for i in range(1, N+1):
        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):
            for k in range((i-1)*min_tokens, j-min_tokens+1):
                span = span_map[(k, j)]
                tgt_emb = span_emb_dict[span]
                
                # 1. ê¸°ë³¸ ì˜ë¯¸ ìœ ì‚¬ì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„)
                sim = float(np.dot(src_embs[i-1], tgt_emb)/((np.linalg.norm(src_embs[i-1])*np.linalg.norm(tgt_emb))+1e-8))
                
                # 2. í‚¤ì›Œë“œ ë§¤ì¹­ ë³´ë„ˆìŠ¤
                keyword_bonus = _calculate_keyword_bonus(src_units[i-1], span)
                
                # 3. ë¬¸ë²•ì  ê²½ê³„ ë³´ë„ˆìŠ¤
                grammar_bonus = _calculate_grammar_bonus(span)
                
                # 4. êµ¬ë¬¸ êµ¬ì¡° ë§¤ì¹­ ë³´ë„ˆìŠ¤
                structure_bonus = _calculate_structure_bonus(src_units[i-1], span)
                
                # 5. ê¸¸ì´ ê· í˜• ë³´ë„ˆìŠ¤
                length_bonus = _calculate_length_balance_bonus(src_units[i-1], span)
                
                # ê°€ì¤‘ì¹˜ ì ìš©í•œ ìµœì¢… ì ìˆ˜
                final_score = (sim * 1.0 + keyword_bonus * 0.8 + grammar_bonus * 0.6 + 
                              structure_bonus * 0.5 + length_bonus * 0.3)
                
                score = dp[i-1, k] + final_score
                
                if score > dp[i, j]:
                    dp[i, j] = score
                    back[i, j] = k

    # ì—­ì¶”ì 
    cuts = [T]
    curr = T
    for i in range(N, 0, -1):
        prev = int(back[i, curr])
        cuts.append(prev)
        curr = prev
    cuts = cuts[::-1]

    tgt_spans = []
    for i in range(N):
        span = " ".join(tgt_tokens[cuts[i]:cuts[i+1]]).strip()
        tgt_spans.append(span)
    return tgt_spans

def split_tgt_meaning_units(
    src_text: str,
    tgt_text: str,
    use_semantic: bool = True,
    min_tokens: int = DEFAULT_MIN_TOKENS,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    embed_func: Callable = None
) -> List[str]:
    """ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
    # ì§€ì—° ì„í¬íŠ¸ë¡œ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
    if embed_func is None:
        from sa_embedders import compute_embeddings_with_cache  # ğŸ”§ ìˆ˜ì •
        embed_func = compute_embeddings_with_cache
        
    src_units = split_src_meaning_units(src_text, min_tokens, max_tokens)

    if use_semantic:
        return split_tgt_by_src_units_semantic(
            src_units,
            tgt_text,
            embed_func=embed_func,
            min_tokens=min_tokens
        )
    else:
        return split_tgt_by_src_units(src_units, tgt_text)

def tokenize_text(text):
    """í˜•íƒœì†Œ ë¶„ì„ ë° í† í°í™” - MeCab ì‚¬ìš©"""
    if mecab:
        result = mecab.parse(text)
        tokens = []
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 1:
                    tokens.append(parts[0])
        return tokens
    else:
        return text.split()

def pos_tag_text(text):
    """í’ˆì‚¬ íƒœê¹… - MeCab ì‚¬ìš©"""
    if mecab:
        result = mecab.parse(text)
        pos_tags = []
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    pos = parts[1].split(',')[0]
                    pos_tags.append((surface, pos))
        return pos_tags
    else:
        return [(word, 'UNKNOWN') for word in text.split()]

def sentence_split(text):
    """ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"""
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]+', text)
    return [s.strip() for s in sentences if s.strip()]

def normalize_for_embedding(text: str) -> str:
    """ì„ë² ë”© ê³„ì‚°ì„ ìœ„í•´ í…ìŠ¤íŠ¸ ì •ê·œí™” - ì „ê° ì½œë¡  ë“± êµ¬ë‘ì  ì œê±°"""
    # ì „ê° ì½œë¡ ê³¼ ê´„í˜¸ ë“±ì„ ì œê±°í•˜ì—¬ ì˜ë¯¸ ë§¤ì¹­ì— ì§‘ì¤‘
    normalized = text.replace('ï¼š', '').replace('(', '').replace(')', '')
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì •ë¦¬
    normalized = ' '.join(normalized.split())
    return normalized

def _normalize_for_embedding(text: str) -> str:
    """ì„ë² ë”© ê³„ì‚°ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì •ê·œí™” - ì „ê° ì½œë¡  ì œê±°"""
    return text.replace('ï¼š', '').strip()

def _calculate_grammar_bonus(span: str) -> float:
    """ë¬¸ë²•ì  ê²½ê³„ì— ëŒ€í•œ ë³´ë„ˆìŠ¤ ì ìˆ˜ ê³„ì‚° - MeCab ê¸°ë°˜ ë‹¨ìˆœí™” ë²„ì „"""
    span = span.strip()
    bonus = 0.0
    
    # 1. ì „ê° ì½œë¡ ìœ¼ë¡œ ëë‚˜ëŠ” ê²½ìš° ê°•í•œ ë³´ë„ˆìŠ¤
    if span.endswith('ï¼š'):
        return 0.8
    
    # 2. MeCabì„ ì´ìš©í•œ ì •í™•í•œ ì–´ë¯¸/ì¡°ì‚¬ ë¶„ì„
    if mecab:
        try:
            result = mecab.parse(span)
            last_pos = None
            for line in result.split('\n'):
                if line and line != 'EOS':
                    parts = line.split('\t')
                    if len(parts) >= 2:
                        pos_detail = parts[1].split(',')
                        last_pos = pos_detail[0]
            
            # í’ˆì‚¬ë³„ ë³´ë„ˆìŠ¤ - ë‹¨ìˆœí™”
            if last_pos == 'EF':  # ì¢…ê²°ì–´ë¯¸
                bonus = 0.5
            elif last_pos == 'EC':  # ì—°ê²°ì–´ë¯¸
                bonus = 0.4
            elif last_pos == 'JX':  # ë³´ì¡°ì‚¬
                bonus = 0.4
            elif last_pos in ['JKS', 'JKO', 'JKB', 'JKC']:  # ì£¼ìš” ì¡°ì‚¬
                bonus = 0.3
            elif last_pos in ['ETN', 'ETM']:  # ì „ì„±ì–´ë¯¸
                bonus = 0.3
        except:
            pass
    
    # 3. ê¸°ë³¸ êµ¬ë‘ì  ì²˜ë¦¬
    if span.endswith(('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ')):
        bonus = max(bonus, 0.4)
    elif span.endswith((',', 'ï¼Œ', ';', 'ï¼›')):
        bonus = max(bonus, 0.2)
        
    return min(bonus, 1.0)

def _split_single_target_to_multiple(src_units: List[str], single_tgt: str, embed_func: Callable) -> List[str]:
    """ë‹¨ì¼ ë²ˆì—­ë¬¸ì„ ì—¬ëŸ¬ ì›ë¬¸ ë‹¨ìœ„ì— ë§ê²Œ ë¶„í• """
    # ì›ë¬¸ì´ ì—¬ëŸ¬ ê°œì´ê³  ë²ˆì—­ë¬¸ì´ í•˜ë‚˜ì¸ ê²½ìš°
    # ë²ˆì—­ë¬¸ì„ ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„ì—ì„œ ë¶„í• í•˜ì—¬ ì›ë¬¸ ê°œìˆ˜ì— ë§ì¶¤
    
    # ë¨¼ì € ìì—°ìŠ¤ëŸ¬ìš´ ë¶„í•  ì‹œë„
    natural_splits = split_inside_chunk(single_tgt)
    
    if len(natural_splits) >= len(src_units):
        # ìì—° ë¶„í• ì´ ì¶©ë¶„í•œ ê²½ìš°, ì˜ë¯¸ì ìœ¼ë¡œ ê°€ì¥ ì í•©í•œ ì¡°í•© ì°¾ê¸°
        return _merge_splits_to_match_src_count(src_units, natural_splits, embed_func)
    else:
        # ìì—° ë¶„í• ì´ ë¶€ì¡±í•œ ê²½ìš°, ê°•ì œ ë¶„í• 
        return _force_split_by_semantic_boundaries(src_units, single_tgt, embed_func)

def _merge_splits_to_match_src_count(src_units: List[str], tgt_splits: List[str], embed_func: Callable) -> List[str]:
    """ë²ˆì—­ë¬¸ ë¶„í• ì„ ì›ë¬¸ ê°œìˆ˜ì— ë§ê²Œ ë³‘í•© - ë‹¨ìˆœí™”ëœ ë²„ì „"""
    if len(src_units) >= len(tgt_splits):
        return tgt_splits
    
    # ë„ˆë¬´ ë§ì´ splitëœ ê²½ìš° ì•ì—ì„œë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ë³‘í•©
    current_splits = tgt_splits[:]
    
    while len(current_splits) > len(src_units):
        # ê°€ì¥ ì§§ì€ ì¸ì ‘í•œ ë‘ ë¶„í• ì„ ë³‘í•©
        best_merge_idx = 0
        min_combined_length = float('inf')
        
        for i in range(len(current_splits) - 1):
            combined_length = len(current_splits[i]) + len(current_splits[i + 1])
            if combined_length < min_combined_length:
                min_combined_length = combined_length
                best_merge_idx = i
        
        # ë³‘í•© ì‹¤í–‰
        merged_text = current_splits[best_merge_idx] + ' ' + current_splits[best_merge_idx + 1]
        current_splits = (current_splits[:best_merge_idx] + 
                         [merged_text] + 
                         current_splits[best_merge_idx + 2:])
    
    return current_splits

def _force_split_by_semantic_boundaries(src_units: List[str], single_tgt: str, embed_func: Callable) -> List[str]:
    """ì˜ë¯¸ì  ê²½ê³„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê°•ì œ ë¶„í•  - ë‹¨ìˆœí™”ëœ ë²„ì „"""
    tokens = single_tgt.split()
    if len(tokens) <= len(src_units):
        return [single_tgt]  # í† í°ì´ ë¶€ì¡±í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    # ë‹¨ìˆœí•˜ê²Œ í† í°ì„ ê±°ì˜ ê· ë“±í•˜ê²Œ ë¶„í• 
    tokens_per_unit = len(tokens) // len(src_units)
    remainder = len(tokens) % len(src_units)
    
    result = []
    start = 0
    
    for i in range(len(src_units)):
        # ë‚˜ë¨¸ì§€ê°€ ìˆìœ¼ë©´ ì•ìª½ ë‹¨ìœ„ë“¤ì— í•˜ë‚˜ì”© ë” ë°°ë¶„
        current_size = tokens_per_unit + (1 if i < remainder else 0)
        end = start + current_size
        
        if end > len(tokens):
            end = len(tokens)
        
        if start < end:
            segment = ' '.join(tokens[start:end]).strip()
            if segment:
                result.append(segment)
        
        start = end
    
    # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ ê²ƒì„ ë°˜í™˜
    if not result:
        result = [single_tgt]
    
    return result

def _should_break_by_mecab_src(word: str, morpheme_info: List[tuple]) -> bool:
    """ì›ë¬¸ìš© - MeCab ë¶„ì„ ê²°ê³¼ + ì¤‘ì„¸êµ­ì–´ ì–´ë¯¸ íŒ¨í„´ìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ ê²½ê³„ ê²°ì •"""
    
    # 1. ì¤‘ì„¸êµ­ì–´ ì–´ë¯¸ íŒ¨í„´ í™•ì¸ (ì›ë¬¸ì—ë§Œ ì ìš©)
    middle_korean_endings = [
        'ë‹ˆë¼', 'ë…¸ë¼', 'ë„ë‹¤', 'ë¡œë‹¤', 'ê°€ë‹¤', 'ê±°ë‹¤',  # ì¢…ê²°ì–´ë¯¸
        'ë ¤ë‹ˆì™€', 'ê±°ë‹ˆì™€', 'ë¡œë˜', 'ë˜',              # ì—°ê²°ì–´ë¯¸
        'ê±´ëŒ„', 'ê±´ëŒ€', 'ì–´ë‹ˆ', 'ê±°ë‹ˆ',               # ì—°ê²°ì–´ë¯¸
        'í•˜ë‹ˆ', 'í•˜ë˜', 'í•˜ì—¬', 'í•˜ì•¼',               # ì—°ê²°ì–´ë¯¸
        'ì´ë‹ˆ', 'ì´ë¡œ', 'ì´ë©°', 'ì´ë©´',               # ì—°ê²°ì–´ë¯¸
        'ì€ì¦‰', 'ì¦‰', 'ë©´', 'ë‹ˆ',                    # ì—°ê²°ì–´ë¯¸
        'ë¼ë„', 'ë§ˆëŠ”', 'ë‚˜ë§ˆ', 'ë ¤ë§ˆëŠ”'              # ë³´ì¡°ì‚¬/ì—°ê²°ì–´ë¯¸
    ]
    
    for ending in middle_korean_endings:
        if word.endswith(ending):
            return True
    
    # 2. ì¼ë°˜ì ì¸ MeCab ë¶„ì„ ê²°ê³¼ í™•ì¸ (ë²ˆì—­ë¬¸ê³¼ ë™ì¼)
    return _should_break_by_mecab(word, morpheme_info)