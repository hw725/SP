"""SA í† í¬ë‚˜ì´ì € ëª¨ë“ˆ íŒ¨í‚¤ì§€"""

from .jieba_mecab import (
    # ì£¼ìš” ë¶„í•  í•¨ìˆ˜ë“¤
    split_tgt_meaning_units,  # ë²ˆì—­ë¬¸ ë¶„í•  (ê¸°ì¡´ í˜¸í™˜ì„±)
    split_tgt_meaning_units_sequential,  # ğŸ†• ìˆœì°¨ ë¶„í•  ë°©ì‹ (ë©”ì¸)
    split_tgt_by_src_units,  # ë‹¨ìˆœ ë¶„í• 
    split_tgt_by_src_units_semantic,  # ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  (ìˆœì°¨ë¡œ ëŒ€ì²´ë¨)
    
    # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤  
    tokenize_text,
    pos_tag_text,
    sentence_split,
    
    # ğŸ†• ë¬¸ë²•ì  í‘œì§€ ê´€ë ¨ í•¨ìˆ˜ë“¤
    is_boundary_marker,
    get_boundary_strength
)
from .bert_tokenizer import split_src_meaning_units as bert_split_src_meaning_units, split_src_sentences as bert_split_src_sentences

def split_src_meaning_units(text: str, *args, **kwargs):
    """ì›ë¬¸(í•œë¬¸)ì€ ë¬´ì¡°ê±´ jiebaë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  (tokenizer ì¸ì ë¬´ì‹œ)"""
    from .jieba_mecab import split_src_meaning_units as jieba_split
    return jieba_split(text, *args, **kwargs)

__all__ = [
    # ì£¼ìš” ë¶„í•  í•¨ìˆ˜ë“¤
    'split_src_meaning_units',
    'split_tgt_meaning_units', 
    'split_tgt_meaning_units_sequential',  # ğŸ†• ìˆœì°¨ ë¶„í•  (ë©”ì¸)
    'split_tgt_by_src_units',
    'split_tgt_by_src_units_semantic',
    
    # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬
    'tokenize_text',
    'pos_tag_text', 
    'sentence_split',
    
    # ğŸ†• ë¬¸ë²•ì  í‘œì§€ í•¨ìˆ˜ë“¤
    'is_boundary_marker',
    'get_boundary_strength'
]