"""SA í† í¬ë‚˜ì´ì € ëª¨ë“ˆ íŒ¨í‚¤ì§€"""

from .jieba_mecab import (
    split_tgt_meaning_units,  # ë²ˆì—­ë¬¸(mecab)
    split_tgt_meaning_units_sequential,  # ğŸ†• ìˆœì°¨ ë¶„í•  ë°©ì‹
    tokenize_text,
    pos_tag_text,
    sentence_split,
    split_tgt_by_src_units_semantic,
    split_tgt_by_src_units  # ë‹¨ìˆœ ë¶„í•  í•¨ìˆ˜ ì¶”ê°€
)
from .bert_tokenizer import split_src_meaning_units as bert_split_src_meaning_units, split_src_sentences as bert_split_src_sentences

def split_src_meaning_units(text: str, *args, **kwargs):
    """ì›ë¬¸(í•œë¬¸)ì€ ë¬´ì¡°ê±´ jiebaë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  (tokenizer ì¸ì ë¬´ì‹œ)"""
    from .jieba_mecab import split_src_meaning_units as jieba_split
    return jieba_split(text, *args, **kwargs)

__all__ = [
    'split_src_meaning_units',
    'split_tgt_meaning_units', 
    'split_tgt_meaning_units_sequential',  # ğŸ†• ìˆœì°¨ ë¶„í•  ì¶”ê°€
    'split_tgt_by_src_units_semantic',
    'split_tgt_by_src_units',
    'tokenize_text',
    'pos_tag_text',
    'sentence_split'
]