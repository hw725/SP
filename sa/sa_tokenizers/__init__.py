"""SA í† í¬ë‚˜ì´ì € ëª¨ë“ˆ íŒ¨í‚¤ì§€"""

from .jieba_mecab import (
    # [ACTIVE] í˜„ì¬ ì‚¬ìš©ë˜ëŠ” í•µì‹¬ í•¨ìˆ˜ë“¤
    split_src_meaning_units,              # ì›ë¬¸ ë¶„í•  (jieba ê¸°ë°˜)
    split_tgt_meaning_units_sequential,   # ğŸ†• ë²ˆì—­ë¬¸ ìˆœì°¨ ë¶„í•  (ë©”ì¸ í•¨ìˆ˜)
    split_tgt_by_src_units_semantic,      # ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  (í´ë°±ìš©)
    
    # [ACTIVE] ë¬¸ë²•ì  í‘œì§€ ê´€ë ¨ í•¨ìˆ˜ë“¤
    is_boundary_marker,
    get_boundary_strength,
    should_attach_to_previous,
    
    # [ACTIVE] ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤  
    tokenize_text,
    pos_tag_text,
    sentence_split,
    split_inside_chunk,
    
    # [DEPRECATED] í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€
    split_tgt_meaning_units,              # â†’ split_tgt_meaning_units_sequentialë¡œ ëŒ€ì²´ë¨
    split_tgt_by_src_units,               # â†’ split_tgt_by_src_units_semanticìœ¼ë¡œ ëŒ€ì²´ë¨
)

# BERT í† í¬ë‚˜ì´ì € (ì„ íƒì  ì‚¬ìš©)
try:
    from .bert_tokenizer import (
        split_src_meaning_units as bert_split_src_meaning_units, 
        split_src_sentences as bert_split_src_sentences
    )
except ImportError:
    # BERT ì˜ì¡´ì„±ì´ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
    bert_split_src_meaning_units = None
    bert_split_src_sentences = None

__all__ = [
    # [ACTIVE] í•µì‹¬ ë¶„í•  í•¨ìˆ˜ë“¤
    'split_src_meaning_units',
    'split_tgt_meaning_units_sequential',  # ğŸ†• ë©”ì¸ ë²ˆì—­ë¬¸ ë¶„í•  í•¨ìˆ˜
    'split_tgt_by_src_units_semantic',
    
    # [ACTIVE] ë¬¸ë²•ì  í‘œì§€ í•¨ìˆ˜ë“¤
    'is_boundary_marker',
    'get_boundary_strength', 
    'should_attach_to_previous',
    
    # [ACTIVE] ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬
    'tokenize_text',
    'pos_tag_text', 
    'sentence_split',
    'split_inside_chunk',
    
    # [DEPRECATED] í•˜ìœ„ í˜¸í™˜ì„±
    'split_tgt_meaning_units',
    'split_tgt_by_src_units',
]