"""SA ì •ë ¬ ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ê¸°"""

import argparse
import logging
import time
import sys
import os
from typing import Optional

import sys
sys.stdout.reconfigure(encoding='utf-8')
from core.io_utils import IOManager

def setup_logging(verbose: bool = False):
    """ë¡œê¹… ì„¤ì •"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s:%(name)s:%(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('sa_processing.log', encoding='utf-8')
        ]
    )

def get_tokenizer_module(tokenizer_name: str):
    """í† í¬ë‚˜ì´ì € ëª¨ë“ˆ ë™ì  ë¡œë“œ - jieba(ì›ë¬¸), mecab(ë²ˆì—­ë¬¸)ë§Œ ì§€ì›"""
    tokenizer_map = {
        'jieba': 'core.tokenizers.jieba_mecab',
        'mecab': 'core.tokenizers.jieba_mecab',
    }
    if tokenizer_name not in tokenizer_map:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í† í¬ë‚˜ì´ì €: {tokenizer_name}. ì§€ì›: jieba(ì›ë¬¸), mecab(ë²ˆì—­ë¬¸)")
    module_name = tokenizer_map[tokenizer_name]
    try:
        import importlib
        return importlib.import_module(module_name)
    except ImportError as e:
        raise ImportError(f"í† í¬ë‚˜ì´ì € ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {module_name} ({e})")

def get_embedder_module(embedder_name: str):
    """ì„ë² ë” ëª¨ë“ˆ ë™ì  ë¡œë“œ"""
    embedder_map = {
        'openai': 'core.embedders.openai',
        'bge': 'core.embedders.bge',
    }
    
    if embedder_name not in embedder_map:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”: {embedder_name}. ì§€ì›: openai, bge")
    
    module_name = embedder_map[embedder_name]
    
    try:
        import importlib
        return importlib.import_module(module_name)
    except ImportError as e:
        raise ImportError(f"ì„ë² ë” ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {module_name} ({e})")

def process_single_file(
    input_file: str,
    output_file: str,
    tokenizer_name: str = 'jieba',
    embedder_name: str = 'bge',  # ê¸°ë³¸ê°’ì„ bgeë¡œ ë³€ê²½
    use_semantic: bool = True,
    min_tokens: int = 1,
    max_tokens: int = 10,
    parallel: bool = True,  # ë³‘ë ¬ ì²˜ë¦¬ ê¸°ë³¸ê°’ Trueë¡œ ë³€ê²½
    openai_model: str = "text-embedding-3-large",
    openai_api_key: str = None,
    progress_callback=None,
    stop_flag=None,
    **kwargs
) -> bool:
    """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ (ë³‘ë ¬ ì˜µì…˜ ì§€ì›)"""
    import time
    start_time = time.time()  # â±ï¸ ì²˜ë¦¬ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    print(f"ğŸš€ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file}")
    print(f"ğŸ“Š ì„¤ì •:")
    print(f"   í† í¬ë‚˜ì´ì €: {tokenizer_name}")
    print(f"   ì„ë² ë”: {embedder_name}")
    print(f"   ì˜ë¯¸ ë§¤ì¹­: {use_semantic}")
    print(f"   ë³‘ë ¬ ì²˜ë¦¬: {parallel} (ê¸°ë³¸ê°’: True)")
    print(f"   í† í° ë²”ìœ„: {min_tokens}-{max_tokens}")
    try:
        if parallel:
            print("âš¡ ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            from core.processor import process_file
            # ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ - embedder_name ì „ë‹¬
            results_df = process_file(
                input_file,
                use_semantic=use_semantic,
                min_tokens=min_tokens,
                max_tokens=max_tokens,
                save_results=True,
                output_file=output_file,
                openai_model=openai_model,
                openai_api_key=openai_api_key,
                embedder_name=embedder_name
            )
            if results_df is not None:
                print(f"ğŸ‰ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ! ê²°ê³¼: {len(results_df)}ê°œ êµ¬")
                return True
            else:
                print(f"âŒ ë³‘ë ¬ ì²˜ë¦¬ ì‹¤íŒ¨")
                return False
        # í•­ìƒ ë™ì  ëª¨ë“ˆ ë¡œë”© ê²½ë¡œ ì‚¬ìš©
        print("âœ… ë™ì  ëª¨ë“ˆ ë¡œë”©...")
        tokenizer_module = get_tokenizer_module(tokenizer_name)
        embedder_module = get_embedder_module(embedder_name)
        print(f"âœ… ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
        from core.processor import process_file_with_modules
        results = process_file_with_modules(
            input_file, output_file,
            tokenizer_module, embedder_module,
            embedder_name,  # ì¶”ê°€!
            use_semantic, min_tokens, max_tokens,
            openai_model=openai_model,
            openai_api_key=openai_api_key
        )

        end_time = time.time()  # â±ï¸ ì²˜ë¦¬ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡

        if results is not None:
            print(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
            print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {len(results)}ê°œ ë¬¸ì¥")
            print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")

            # ê¸°ë³¸ í˜•ì‹ ì €ì¥
            output_file_basic = output_file

            # êµ¬ ë‹¨ìœ„ í˜•ì‹ ì €ì¥
            output_file_phrase = output_file.replace('.xlsx', '_phrase.xlsx').replace('.csv', '_phrase.csv').replace('.txt', '_phrase.txt')

            from sa.io_utils import save_phrase_format_results
            save_phrase_format_results(results, output_file_phrase)

            print(f"ğŸ“ ê¸°ë³¸ ì¶œë ¥: {output_file_basic}")
            print(f"ğŸ“ êµ¬ ë‹¨ìœ„ ì¶œë ¥: {output_file_phrase}")

            return True
        else:
            print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨")
            return False

    except Exception as e:
        print(f"ğŸ’¥ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False

def check_dependencies():
    """ì˜ì¡´ì„± ë° í™˜ê²½ ì ê²€
    - í•„ìˆ˜ íŒ¨í‚¤ì§€, torch, transformers, sentence-transformers, mecab-python3 ë“±
    - ì£¼ìš” ê²½ë¡œ/í™˜ê²½ë³€ìˆ˜
    """
    missing = []
    try:
        import pandas
    except ImportError:
        missing.append("pandas")
    try:
        import numpy
    except ImportError:
        missing.append("numpy")
    try:
        import torch
    except ImportError:
        missing.append("torch")
    try:
        import transformers
    except ImportError:
        missing.append("transformers")
    try:
        import sentence_transformers
    except ImportError:
        missing.append("sentence_transformers")
    try:
        import mecab
    except ImportError:
        print("\u26a0\ufe0f mecab-python3ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. mecab ê´€ë ¨ ê¸°ëŠ¥ì€ ë™ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    if missing:
        print(f"\u274c í•„ìˆ˜ íŒ¨í‚¤ì§€ ëˆ„ë½: {', '.join(missing)}")
        print("ì„¤ì¹˜ ëª…ë ¹: pip install " + " ".join(missing))
        return False
    return True

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    if not check_dependencies():
        print("\u274c ì˜ì¡´ì„±/í™˜ê²½ ì ê²€ ì‹¤íŒ¨. í•„ìˆ˜ íŒ¨í‚¤ì§€ ë˜ëŠ” ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
    parser = argparse.ArgumentParser(
        description='SA ì •ë ¬ ì‹œìŠ¤í…œ - ë¬¸ì¥ ë‹¨ìœ„ í† í° ì •ë ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì²˜ë¦¬
  python main.py input.xlsx output.xlsx
  
  # í† í¬ë‚˜ì´ì €/ì„ë² ë” ì§€ì •
  python main.py input.xlsx output.xlsx --tokenizer soy --embedder openai
  
  # ë³‘ë ¬ ì²˜ë¦¬
  python main.py input.xlsx output.xlsx --parallel
  
  # ìƒì„¸ ì„¤ì •
  python main.py input.xlsx output.xlsx --tokenizer jieba --embedder bge --min-tokens 2 --max-tokens 15 --no-semantic
  
ì§€ì› í† í¬ë‚˜ì´ì €: jieba, soy, kkma
ì§€ì› ì„ë² ë”: openai, bge
        """
    )
    
    # í•„ìˆ˜ ì¸ì
    parser.add_argument('input_file', help='ì…ë ¥ Excel íŒŒì¼ (ë¬¸ì¥ì‹ë³„ì, ì›ë¬¸, ë²ˆì—­ë¬¸ ì»¬ëŸ¼)')
    parser.add_argument('output_file', help='ì¶œë ¥ Excel íŒŒì¼')
    
    # ì„ íƒ ì¸ì
    parser.add_argument('--tokenizer', '-t', default='jieba', 
                       choices=['jieba', 'mecab'],
                       help='í† í¬ë‚˜ì´ì € ì„ íƒ (jieba: ì›ë¬¸, mecab: ë²ˆì—­ë¬¸, ê¸°ë³¸: jieba)')
    
    parser.add_argument('--embedder', '-e', default='bge',
                       choices=['openai', 'bge'], 
                       help='ì„ë² ë” ì„ íƒ (ê¸°ë³¸: bge)')
    
    parser.add_argument('--parallel', '-p', action='store_true', default=True,
                       help='ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” (ê¸°ë³¸: í™œì„±í™”)')
    
    parser.add_argument('--no-semantic', action='store_true',
                       help='ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­ ë¹„í™œì„±í™”')
    
    parser.add_argument('--min-tokens', type=int, default=1,
                       help='ìµœì†Œ í† í° ìˆ˜ (ê¸°ë³¸: 1)')
    
    parser.add_argument('--max-tokens', type=int, default=10,
                       help='ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 10)')
    
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥')
    
    parser.add_argument('--openai-model', default="text-embedding-3-large", help='OpenAI ì„ë² ë”© ëª¨ë¸ëª…')
    parser.add_argument('--openai-api-key', default=None, help='OpenAI API í‚¤')
    parser.add_argument('--save-phrase', action='store_true', default=True, help='êµ¬ ë‹¨ìœ„ ê²°ê³¼ë„ ì €ì¥')

    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    setup_logging(args.verbose)
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.input_file):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input_file}")
        sys.exit(1)
    
    # ì²˜ë¦¬ ì‹¤í–‰
    success = process_single_file(
        input_file=args.input_file,
        output_file=args.output_file,
        tokenizer_name=args.tokenizer,
        embedder_name=args.embedder,
        use_semantic=not args.no_semantic,
        min_tokens=args.min_tokens,
        max_tokens=args.max_tokens,
        parallel=args.parallel,
        openai_model=args.openai_model,
        openai_api_key=args.openai_api_key
        # progress_callback, stop_flag ì¸ì ì œê±°
    )
    
    if success:
        print("\nğŸ‰ ì²˜ë¦¬ ì„±ê³µ!")
        sys.exit(0)
    else:
        print("\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨!")
        sys.exit(1)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SA: Sentence Aligner")
    parser.add_argument('input_file', help='ì…ë ¥ íŒŒì¼ (Excel)')
    parser.add_argument('output_file', help='ì¶œë ¥ íŒŒì¼ (Excel)')
    parser.add_argument('--tokenizer', default='jieba', help='ì›ë¬¸ í† í¬ë‚˜ì´ì € (ê¸°ë³¸: jieba)')
    parser.add_argument('--embedder', default='bge', help='ì„ë² ë” (ê¸°ë³¸: bge)')
    parser.add_argument('--parallel', '-p', action='store_true', default=True, help='ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™” (ê¸°ë³¸: í™œì„±í™”)')
    parser.add_argument('--no-semantic', action='store_true', help='ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­ ë¹„í™œì„±í™”')
    parser.add_argument('--min-tokens', type=int, default=1, help='ìµœì†Œ í† í° ìˆ˜ (ê¸°ë³¸: 1)')
    parser.add_argument('--max-tokens', type=int, default=10, help='ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 10)')
    parser.add_argument('--verbose', '-v', action='store_true', help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥')
    parser.add_argument('--openai-model', default="text-embedding-3-large", help='OpenAI ì„ë² ë”© ëª¨ë¸ëª…')
    parser.add_argument('--openai-api-key', default=None, help='OpenAI API í‚¤')
    parser.add_argument('--save-phrase', action='store_true', default=True, help='êµ¬ ë‹¨ìœ„ ê²°ê³¼ë„ ì €ì¥')
    args = parser.parse_args()
    # ë¡œê¹… ì„¤ì •
    setup_logging(args.verbose)
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.input_file):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input_file}")
        sys.exit(1)
    
    # ì²˜ë¦¬ ì‹¤í–‰
    success = process_single_file(
        input_file=args.input_file,
        output_file=args.output_file,
        tokenizer_name=args.tokenizer,
        embedder_name=args.embedder,
        use_semantic=not args.no_semantic,
        min_tokens=args.min_tokens,
        max_tokens=args.max_tokens,
        parallel=args.parallel,
        openai_model=args.openai_model,
        openai_api_key=args.openai_api_key
        # progress_callback, stop_flag ì¸ì ì œê±°
    )
    
    if success:
        print("\nğŸ‰ ì²˜ë¦¬ ì„±ê³µ!")
        sys.exit(0)
    else:
        print("\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨!")
        sys.exit(1)