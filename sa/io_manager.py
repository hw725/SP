"""ê°œì„ ëœ ë³‘ë ¬ ì²˜ë¦¬ - ì‘ì—… ë‹¨ìœ„ ë¶„ì‚°"""
try:
    import pandas as pd
except ImportError as e:
    import logging
    logging.error(f"\u274c pandas import ì‹¤íŒ¨: {e}")
    pd = None
try:
    from tqdm import tqdm
except ImportError as e:
    import logging
    logging.error(f"\u274c tqdm import ì‹¤íŒ¨: {e}")
    def tqdm(x, *args, **kwargs):
        return x
import logging
from pathlib import Path
import multiprocessing as mp
from typing import Dict, Any, List
import math
import sys, os

CSP_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if CSP_ROOT not in sys.path:
    sys.path.insert(0, CSP_ROOT)

logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ (ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ì´ˆê¸°í™”)
worker_embed_func = None
worker_modules = {}
verbose_mode = False  # ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”

def set_verbose_mode(verbose=False):
    """ì „ì—­ verbose ëª¨ë“œ ì„¤ì •"""
    global verbose_mode
    verbose_mode = verbose

def init_worker(device_id=None, embedder_name='bge'):
    """ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì´ˆê¸°í™” - í•œ ë²ˆë§Œ ì‹¤í–‰ (device_idë¡œ GPU ë¶„ë°°, ì„ë² ë” ì„ íƒ)"""
    global worker_embed_func, worker_modules
    try:
        # jinja2 ê´€ë ¨ ì˜¤ë¥˜ ìš°íšŒ - ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½ì—ì„œ jinja2.tests ëª¨ë“ˆ ëˆ„ë½ ë¬¸ì œ í•´ê²°
        import sys
        try:
            import jinja2.tests
        except (ImportError, ModuleNotFoundError):
            # jinja2.tests ëª¨ë“ˆì„ mockìœ¼ë¡œ ìƒì„±í•˜ì—¬ ì„í¬íŠ¸ ì˜¤ë¥˜ ë°©ì§€
            import types
            mock_module = types.ModuleType('jinja2.tests')
            sys.modules['jinja2.tests'] = mock_module
            try:
                import jinja2
                jinja2.tests = mock_module
            except:
                pass
        
        if verbose_mode:
            print(f"ì›Œì»¤ {mp.current_process().pid}: ì´ˆê¸°í™” ì‹œì‘ (device_id={device_id}, embedder={embedder_name})")
        
        # ì„ë² ë” ì´ˆê¸°í™” - ì„ë² ë” íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ í•¨ìˆ˜ ì‚¬ìš©
        if embedder_name == 'openai':
            from sa_embedders import get_embedder
            worker_embed_func = get_embedder('openai')
            if verbose_mode:
                print(f"ì›Œì»¤ {mp.current_process().pid}: OpenAI ì„ë² ë” ì´ˆê¸°í™” ì™„ë£Œ")
        else:  # bge ë˜ëŠ” ê¸°ë³¸ê°’
            from sa_embedders import get_embed_func
            worker_embed_func = get_embed_func(device_id=device_id)
            if verbose_mode:
                print(f"ì›Œì»¤ {mp.current_process().pid}: BGE ì„ë² ë” ì´ˆê¸°í™” ì™„ë£Œ")
        
        # í•„ìš”í•œ ëª¨ë“ˆë“¤ ì„í¬íŠ¸ (í•µì‹¬ í•¨ìˆ˜ë§Œ)
        from sa_tokenizers.jieba_mecab import (
            split_src_meaning_units, 
            split_tgt_meaning_units_sequential,  # ë©”ì¸ ë²ˆì—­ë¬¸ ë¶„í•  í•¨ìˆ˜
            split_tgt_by_src_units_semantic     # í´ë°±ìš©
        )
        from punctuation import mask_brackets, restore_brackets
        
        worker_modules = {
            'split_src_meaning_units': split_src_meaning_units,
            'split_tgt_meaning_units_sequential': split_tgt_meaning_units_sequential,
            'split_tgt_by_src_units_semantic': split_tgt_by_src_units_semantic,
            'mask_brackets': mask_brackets,
            'restore_brackets': restore_brackets
        }
        
        if verbose_mode:
            print(f"ì›Œì»¤ {mp.current_process().pid}: ì´ˆê¸°í™” ì™„ë£Œ (device_id={device_id})")
        
    except Exception as e:
        # ì´ˆê¸°í™” ì‹¤íŒ¨ëŠ” ì¤‘ìš”í•œ ì˜¤ë¥˜ì´ë¯€ë¡œ í•­ìƒ ì¶œë ¥
        print(f"ì›Œì»¤ {mp.current_process().pid}: ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        worker_embed_func = None
        worker_modules = {}

def process_batch_sentences(sentence_batch: List[Dict[str, Any]], device_id=None) -> List[Dict[str, Any]]:
    """ë°°ì¹˜ ë‹¨ìœ„ ë¬¸ì¥ ì²˜ë¦¬ - í•œ í”„ë¡œì„¸ìŠ¤ê°€ ì—¬ëŸ¬ ë¬¸ì¥ ì²˜ë¦¬"""
    global worker_embed_func, worker_modules
    
    if worker_embed_func is None or not worker_modules:
        # ì¤‘ìš” ì˜¤ë¥˜ëŠ” í•­ìƒ ì¶œë ¥
        print(f"ì›Œì»¤ {mp.current_process().pid}: ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ")
        return []
    
    results = []
    
    for sentence_data in sentence_batch:
        try:
            sentence_id = sentence_data['sentence_id']
            src_text = sentence_data['src_text']
            tgt_text = sentence_data['tgt_text']
            
            # ëª¨ë“ˆë“¤ ê°€ì ¸ì˜¤ê¸°
            mask_brackets = worker_modules['mask_brackets']
            restore_brackets = worker_modules['restore_brackets']
            split_src_meaning_units = worker_modules['split_src_meaning_units']
            split_tgt_by_src_units_semantic = worker_modules['split_tgt_by_src_units_semantic']
            split_tgt_meaning_units_sequential = worker_modules['split_tgt_meaning_units_sequential']
            
            # ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
            masked_src, src_masks = mask_brackets(src_text, 'source')
            masked_tgt, tgt_masks = mask_brackets(tgt_text, 'target')
            
            src_units = split_src_meaning_units(masked_src)
            # ğŸ†• ì˜ë¯¸ ê¸°ë°˜ ìˆœì°¨ ë¶„í•  (ì„ë² ë”© í•¨ìˆ˜ ì „ë‹¬)
            tgt_units = split_tgt_meaning_units_sequential(
                masked_src,  # ì›ë¬¸ë„ ì „ë‹¬
                masked_tgt, 
                min_tokens=1,
                embed_func=worker_embed_func  # ì„ë² ë”© í•¨ìˆ˜ ì „ë‹¬
            )
            
            # ê²°ê³¼ ìƒì„±
            for phrase_idx, (src_unit, tgt_unit) in enumerate(zip(src_units, tgt_units), 1):
                restored_src = restore_brackets(src_unit, src_masks)
                restored_tgt = restore_brackets(tgt_unit, tgt_masks)
                
                results.append({
                    'ë¬¸ì¥ì‹ë³„ì': sentence_id,
                    'êµ¬ì‹ë³„ì': phrase_idx,
                    'ì›ë¬¸': restored_src,
                    'ë²ˆì—­ë¬¸': restored_tgt
                })
            
        except Exception as e:
            if verbose_mode:
                print(f"ì›Œì»¤ {mp.current_process().pid}: ë¬¸ì¥ {sentence_data.get('sentence_id', '?')} ì‹¤íŒ¨: {e}")
            logger.error(f"ë¬¸ì¥ {sentence_data.get('sentence_id', '?')} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            continue
    
    if verbose_mode:
        print(f"ì›Œì»¤ {mp.current_process().pid}: ë°°ì¹˜ {len(sentence_batch)}ê°œ ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ â†’ {len(results)}ê°œ êµ¬")
    
    return results

def process_file(input_path: str, output_path: str, parallel: bool = False, workers: int = 4, 
                batch_size: int = 20, device_ids=None, verbose: bool = False, embedder_name: str = 'bge'):
    """ê°œì„ ëœ ë³‘ë ¬ ì²˜ë¦¬ (ì„ë² ë” ì„ íƒ ì§€ì›)"""
    
    # verbose ëª¨ë“œ ì„¤ì •
    set_verbose_mode(verbose)
    
    # ë°ì´í„° ë¡œë“œ
    logger.info(f"íŒŒì¼ ë¡œë“œ: {input_path}")
    
    try:
        if input_path.endswith('.xlsx'):
            df = pd.read_excel(input_path)
        else:
            df = pd.read_csv(input_path)
    except Exception as e:
        logger.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    logger.info(f"ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰")
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_columns = ['ë¬¸ì¥ì‹ë³„ì', 'ì›ë¬¸', 'ë²ˆì—­ë¬¸']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
    
    # ë°ì´í„° ì¤€ë¹„
    sentence_data_list = []
    for idx, row in df.iterrows():
        src_text = str(row['ì›ë¬¸']).strip()
        tgt_text = str(row['ë²ˆì—­ë¬¸']).strip()
        
        if src_text and tgt_text:
            sentence_data_list.append({
                'sentence_id': row['ë¬¸ì¥ì‹ë³„ì'],
                'src_text': src_text,
                'tgt_text': tgt_text
            })
    
    logger.info(f"ì²˜ë¦¬í•  ë¬¸ì¥: {len(sentence_data_list)}ê°œ")
    
    # ì²˜ë¦¬ ì‹¤í–‰
    results = []
    
    if parallel and len(sentence_data_list) > workers:
        logger.info(f"ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ ({workers}ê°œ í”„ë¡œì„¸ìŠ¤)")
        
        # ë¬¸ì¥ë“¤ì„ ë°°ì¹˜ë¡œ ë¶„í• 
        batch_size_per_worker = max(1, len(sentence_data_list) // workers)
        sentence_batches = []
        
        for i in range(0, len(sentence_data_list), batch_size_per_worker):
            batch = sentence_data_list[i:i + batch_size_per_worker]
            sentence_batches.append(batch)
        
        logger.info(f"ë°°ì¹˜ ë¶„í• : {len(sentence_batches)}ê°œ ë°°ì¹˜, ë°°ì¹˜ë‹¹ í‰ê·  {batch_size_per_worker}ê°œ ë¬¸ì¥")
        
        # device id ë¶„ë°°
        if device_ids is None:
            # ê¸°ë³¸: ì›Œì»¤ ìˆ˜ë§Œí¼ device idë¥¼ 0,1,2,...ë¡œ ë¶„ë°° (ë‹¨ì¼ GPUë©´ ëª¨ë‘ 0)
            import torch
            n_gpu = torch.cuda.device_count() if torch.cuda.is_available() else 0
            if n_gpu > 1:
                device_ids = [i % n_gpu for i in range(workers)]
            else:
                device_ids = [0 for _ in range(workers)]
        
        # í”„ë¡œì„¸ìŠ¤ í’€ë¡œ ë°°ì¹˜ ì²˜ë¦¬
        logger.info("â³ ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì´ˆê¸°í™” ì¤‘... (ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ì¸í•´ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        print(f"â³ {embedder_name.upper()} ëª¨ë¸ ë¡œë”© ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        
        with mp.Pool(processes=workers, initializer=init_worker, initargs=(device_ids[0], embedder_name)) as pool:
            try:
                async_results = []
                for i, batch in enumerate(sentence_batches):
                    device_id = device_ids[i % len(device_ids)]
                    async_result = pool.apply_async(process_batch_sentences, (batch,), {'device_id': device_id})
                    async_results.append((i, async_result))
                
                print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ë¬¸ì¥ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
                logger.info("âœ… ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ, ë¬¸ì¥ ì²˜ë¦¬ ì‹œì‘")
                
                # ê²°ê³¼ ìˆ˜ì§‘ - ë¬¸ì¥ ê¸°ì¤€ ì§„í–‰ë¥  í‘œì‹œ
                processed_sentences = 0
                with tqdm(total=len(sentence_data_list), desc="ë¬¸ì¥ ì²˜ë¦¬", unit="ë¬¸ì¥") as pbar:
                    for batch_idx, async_result in async_results:
                        try:
                            batch_results = async_result.get()  # íƒ€ì„ì•„ì›ƒ ì œê±°
                            results.extend(batch_results)
                            
                            # í˜„ì¬ ë°°ì¹˜ì˜ ë¬¸ì¥ ìˆ˜ ê³„ì‚°
                            batch_sentences = len(sentence_batches[batch_idx])
                            processed_sentences += batch_sentences
                            pbar.update(batch_sentences)
                            pbar.set_postfix({"êµ¬": len(results), "ë°°ì¹˜": f"{batch_idx+1}/{len(sentence_batches)}"})
                            
                            logger.info(f"ë°°ì¹˜ {batch_idx+1}/{len(sentence_batches)} ì™„ë£Œ: {batch_sentences}ê°œ ë¬¸ì¥ â†’ {len(batch_results)}ê°œ êµ¬")
                        except Exception as e:
                            # ì‹¤íŒ¨í•œ ë°°ì¹˜ì˜ ë¬¸ì¥ ìˆ˜ë„ ì—…ë°ì´íŠ¸
                            batch_sentences = len(sentence_batches[batch_idx])
                            processed_sentences += batch_sentences
                            pbar.update(batch_sentences)
                            logger.error(f"ë°°ì¹˜ {batch_idx+1} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                        
            except KeyboardInterrupt:
                logger.info("ì‚¬ìš©ì ì¤‘ë‹¨")
                pool.terminate()
                pool.join()
                raise
                
    else:
        logger.info("ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘")
        
        # ìˆœì°¨ ì²˜ë¦¬ - ì„ë² ë” ì„ íƒ
        print(f"â³ {embedder_name.upper()} ëª¨ë¸ ë¡œë”© ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
        if embedder_name == 'openai':
            from sa_embedders import get_embedder
            embed_func = get_embedder('openai')
        else:  # bge ë˜ëŠ” ê¸°ë³¸ê°’
            from sa_embedders import get_embed_func
            embed_func = get_embed_func()
        
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ë¬¸ì¥ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ, ìˆœì°¨ ì²˜ë¦¬ ì‹œì‘")
        
        from sa_tokenizers.jieba_mecab import (
            split_src_meaning_units, 
            split_tgt_meaning_units_sequential,
            split_tgt_by_src_units_semantic  # í´ë°±ìš©
        )
        from punctuation import mask_brackets, restore_brackets
        
        for sentence_data in tqdm(sentence_data_list, desc="ë¬¸ì¥ ì²˜ë¦¬"):
            try:
                sentence_id = sentence_data['sentence_id']
                src_text = sentence_data['src_text']
                tgt_text = sentence_data['tgt_text']
                
                # ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
                masked_src, src_masks = mask_brackets(src_text, 'source')
                masked_tgt, tgt_masks = mask_brackets(tgt_text, 'target')
                
                src_units = split_src_meaning_units(masked_src)
                tgt_units = split_tgt_by_src_units_semantic(
                    src_units, masked_tgt, embed_func, min_tokens=1
                )
                
                # ê²°ê³¼ ìƒì„±
                for phrase_idx, (src_unit, tgt_unit) in enumerate(zip(src_units, tgt_units), 1):
                    restored_src = restore_brackets(src_unit, src_masks)
                    restored_tgt = restore_brackets(tgt_unit, tgt_masks)
                    
                    results.append({
                        'ë¬¸ì¥ì‹ë³„ì': sentence_id,
                        'êµ¬ì‹ë³„ì': phrase_idx,
                        'ì›ë¬¸': restored_src,
                        'ë²ˆì—­ë¬¸': restored_tgt
                    })
                
            except Exception as e:
                logger.error(f"ë¬¸ì¥ {sentence_data.get('sentence_id', '?')} ìˆœì°¨ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                if verbose:
                    print(f"ìˆœì°¨ ì²˜ë¦¬ ë¬¸ì¥ {sentence_data.get('sentence_id', '?')} ì‹¤íŒ¨: {e}")
                continue
    
    # ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ê³¼ ë™ì¼)
    if not results:
        raise ValueError("ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    result_df = pd.DataFrame(results)
    
    if output_path.endswith('.xlsx'):
        result_df.to_excel(output_path, index=False)
    else:
        result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {len(sentence_data_list)}ê°œ ë¬¸ì¥ â†’ {len(results)}ê°œ êµ¬")
    logger.info(f"ì¶œë ¥: {output_path}")
    
    return result_df