"""í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• ëª¨ë“ˆ - ì„¤ì¹˜ëœ MeCab ì‚¬ìš©"""

import jieba
import logging
import re
import regex  # ğŸ†• ìœ ë‹ˆì½”ë“œ ì†ì„± ì •ê·œì‹
from typing import List, Optional, Callable
import numpy as np

logger = logging.getLogger(__name__)

# MeCab ì´ˆê¸°í™” (ì´ë¯¸ ì„¤ì¹˜ëœ mecab-ko ì‚¬ìš©)
mecab = None
try:
    import MeCab
    mecab = MeCab.Tagger()
    logger.info("âœ… MeCab-ko ì´ˆê¸°í™” ì„±ê³µ")
except ImportError:
    logger.warning("âš ï¸ MeCab ì—†ìŒ, ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©")
    mecab = None
except Exception as e:
    logger.warning(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    mecab = None

def split_src_meaning_units(
    text: str, 
    min_tokens: int = 1, 
    max_tokens: int = 10,
    use_advanced: bool = True
) -> List[str]:
    """ì›ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
    
    if not text or not text.strip():
        return []
    
    try:
        # ğŸ¯ í•œë¬¸ êµ¬ë¬¸ ê²½ê³„ íŒ¨í„´
        patterns = [
            r'ç„¶å¾Œì—',
            r'ì´ìš”(?=\s|\p{Han}|\p{Hangul}|$)',
            r'ì´ë¼ê°€',
            r'ì´ë©´',
            r'í•˜ë©´',
            r'å‰‡(?=\s|\p{Han}|\p{Hangul})',
            r'è€Œ(?=\s|\p{Han}|\p{Hangul})',
            r'ä¸”(?=\s|\p{Han}|\p{Hangul})'
        ]
        
        units = [text]
        
        for pattern in patterns:
            new_units = []
            for unit in units:
                if regex.search(pattern, unit):
                    parts = regex.split(f'({pattern})', unit)
                    current = ""
                    for part in parts:
                        if regex.match(pattern, part):
                            if current:
                                new_units.append(current + part)
                                current = ""
                        else:
                            current += part
                    if current:
                        new_units.append(current)
                else:
                    new_units.append(unit)
            units = [u.strip() for u in new_units if u.strip()]
        
        if use_advanced:
            units = _advanced_han_split(units)
        
        return units
        
    except Exception as e:
        logger.error(f"âŒ ì›ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [text]

def _advanced_han_split(units: List[str]) -> List[str]:
    """í•œìì–´ + ì¡°ì‚¬ ë‹¨ìœ„ë¡œ ê³ ê¸‰ ë¶„í• """
    
    advanced_units = []
    
    for unit in units:
        if len(unit) > 15:
            pattern = r'(\p{Han}+\p{Hangul}*(?:ì´ë¼|ì´ìš”|ì—ì„œ|ë¼ì„œ|í•˜ì—¬|ë©´ì„œ|ì—|ëŠ”|ì€|ì´|ê°€)?)'
            matches = regex.findall(pattern, unit)
            
            if len(matches) > 1:
                remaining = unit
                for match in matches:
                    if match in remaining:
                        pos = remaining.find(match)
                        if pos > 0:
                            advanced_units.append(remaining[:pos].strip())
                        advanced_units.append(match)
                        remaining = remaining[pos + len(match):]
                
                if remaining.strip():
                    advanced_units.append(remaining.strip())
            else:
                advanced_units.append(unit)
        else:
            advanced_units.append(unit)
    
    return [u for u in advanced_units if u.strip()]

def split_tgt_meaning_units(
    src_text: str,
    tgt_text: str,
    embed_func: Optional[Callable] = None,
    use_semantic: bool = True,
    min_tokens: int = 1,
    max_tokens: int = 10,
    similarity_threshold: float = 0.3
) -> List[str]:
    """ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
    
    if not tgt_text or not tgt_text.strip():
        return []
    
    try:
        if mecab:
            return _mecab_split_with_han_awareness(tgt_text, max_tokens)
        else:
            return _basic_split_with_regex(tgt_text, max_tokens)
        
    except Exception as e:
        logger.error(f"âŒ ë²ˆì—­ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [tgt_text]

def _mecab_split_with_han_awareness(text: str, max_tokens: int) -> List[str]:
    """MeCab ë¶„í•  + í•œì ì¸ì‹"""
    
    try:
        # MeCab í˜•íƒœì†Œ ë¶„ì„
        result = mecab.parse(text)
        morphemes = []
        
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    pos = parts[1].split(',')[0]
                    morphemes.append((surface, pos))
        
        # ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
        units = []
        current_unit = ""
        
        for i, (surface, pos) in enumerate(morphemes):
            current_unit += surface
            
            # í•œì + ì¡°ì‚¬ íŠ¹ë³„ ì²˜ë¦¬
            is_han_particle_boundary = False
            if _is_han(surface) and i + 1 < len(morphemes):
                next_surface, next_pos = morphemes[i + 1]
                if _is_particle_pos(next_pos):
                    current_unit += next_surface
                    morphemes[i + 1] = ('', '')  # ìŠ¤í‚µ í‘œì‹œ
                    is_han_particle_boundary = True
            
            # ê²½ê³„ ì¡°ê±´
            is_boundary = (
                pos in ['JKS', 'JKO', 'JKC', 'JX', 'SF', 'SP'] or
                pos.startswith('E') or
                is_han_particle_boundary or
                len(current_unit) >= max_tokens * 2
            )
            
            if is_boundary and current_unit.strip():
                units.append(current_unit.strip())
                current_unit = ""
        
        if current_unit.strip():
            units.append(current_unit.strip())
        
        return [u for u in units if u]
        
    except Exception as e:
        logger.error(f"âŒ MeCab ë¶„í•  ì‹¤íŒ¨: {e}")
        return _basic_split_with_regex(text, max_tokens)

def _basic_split_with_regex(text: str, max_tokens: int) -> List[str]:
    """regex ê¸°ë°˜ ê¸°ë³¸ ë¶„í• """
    
    patterns = [
        r'([.!?ã€‚ï¼ï¼Ÿ]+)',
        r'(\p{Han}+\p{Hangul}*)',
        r'(\p{Hangul}+(?:ë‹¤|ê³ |ë©°|ì§€ë§Œ))',
        r'([,ï¼Œ;ï¼š:]+)'
    ]
    
    units = [text]
    
    for pattern in patterns:
        new_units = []
        for unit in units:
            if regex.search(pattern, unit):
                parts = regex.split(pattern, unit)
                current = ""
                for part in parts:
                    if regex.match(pattern, part):
                        current += part
                        if len(current) >= max_tokens or part in ['.', '!', '?', 'ã€‚']:
                            new_units.append(current.strip())
                            current = ""
                    else:
                        current += part
                if current.strip():
                    new_units.append(current.strip())
            else:
                new_units.append(unit)
        units = [u.strip() for u in new_units if u.strip()]
    
    return units

def _is_han(token: str) -> bool:
    """í•œì í¬í•¨ ì—¬ë¶€"""
    return bool(regex.search(r'\p{Han}', token))

def _is_hangul(token: str) -> bool:
    """í•œê¸€ í¬í•¨ ì—¬ë¶€"""
    return bool(regex.search(r'\p{Hangul}', token))

def _is_particle_pos(pos: str) -> bool:
    """í’ˆì‚¬ê°€ ì¡°ì‚¬ì¸ì§€ í™•ì¸"""
    particle_pos = ['JKS', 'JKO', 'JKC', 'JX']
    return pos in particle_pos

def _is_particle(token: str) -> bool:
    """ì¡°ì‚¬/ì–´ë¯¸ ì—¬ë¶€"""
    particles = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 
                'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë¼', 'ì´ë¼']
    return token in particles or (len(token) <= 2 and _is_hangul(token))

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    print(f"ğŸ”§ MeCab ìƒíƒœ: {'ì‚¬ìš© ê°€ëŠ¥' if mecab else 'ì‚¬ìš© ë¶ˆê°€'}")