"""ê°œì„ ëœ í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• ëª¨ë“ˆ"""

import jieba
import MeCab
import logging
import re
from typing import List, Optional, Callable, Dict, Tuple
import numpy as np

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# MeCab ì´ˆê¸°í™”
try:
    mecab = MeCab.Tagger()
    logger.info("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ")
except Exception as e:
    logger.warning(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    mecab = None

class ImprovedTokenizer:
    """ê°œì„ ëœ í† í¬ë‚˜ì´ì € í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.mecab = mecab
        
        # í•œë¬¸ êµ¬ë¬¸ ë¶„í•  íŒ¨í„´
        self.classical_patterns = [
            r'ç„¶å¾Œì—?',      # ì‹œê°„ ì ‘ì†
            r'ç„¶å¾Œ',
            r'ì´ìš”(?!\w)',   # ë³‘ë ¬ ì ‘ì† (ë’¤ì— ë¬¸ìê°€ ì—†ì„ ë•Œ)
            r'ì´ë©°',
            r'ì´ê³ ',
            r'ë¼ê°€(?!\w)',   # ì „í™˜
            r'ë¼ì„œ',
            r'(?<!.)ë©´(?!\w)',     # ì¡°ê±´ (ì•ì— ë¬¸ìê°€ ì—†ê³  ë’¤ì— ë¬¸ìê°€ ì—†ì„ ë•Œ)
            r'ì´ë©´',
            r'í•˜ë©´',
            r'å‰‡(?=\s|\w)',   # í•œë¬¸ ì ‘ì†ì‚¬ (ë’¤ì— ê³µë°±ì´ë‚˜ ë¬¸ì)
            r'è€Œ(?=\s|\w)',
            r'ä¸”(?=\s|\w)',
        ]
        
        # í•œêµ­ì–´ êµ¬ë¬¸ ê²½ê³„ íŒ¨í„´
        self.korean_boundaries = [
            r'(?<=ë‹¤)\s*(?=[ê°€-í£])',  # ì„œìˆ ì–´ ë’¤
            r'(?<=[.!?])\s*',         # êµ¬ë‘ì  ë’¤
            r'(?<=ê³ )\s+(?=[ê°€-í£])',  # ì—°ê²°ì–´ë¯¸ ë’¤
            r'(?<=ë©°)\s+(?=[ê°€-í£])',
            r'(?<=ì§€ë§Œ)\s+(?=[ê°€-í£])',
            r'(?<=í•˜ì—¬)\s+(?=[ê°€-í£])',
        ]

    def split_src_meaning_units(
        self, 
        text: str, 
        min_tokens: int = 1, 
        max_tokens: int = 15,
        use_advanced: bool = True
    ) -> List[str]:
        """ê°œì„ ëœ ì›ë¬¸ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• """
        
        if not text or not text.strip():
            return []
        
        try:
            logger.debug(f"ğŸ”¤ ì›ë¬¸ ë¶„í•  ì‹œì‘: {text[:50]}...")
            
            # 1ë‹¨ê³„: êµ¬ë¬¸ íŒ¨í„´ ê¸°ë°˜ ê¸°ë³¸ ë¶„í• 
            units = self._split_by_classical_patterns(text)
            
            # 2ë‹¨ê³„: ê³ ê¸‰ ë¶„í•  (í•œìì–´ + ì¡°ì‚¬ ë‹¨ìœ„)
            if use_advanced:
                units = self._advanced_src_split(units)
            
            # 3ë‹¨ê³„: ê¸¸ì´ ì œí•œ ì ìš©
            units = self._apply_length_constraints(units, min_tokens, max_tokens, is_src=True)
            
            # 4ë‹¨ê³„: ë¹ˆ ë‹¨ìœ„ ì œê±° ë° ì •ë¦¬
            units = [u.strip() for u in units if u.strip()]
            
            logger.debug(f"âœ… ì›ë¬¸ ë¶„í•  ì™„ë£Œ: {len(units)}ê°œ ë‹¨ìœ„")
            return units
            
        except Exception as e:
            logger.error(f"âŒ ì›ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
            return [text]  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

    def split_tgt_meaning_units(
        self,
        src_text: str,
        tgt_text: str,
        embed_func: Optional[Callable] = None,
        use_semantic: bool = True,
        min_tokens: int = 1,
        max_tokens: int = 15,
        similarity_threshold: float = 0.3
    ) -> List[str]:
        """ê°œì„ ëœ ë²ˆì—­ë¬¸ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• """
        
        if not tgt_text or not tgt_text.strip():
            return []
        
        try:
            logger.debug(f"ğŸ”¤ ë²ˆì—­ë¬¸ ë¶„í•  ì‹œì‘: {tgt_text[:50]}...")
            
            if use_semantic and embed_func is not None:
                # ì˜ë¯¸ ê¸°ë°˜ ë¶„í• 
                units = self._semantic_tgt_split(
                    src_text, tgt_text, embed_func, 
                    similarity_threshold, min_tokens, max_tokens
                )
            else:
                # ê°œì„ ëœ ë‹¨ìˆœ ë¶„í• 
                units = self._improved_simple_tgt_split(tgt_text, min_tokens, max_tokens)
            
            logger.debug(f"âœ… ë²ˆì—­ë¬¸ ë¶„í•  ì™„ë£Œ: {len(units)}ê°œ ë‹¨ìœ„")
            return units
            
        except Exception as e:
            logger.error(f"âŒ ë²ˆì—­ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
            return [tgt_text]  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

    def _split_by_classical_patterns(self, text: str) -> List[str]:
        """í•œë¬¸ êµ¬ë¬¸ íŒ¨í„´ ê¸°ë°˜ ë¶„í• """
        
        units = [text]
        
        for pattern in self.classical_patterns:
            new_units = []
            
            for unit in units:
                # íŒ¨í„´ìœ¼ë¡œ ë¶„í• í•˜ë˜ êµ¬ë¶„ì ë³´ì¡´
                parts = re.split(f'({pattern})', unit)
                
                current = ""
                for part in parts:
                    if re.match(pattern, part):
                        # êµ¬ë¶„ìëŠ” ì• ë‹¨ìœ„ì— ë¶™ì„
                        if current:
                            new_units.append(current + part)
                            current = ""
                        else:
                            # ë‹¨ë… êµ¬ë¶„ìëŠ” ë”°ë¡œ ì²˜ë¦¬
                            new_units.append(part)
                    else:
                        current += part
                
                if current:
                    new_units.append(current)
            
            units = [u.strip() for u in new_units if u.strip()]
        
        return units

    def _advanced_src_split(self, units: List[str]) -> List[str]:
        """ê³ ê¸‰ ì›ë¬¸ ë¶„í•  - í•œìì–´ + ì¡°ì‚¬ ë‹¨ìœ„"""
        
        advanced_units = []
        
        for unit in units:
            if len(unit) > 20:  # ê¸´ ë‹¨ìœ„ë§Œ ì¶”ê°€ ë¶„í• 
                # í•œìì–´ + ì¡°ì‚¬/ì–´ë¯¸ íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
                pattern = r'([\u4e00-\u9fff]+[ê°€-í£]*(?:ì´ë¼|ì´ìš”|ì—ì„œ|ë¼ì„œ|í•˜ì—¬|ë©´ì„œ)?)'
                matches = re.findall(pattern, unit)
                
                if len(matches) > 1:
                    # íŒ¨í„´ ë§¤ì¹­ëœ ë¶€ë¶„ë“¤ ì¶”ì¶œ
                    remaining = unit
                    for match in matches:
                        if match in remaining:
                            pos = remaining.find(match)
                            if pos > 0:
                                # ì•ë¶€ë¶„ì´ ìˆìœ¼ë©´ ì¶”ê°€
                                advanced_units.append(remaining[:pos].strip())
                            advanced_units.append(match)
                            remaining = remaining[pos + len(match):]
                    
                    if remaining.strip():
                        advanced_units.append(remaining.strip())
                else:
                    advanced_units.append(unit)
            else:
                advanced_units.append(unit)
        
        return [u for u in advanced_units if u.strip()]

    def _improved_simple_tgt_split(self, text: str, min_tokens: int, max_tokens: int) -> List[str]:
        """ê°œì„ ëœ ë‹¨ìˆœ ë²ˆì—­ë¬¸ ë¶„í• """
        
        if not self.mecab:
            return self._basic_tgt_split(text, min_tokens, max_tokens)
        
        try:
            # MeCab í˜•íƒœì†Œ ë¶„ì„
            morphemes = self._analyze_morphemes(text)
            
            # ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™” (ê³µë°± ë³´ì¡´)
            units = self._group_morphemes_meaningfully(morphemes, min_tokens, max_tokens)
            
            return units
            
        except Exception as e:
            logger.error(f"âŒ MeCab ë¶„í•  ì‹¤íŒ¨: {e}")
            return self._basic_tgt_split(text, min_tokens, max_tokens)

    def _analyze_morphemes(self, text: str) -> List[Dict]:
        """í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”"""
        
        result = self.mecab.parse(text)
        morphemes = []
        position = 0
        
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    features = parts[1].split(',')
                    pos = features[0] if features else ''
                    
                    morphemes.append({
                        'surface': surface,
                        'pos': pos,
                        'features': features,
                        'start': position,
                        'end': position + len(surface)
                    })
                    position += len(surface)
        
        return morphemes

    def _group_morphemes_meaningfully(
        self, 
        morphemes: List[Dict], 
        min_tokens: int, 
        max_tokens: int
    ) -> List[str]:
        """í˜•íƒœì†Œë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ê·¸ë£¹í™” (ê³µë°± ë³´ì¡´)"""
        
        if not morphemes:
            return []
        
        units = []
        current_group = []
        current_text = ""
        
        # ì˜ë¯¸ ê²½ê³„ í’ˆì‚¬ë“¤
        boundary_pos = ['SF', 'SP', 'SS', 'EC', 'EF', 'ETM', 'ETN']
        # ë…ë¦½ì  ì˜ë¯¸ í’ˆì‚¬ë“¤
        content_pos = ['NNG', 'NNP', 'VV', 'VA', 'MAG', 'MM']
        
        i = 0
        while i < len(morphemes):
            morph = morphemes[i]
            surface = morph['surface']
            pos = morph['pos']
            
            current_group.append(morph)
            current_text += surface
            
            # ê²½ê³„ ì¡°ê±´ í™•ì¸
            is_boundary = (
                pos in boundary_pos or  # í’ˆì‚¬ ê²½ê³„
                len(current_group) >= max_tokens or  # ìµœëŒ€ ê¸¸ì´
                (len(current_group) >= min_tokens and 
                 pos in content_pos and 
                 i + 1 < len(morphemes) and 
                 morphemes[i + 1]['pos'] in content_pos)  # ë‚´ìš©ì–´ ì—°ì†
            )
            
            # í•œìì–´ + ì¡°ì‚¬ íŒ¨í„´ íŠ¹ë³„ ì²˜ë¦¬
            if (self._is_hanja(surface) and 
                i + 1 < len(morphemes) and 
                self._is_particle(morphemes[i + 1]['surface'])):
                # ë‹¤ìŒ í† í°(ì¡°ì‚¬)ê¹Œì§€ í¬í•¨
                i += 1
                next_morph = morphemes[i]
                current_group.append(next_morph)
                current_text += next_morph['surface']
                is_boundary = True
            
            if is_boundary and len(current_group) >= min_tokens:
                units.append(current_text.strip())
                current_group = []
                current_text = ""
            
            i += 1
        
        # ë§ˆì§€ë§‰ ê·¸ë£¹ ì²˜ë¦¬
        if current_group and current_text.strip():
            if units and len(current_group) < min_tokens:
                # ë„ˆë¬´ ì§§ìœ¼ë©´ ì´ì „ ë‹¨ìœ„ì™€ í•©ì¹˜ê¸°
                units[-1] = units[-1] + current_text
            else:
                units.append(current_text.strip())
        
        return [u for u in units if u.strip()]

    def _semantic_tgt_split(
        self,
        src_text: str, 
        tgt_text: str, 
        embed_func: Callable,
        similarity_threshold: float,
        min_tokens: int,
        max_tokens: int
    ) -> List[str]:
        """ì˜ë¯¸ ê¸°ë°˜ ë²ˆì—­ë¬¸ ë¶„í•  - ì›ë¬¸ ë‹¨ìœ„ë¥¼ ê³ ë ¤"""
        
        try:
            # 1. ì›ë¬¸ ë‹¨ìœ„ ë¶„í• 
            src_units = self.split_src_meaning_units(src_text, min_tokens, max_tokens)
            
            # 2. ë²ˆì—­ë¬¸ ê¸°ë³¸ ë¶„í• 
            tgt_candidates = self._improved_simple_tgt_split(tgt_text, 1, max_tokens // 2)
            
            # 3. ì›ë¬¸ ë‹¨ìœ„ ìˆ˜ì— ë”°ë¥¸ ì ì‘ì  ì¬ì¡°í•©
            if len(src_units) > 1 and len(tgt_candidates) > len(src_units):
                tgt_units = self._adaptive_regrouping(
                    src_units, tgt_candidates, embed_func, similarity_threshold
                )
            else:
                tgt_units = tgt_candidates
            
            return tgt_units
            
        except Exception as e:
            logger.error(f"âŒ ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ì‹¤íŒ¨: {e}")
            return self._improved_simple_tgt_split(tgt_text, min_tokens, max_tokens)

    def _adaptive_regrouping(
        self,
        src_units: List[str], 
        tgt_candidates: List[str], 
        embed_func: Callable,
        similarity_threshold: float
    ) -> List[str]:
        """ì ì‘ì  ì¬ì¡°í•© - ì›ë¬¸ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë²ˆì—­ë¬¸ ì¬êµ¬ì„±"""
        
        try:
            # ëª©í‘œ ë¶„í•  ìˆ˜ ê²°ì •
            src_count = len(src_units)
            cand_count = len(tgt_candidates)
            
            if cand_count <= src_count:
                return tgt_candidates
            
            # ëª©í‘œ: ì›ë¬¸ ë‹¨ìœ„ ìˆ˜ì˜ 80-120% ë²”ìœ„
            target_count = max(2, min(src_count + 2, cand_count // 2))
            
            # ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë£¹í™”
            embeddings = embed_func(src_units + tgt_candidates)
            src_embeddings = embeddings[:src_count]
            tgt_embeddings = embeddings[src_count:]
            
            # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            similarity_matrix = self._calculate_similarity_matrix(src_embeddings, tgt_embeddings)
            
            # ìµœì  ê·¸ë£¹í™”
            grouped_units = self._find_optimal_grouping(
                tgt_candidates, similarity_matrix, target_count, similarity_threshold
            )
            
            return grouped_units
            
        except Exception as e:
            logger.error(f"âŒ ì ì‘ì  ì¬ì¡°í•© ì‹¤íŒ¨: {e}")
            return tgt_candidates

    def _basic_tgt_split(self, text: str, min_tokens: int, max_tokens: int) -> List[str]:
        """ê¸°ë³¸ ë²ˆì—­ë¬¸ ë¶„í•  (MeCab ì—†ì„ ë•Œ)"""
        
        # êµ¬ë‘ì ê³¼ ì ‘ì†ì‚¬ ê¸°ì¤€ ë¶„í• 
        patterns = [
            r'([.!?ã€‚ï¼ï¼Ÿ]+)',      # êµ¬ë‘ì 
            r'(ê·¸ëŸ°ë°|í•˜ì§€ë§Œ|ë”°ë¼ì„œ|ê·¸ëŸ¬ë¯€ë¡œ|ì¦‰|ë˜í•œ)',  # ì ‘ì†ì‚¬
            r'([,ï¼Œ;ï¼š:]+)',        # ì‰¼í‘œë¥˜
        ]
        
        units = [text]
        
        for pattern in patterns:
            new_units = []
            
            for unit in units:
                parts = re.split(pattern, unit)
                current = ""
                
                for part in parts:
                    if re.match(pattern, part):
                        if current:
                            new_units.append(current + part)
                            current = ""
                    else:
                        current += part
                
                if current:
                    new_units.append(current)
            
            units = [u.strip() for u in new_units if u.strip()]
        
        return self._apply_length_constraints(units, min_tokens, max_tokens, is_src=False)

    def _apply_length_constraints(
        self, 
        units: List[str], 
        min_tokens: int, 
        max_tokens: int, 
        is_src: bool = True
    ) -> List[str]:
        """ê¸¸ì´ ì œí•œ ì ìš©"""
        
        if min_tokens <= 1 and max_tokens >= 50:
            return units
        
        # ìµœëŒ€ ê¸¸ì´ ì œí•œ
        constrained_units = []
        
        for unit in units:
            if len(unit) > max_tokens * 4:  # ê¸€ì ìˆ˜ ê¸°ì¤€ (ëŒ€ëµ)
                # ê¸´ ë‹¨ìœ„ ë¶„í• 
                split_points = self._find_split_points(unit)
                if split_points:
                    start = 0
                    for point in split_points:
                        constrained_units.append(unit[start:point].strip())
                        start = point
                    if start < len(unit):
                        constrained_units.append(unit[start:].strip())
                else:
                    # ë¶„í• ì  ì—†ìœ¼ë©´ ì¤‘ê°„ì—ì„œ ë¶„í• 
                    mid = len(unit) // 2
                    constrained_units.append(unit[:mid].strip())
                    constrained_units.append(unit[mid:].strip())
            else:
                constrained_units.append(unit)
        
        # ìµœì†Œ ê¸¸ì´ ì œí•œ (ë³‘í•©)
        if min_tokens > 1:
            merged_units = []
            temp = ""
            
            for unit in constrained_units:
                if len(temp + unit) < min_tokens * 3:  # ëŒ€ëµì  ê¸°ì¤€
                    temp += unit
                else:
                    if temp:
                        merged_units.append(temp.strip())
                    temp = unit
            
            if temp:
                merged_units.append(temp.strip())
            
            constrained_units = merged_units
        
        return [u for u in constrained_units if u.strip()]

    def _find_split_points(self, text: str) -> List[int]:
        """ë¶„í• ì  ì°¾ê¸°"""
        
        split_points = []
        
        # êµ¬ë‘ì  ìœ„ì¹˜
        for match in re.finditer(r'[.!?ã€‚ï¼ï¼Ÿ,ï¼Œ]', text):
            split_points.append(match.end())
        
        # ì ‘ì† í‘œí˜„ ìœ„ì¹˜
        connectors = ['ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ë”°ë¼ì„œ', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ']
        for connector in connectors:
            for match in re.finditer(connector, text):
                split_points.append(match.start())
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        split_points = sorted(set(split_points))
        
        # ë„ˆë¬´ ê°€ê¹Œìš´ ë¶„í• ì  ì œê±°
        filtered_points = []
        last_point = 0
        
        for point in split_points:
            if point - last_point > 10:  # ìµœì†Œ ê°„ê²©
                filtered_points.append(point)
                last_point = point
        
        return filtered_points

    def _calculate_similarity_matrix(self, emb1: List, emb2: List) -> np.ndarray:
        """ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
        
        try:
            embeddings1 = np.array(emb1)
            embeddings2 = np.array(emb2)
            
            # ì •ê·œí™”
            emb1_norm = embeddings1 / np.linalg.norm(embeddings1, axis=1, keepdims=True)
            emb2_norm = embeddings2 / np.linalg.norm(embeddings2, axis=1, keepdims=True)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = np.dot(emb1_norm, emb2_norm.T)
            
            # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
            similarity = (similarity + 1) / 2
            
            return similarity
            
        except Exception as e:
            logger.error(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return np.zeros((len(emb1), len(emb2)))

    def _find_optimal_grouping(
        self,
        candidates: List[str], 
        similarity_matrix: np.ndarray,
        target_count: int,
        threshold: float
    ) -> List[str]:
        """ìµœì  ê·¸ë£¹í™” - ë™ì  ê³„íšë²• ê¸°ë°˜"""
        
        try:
            # ê·¸ë¦¬ë”” ê·¸ë£¹í™”
            groups = []
            used = set()
            
            # ìœ ì‚¬ë„ê°€ ë†’ì€ ì¸ì ‘ í›„ë³´ë“¤ì„ ìš°ì„  ê·¸ë£¹í™”
            for i in range(len(candidates)):
                if i in used:
                    continue
                
                current_group = [candidates[i]]
                used.add(i)
                
                # ì¸ì ‘í•œ í›„ë³´ ì¤‘ ìœ ì‚¬ë„ê°€ ë†’ì€ ê²ƒë“¤ ì¶”ê°€
                for j in range(i + 1, min(i + 3, len(candidates))):  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ í™•ì¸
                    if j not in used:
                        max_sim = 0
                        if i < similarity_matrix.shape[1]:
                            max_sim = max(similarity_matrix[:, j % similarity_matrix.shape[1]])
                        
                        if max_sim >= threshold:
                            current_group.append(candidates[j])
                            used.add(j)
                            break
                
                # ê·¸ë£¹ ê²°í•©
                groups.append(''.join(current_group))
            
            # ëª©í‘œ ê°œìˆ˜ì— ë§ì¶° ì¡°ì •
            while len(groups) > target_count and len(groups) > 1:
                # ê°€ì¥ ì§§ì€ ë‘ ê·¸ë£¹ ë³‘í•©
                min_len = min(len(g) for g in groups)
                for i in range(len(groups) - 1):
                    if len(groups[i]) == min_len:
                        groups[i] = groups[i] + groups[i + 1]
                        groups.pop(i + 1)
                        break
            
            return groups
            
        except Exception as e:
            logger.error(f"âŒ ìµœì  ê·¸ë£¹í™” ì‹¤íŒ¨: {e}")
            return candidates

    def _is_hanja(self, token: str) -> bool:
        """í•œì í¬í•¨ ì—¬ë¶€"""
        return bool(re.search(r'[\u4e00-\u9fff]', token))
    
    def _is_particle(self, token: str) -> bool:
        """ì¡°ì‚¬/ì–´ë¯¸ ì—¬ë¶€"""
        particles = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 
                    'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë¼', 'ì´ë¼']
        return token in particles or (len(token) <= 2 and re.search(r'[ê°€-í£]', token))

# ì „ì—­ í† í¬ë‚˜ì´ì € ì¸ìŠ¤í„´ìŠ¤
_tokenizer = ImprovedTokenizer()

def split_src_meaning_units(
    text: str, 
    min_tokens: int = 1, 
    max_tokens: int = 15,
    use_advanced: bool = True
) -> List[str]:
    """ì›ë¬¸ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  (ì „ì—­ í•¨ìˆ˜)"""
    return _tokenizer.split_src_meaning_units(text, min_tokens, max_tokens, use_advanced)

def split_tgt_meaning_units(
    src_text: str,
    tgt_text: str,
    embed_func: Optional[Callable] = None,
    use_semantic: bool = True,
    min_tokens: int = 1,
    max_tokens: int = 15,
    similarity_threshold: float = 0.3
) -> List[str]:
    """ë²ˆì—­ë¬¸ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  (ì „ì—­ í•¨ìˆ˜)"""
    return _tokenizer.split_tgt_meaning_units(
        src_text, tgt_text, embed_func, use_semantic, 
        min_tokens, max_tokens, similarity_threshold
    )

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.DEBUG)
    
    print("ğŸ§ª ê°œì„ ëœ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    test_cases = [
        (
            "èˆˆä¹Ÿë¼",
            "èˆˆì´ë‹¤."
        ),
        (
            "è’¹ì€ è–•(ë ´)ì´ìš” è‘­ëŠ” è˜†ä¹Ÿë¼",
            "è’¹ì€ ë¬¼ì–µìƒˆì´ê³  è‘­ëŠ” ê°ˆëŒ€ì´ë‹¤."
        ),
        (
            "ç™½éœ²å‡æˆ¾çˆ²éœœç„¶å¾Œì— æ­²äº‹æˆì´ìš” åœ‹å®¶å¾…ç¦®ç„¶å¾Œèˆˆì´ë¼",
            "ç™½éœ²ê°€ ì–¼ì–´ ì„œë¦¬ê°€ ëœ ë’¤ì—ì•¼ æ­²äº‹ê°€ ì´ë£¨ì–´ì§€ê³  åœ‹å®¶ëŠ” ç¦®ê°€ í–‰í•´ì§„ ë’¤ì—ì•¼ í¥ì„±í•œë‹¤."
        )
    ]
    
    for i, (src, tgt) in enumerate(test_cases, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}:")
        print(f"ì›ë¬¸: {src}")
        print(f"ë²ˆì—­: {tgt}")
        
        # ì›ë¬¸ ë¶„í• 
        src_units = split_src_meaning_units(src, min_tokens=1, max_tokens=15)
        print(f"âœ… ì›ë¬¸ ë¶„í• : {src_units}")
        
        # ë²ˆì—­ë¬¸ ë¶„í•  (ë‹¨ìˆœ)
        tgt_units = split_tgt_meaning_units(
            src, tgt, embed_func=None, use_semantic=False, 
            min_tokens=1, max_tokens=10
        )
        print(f"âœ… ë²ˆì—­ ë¶„í• : {tgt_units}")