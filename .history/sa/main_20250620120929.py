"""ë©”ì¸ ì‹¤í–‰ íŒŒì¼ - ê¸°ë³¸ê°’ ìë™ ì ìš©"""
import argparse
import logging
from src.config import Config
from src.orchestrator import run_processing
from src.components import list_available_tokenizers, list_available_embedders

def main():
    parser = argparse.ArgumentParser(description="Prototype02 + ë¶„ì„ê¸° ì‹œìŠ¤í…œ (ê¸°ë³¸ê°’: ì›ë¬¸ jieba, ë²ˆì—­ë¬¸ mecab)")
    parser.add_argument("input_path", help="ì…ë ¥ íŒŒì¼ ê²½ë¡œ (.xlsx ë˜ëŠ” .csv)")
    parser.add_argument("output_path", help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ")
    
    # í† í¬ë‚˜ì´ì € ì˜µì…˜ (ê¸°ë³¸ê°’ ìë™ ì ìš©)
    parser.add_argument(
        "--tokenizer", 
        default="default",  # ì›ë¬¸ jieba, ë²ˆì—­ë¬¸ mecab
        choices=list_available_tokenizers(),
        help="í† í¬ë‚˜ì´ì € ì¡°í•© (ê¸°ë³¸ê°’: ì›ë¬¸ jieba, ë²ˆì—­ë¬¸ mecab)"
    )
    
    # ì¡°í•©í˜• ì˜µì…˜
    parser.add_argument("--use-combo", action="store_true", 
                       help="ë²ˆì—­ë¬¸ì— ë‹¤ì¤‘ ë¶„ì„ê¸° ì‚¬ìš© (jieba+mecab)")
    
    # ê³ ê¸‰ ì‚¬ìš©ììš© ê°œë³„ ì„¤ì •
    parser.add_argument("--source-tokenizer", help="ì›ë¬¸ í† í¬ë‚˜ì´ì € ê°œë³„ ì„¤ì •")
    parser.add_argument("--target-tokenizer", help="ë²ˆì—­ë¬¸ í† í¬ë‚˜ì´ì € ê°œë³„ ì„¤ì •")
    
    parser.add_argument(
        "--embedder", 
        default="bge-m3",
        choices=list_available_embedders(),
        help="ì„ë² ë” ì„ íƒ (ê¸°ë³¸ê°’: bge-m3)"
    )
    
    parser.add_argument("--parallel", action="store_true", help="ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©")
    parser.add_argument("--workers", type=int, default=4, help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜")
    parser.add_argument("--chunk-size", type=int, default=50, help="ì²­í¬ í¬ê¸°")
    parser.add_argument("--verbose", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger = logging.getLogger(__name__)
    
    # *** ê¸°ë³¸ê°’ ìë™ ì ìš© ë¡œì§ ***
    if args.use_combo:
        # ì¡°í•©í˜• ì‚¬ìš©
        tokenizer_type = 'default-combo'
        logger.info("ë‹¤ì¤‘ ë¶„ì„ê¸° ì¡°í•© ëª¨ë“œ í™œì„±í™”")
    elif args.source_tokenizer or args.target_tokenizer:
        # ê°œë³„ ì„¤ì • ì‚¬ìš©
        tokenizer_type = args.tokenizer
        logger.info("ê°œë³„ í† í¬ë‚˜ì´ì € ì„¤ì • ì‚¬ìš©")
    else:
        # ì™„ì „ ê¸°ë³¸ê°’ (ì•„ë¬´ ì˜µì…˜ ì—†ìŒ)
        tokenizer_type = 'default'  # ì›ë¬¸ jieba, ë²ˆì—­ë¬¸ mecab
        logger.info("ê¸°ë³¸ê°’ ìë™ ì ìš©: ì›ë¬¸ jieba, ë²ˆì—­ë¬¸ mecab")
    
    # ì„¤ì • ìƒì„± (ê¸°ë³¸ê°’ì´ ì´ë¯¸ ì„¤ì •ë¨)
    config = Config(
        input_path=args.input_path,
        output_path=args.output_path,
        source_tokenizer_type=args.source_tokenizer or tokenizer_type,
        target_tokenizer_type=args.target_tokenizer or tokenizer_type,
        embedder_type=args.embedder,
        use_parallel=args.parallel,
        num_workers=args.workers,
        chunk_size=args.chunk_size,
        verbose=args.verbose
    )
    
    # ì‹¤í–‰
    logger.info("=== Prototype02 + ë¶„ì„ê¸° ìë™ ì‹œìŠ¤í…œ ì‹œì‘ ===")
    logger.info(f"ì…ë ¥: {args.input_path}")
    logger.info(f"ì¶œë ¥: {args.output_path}")
    logger.info(f"ğŸ¯ ê¸°ë³¸ ì„¤ì •: ì›ë¬¸ Jieba ë¶„ì„ + ë²ˆì—­ë¬¸ MeCab ë¶„ì„")
    logger.info(f"ì„ë² ë”: {args.embedder}")
    logger.info(f"ë³‘ë ¬ ì²˜ë¦¬: {args.parallel}")
    
    try:
        run_processing(config)
        logger.info("=== ì²˜ë¦¬ ì™„ë£Œ ===")
        logger.info("ğŸ’¡ íŒ: ë‹¤ë¥¸ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ --tokenizer ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”")
        logger.info("     ì˜ˆ: --tokenizer mecab-jieba ë˜ëŠ” --use-combo")
    except Exception as e:
        logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    main()