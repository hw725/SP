"""SA ì •ë ¬ ì‹œìŠ¤í…œ ë©”ì¸ ì‹¤í–‰ê¸°"""

import argparse
import logging
import time
import sys
import os
from typing import Optional

def setup_logging(verbose: bool = False):
    """ë¡œê¹… ì„¤ì •"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s:%(name)s:%(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('sa_processing.log', encoding='utf-8')
        ]
    )

def get_tokenizer_module(tokenizer_name: str):
    """í† í¬ë‚˜ì´ì € ëª¨ë“ˆ ë™ì  ë¡œë“œ"""
    tokenizer_map = {
        'jieba': 'sa_tokenizers.jieba_mecab',
        'mecab': 'sa_tokenizers.jieba_mecab', 
        'soy': 'sa_tokenizers.soynlp',
        'kkma': 'sa_tokenizers.kkma'
    }
    
    if tokenizer_name not in tokenizer_map:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í† í¬ë‚˜ì´ì €: {tokenizer_name}")
    
    module_name = tokenizer_map[tokenizer_name]
    
    try:
        import importlib
        module = importlib.import_module(module_name)
        return module
    except ImportError as e:
        raise ImportError(f"í† í¬ë‚˜ì´ì € ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨ {tokenizer_name}: {e}")

def get_embedder_module(embedder_name: str):
    """ì„ë² ë” ëª¨ë“ˆ ë™ì  ë¡œë“œ"""
    embedder_map = {
        'sentence_transformer': 'sa_embedders.sentence_transformer',
        'st': 'sa_embedders.sentence_transformer',
        'openai': 'sa_embedders.openai',
        'bge': 'sa_embedders.bge',
        'hf': 'sa_embedders.hf'
    }
    
    if embedder_name not in embedder_map:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”: {embedder_name}")
    
    module_name = embedder_map[embedder_name]
    
    try:
        import importlib
        module = importlib.import_module(module_name)
        return module
    except ImportError as e:
        raise ImportError(f"ì„ë² ë” ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨ {embedder_name}: {e}")

def process_single_file(
    input_file: str,
    output_file: str,
    tokenizer_name: str = 'jieba',
    embedder_name: str = 'st',
    use_semantic: bool = True,
    min_tokens: int = 1,
    max_tokens: int = 10,
    parallel: bool = False,
    **kwargs
) -> bool:
    """ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬"""
    
    print(f"ğŸš€ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file}")
    print(f"ğŸ“Š ì„¤ì •:")
    print(f"   í† í¬ë‚˜ì´ì €: {tokenizer_name}")
    print(f"   ì„ë² ë”: {embedder_name}")
    print(f"   ì˜ë¯¸ ë§¤ì¹­: {use_semantic}")
    print(f"   ë³‘ë ¬ ì²˜ë¦¬: {parallel}")
    print(f"   í† í° ë²”ìœ„: {min_tokens}-{max_tokens}")
    
    try:
        # ğŸ”§ ìˆ˜ì •: ê¸°ë³¸ í† í¬ë‚˜ì´ì €ëŠ” ë™ì  ë¡œë”© ì—†ì´ ë°”ë¡œ ì²˜ë¦¬
        if tokenizer_name == 'jieba' and embedder_name == 'st':
            print("âœ… ê¸°ë³¸ ëª¨ë“ˆ ì‚¬ìš© (jieba + sentence_transformer)")
            
            from processor import process_file
            
            start_time = time.time()
            
            results = process_file(
                input_file,
                use_semantic=use_semantic,
                min_tokens=min_tokens,
                max_tokens=max_tokens,
                save_results=True,
                output_file=output_file
            )
            
        else:
            print("âœ… ë™ì  ëª¨ë“ˆ ë¡œë”©...")
            
            # ë™ì  ëª¨ë“ˆ ë¡œë“œ
            tokenizer_module = get_tokenizer_module(tokenizer_name)
            embedder_module = get_embedder_module(embedder_name)
            
            print(f"âœ… ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
            
            from processor import process_file_with_modules
            
            start_time = time.time()
            
            results = process_file_with_modules(
                input_file, output_file,
                tokenizer_module, embedder_module,
                use_semantic, min_tokens, max_tokens
            )
        
        end_time = time.time()
        
        if results is not None:
            print(f"ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
            print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {len(results)}ê°œ ë¬¸ì¥")
            print(f"ğŸ“ ì¶œë ¥ íŒŒì¼: {output_file}")
            return True
        else:
            print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        print(f"ğŸ’¥ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description='SA ì •ë ¬ ì‹œìŠ¤í…œ - ë¬¸ì¥ ë‹¨ìœ„ í† í° ì •ë ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì²˜ë¦¬
  python main.py input.xlsx output.xlsx
  
  # í† í¬ë‚˜ì´ì €/ì„ë² ë” ì§€ì •
  python main.py input.xlsx output.xlsx --tokenizer soy --embedder openai
  
  # ë³‘ë ¬ ì²˜ë¦¬
  python main.py input.xlsx output.xlsx --parallel
  
  # ìƒì„¸ ì„¤ì •
  python main.py input.xlsx output.xlsx --tokenizer jieba --embedder bge --min-tokens 2 --max-tokens 15 --no-semantic
  
ì§€ì› í† í¬ë‚˜ì´ì €: jieba, soy, kkma
ì§€ì› ì„ë² ë”: st, openai, bge, hf
        """
    )
    
    # í•„ìˆ˜ ì¸ì
    parser.add_argument('input_file', help='ì…ë ¥ Excel íŒŒì¼ (ë¬¸ì¥ì‹ë³„ì, ì›ë¬¸, ë²ˆì—­ë¬¸ ì»¬ëŸ¼)')
    parser.add_argument('output_file', help='ì¶œë ¥ Excel íŒŒì¼')
    
    # ì„ íƒ ì¸ì
    parser.add_argument('--tokenizer', '-t', default='jieba', 
                       choices=['jieba', 'mecab', 'soy', 'kkma'],
                       help='í† í¬ë‚˜ì´ì € ì„ íƒ (ê¸°ë³¸: jieba)')
    
    parser.add_argument('--embedder', '-e', default='st',
                       choices=['st', 'sentence_transformer', 'openai', 'bge', 'hf'], 
                       help='ì„ë² ë” ì„ íƒ (ê¸°ë³¸: st)')
    
    parser.add_argument('--parallel', '-p', action='store_true',
                       help='ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”')
    
    parser.add_argument('--no-semantic', action='store_true',
                       help='ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­ ë¹„í™œì„±í™”')
    
    parser.add_argument('--min-tokens', type=int, default=1,
                       help='ìµœì†Œ í† í° ìˆ˜ (ê¸°ë³¸: 1)')
    
    parser.add_argument('--max-tokens', type=int, default=10,
                       help='ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 10)')
    
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥')
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    setup_logging(args.verbose)
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.input_file):
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input_file}")
        sys.exit(1)
    
    # ì²˜ë¦¬ ì‹¤í–‰
    success = process_single_file(
        input_file=args.input_file,
        output_file=args.output_file,
        tokenizer_name=args.tokenizer,
        embedder_name=args.embedder,
        use_semantic=not args.no_semantic,
        min_tokens=args.min_tokens,
        max_tokens=args.max_tokens,
        parallel=args.parallel
    )
    
    if success:
        print("\nğŸ‰ ì²˜ë¦¬ ì„±ê³µ!")
        sys.exit(0)
    else:
        print("\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨!")
        sys.exit(1)

if __name__ == "__main__":
    main()