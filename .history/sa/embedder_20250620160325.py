"""í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±ê¸°"""

import numpy as np
from sentence_transformers import SentenceTransformer
import logging
from typing import List, Optional
import torch

logger = logging.getLogger(__name__)

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜ (í•œ ë²ˆë§Œ ë¡œë“œ)
_model = None

def get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _model
    
    if _model is None:
        try:
            logger.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
            
            # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPU, ì•„ë‹ˆë©´ CPU
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.info(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
            
            # ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ ì‚¬ìš©
            model_name = "paraphrase-multilingual-MiniLM-L12-v2"
            _model = SentenceTransformer(model_name, device=device)
            
            logger.info(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.info("ğŸ”„ ê¸°ë³¸ ë”ë¯¸ ëª¨ë¸ë¡œ ëŒ€ì²´...")
            _model = "dummy"  # ë”ë¯¸ ëª¨ë¸ í‘œì‹œ
    
    return _model

def get_embeddings(texts: List[str]) -> List[np.ndarray]:
    """
    í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
    
    Args:
        texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        List[np.ndarray]: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
    """
    if not texts:
        return []
    
    try:
        model = get_embedding_model()
        
        if model == "dummy":
            # ë”ë¯¸ ì„ë² ë”© ìƒì„± (ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ)
            logger.warning("âš ï¸ ë”ë¯¸ ì„ë² ë”© ì‚¬ìš© ì¤‘")
            return [np.random.randn(384) for _ in texts]  # MiniLM ì°¨ì›ìˆ˜
        
        # ì‹¤ì œ ì„ë² ë”© ìƒì„±
        embeddings = model.encode(texts, convert_to_numpy=True)
        
        logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(texts)}ê°œ â†’ {embeddings.shape}")
        
        return [emb for emb in embeddings]
        
    except Exception as e:
        logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        logger.info("ğŸ”„ ë”ë¯¸ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ì²´")
        
        # ì˜¤ë¥˜ ì‹œ ë”ë¯¸ ì„ë² ë”© ë°˜í™˜
        return [np.random.randn(384) for _ in texts]

def get_similarity(text1: str, text2: str) -> float:
    """
    ë‘ í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        text1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
        text2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸
    
    Returns:
        float: ìœ ì‚¬ë„ (0~1)
    """
    try:
        embeddings = get_embeddings([text1, text2])
        
        if len(embeddings) != 2:
            return 0.0
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        emb1, emb2 = embeddings[0], embeddings[1]
        
        # ì •ê·œí™”
        emb1_norm = emb1 / np.linalg.norm(emb1)
        emb2_norm = emb2 / np.linalg.norm(emb2)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        similarity = np.dot(emb1_norm, emb2_norm)
        
        # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
        similarity = (similarity + 1) / 2
        
        return float(similarity)
        
    except Exception as e:
        logger.error(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return 0.0

def batch_similarity(texts1: List[str], texts2: List[str]) -> np.ndarray:
    """
    ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        texts1: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        texts2: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        np.ndarray: ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ (len(texts1) x len(texts2))
    """
    try:
        if not texts1 or not texts2:
            return np.zeros((len(texts1), len(texts2)))
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ ì„ë² ë”©
        all_texts = texts1 + texts2
        all_embeddings = get_embeddings(all_texts)
        
        if len(all_embeddings) != len(all_texts):
            logger.error("âŒ ì„ë² ë”© ìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŒ")
            return np.zeros((len(texts1), len(texts2)))
        
        # ë¶„í• 
        emb1 = np.array(all_embeddings[:len(texts1)])
        emb2 = np.array(all_embeddings[len(texts1):])
        
        # ì •ê·œí™”
        emb1_norm = emb1 / np.linalg.norm(emb1, axis=1, keepdims=True)
        emb2_norm = emb2 / np.linalg.norm(emb2, axis=1, keepdims=True)
        
        # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        similarity_matrix = np.dot(emb1_norm, emb2_norm.T)
        
        # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
        similarity_matrix = (similarity_matrix + 1) / 2
        
        return similarity_matrix
        
    except Exception as e:
        logger.error(f"âŒ ë°°ì¹˜ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return np.zeros((len(texts1), len(texts2)))

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.INFO)
    
    test_texts = ["èˆˆä¹Ÿë¼", "èˆˆì´ë‹¤.", "è’¹ì€ è–•ì´ìš”"]
    
    print("ğŸ§ª ì„ë² ë”© í…ŒìŠ¤íŠ¸")
    embeddings = get_embeddings(test_texts)
    print(f"âœ… ì„ë² ë”© í˜•íƒœ: {[emb.shape for emb in embeddings]}")
    
    print("\nğŸ§ª ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸")
    sim = get_similarity("èˆˆä¹Ÿë¼", "èˆˆì´ë‹¤.")
    print(f"âœ… ìœ ì‚¬ë„: {sim:.3f}")
    
    print("\nğŸ§ª ë°°ì¹˜ ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸")
    batch_sim = batch_similarity(["èˆˆä¹Ÿë¼"], ["èˆˆì´ë‹¤.", "è’¹ì€ è–•ì´ìš”"])
    print(f"âœ… ë°°ì¹˜ ìœ ì‚¬ë„:\n{batch_sim}")