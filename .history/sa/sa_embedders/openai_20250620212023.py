"""OpenAI ì„ë² ë”© API ê¸°ë°˜ ì„ë² ë”"""

import os
import logging
import numpy as np
from typing import List, Optional, Union
import json
import hashlib
from pathlib import Path

logger = logging.getLogger(__name__)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    import openai
    logger.info("âœ… OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
except ImportError:
    logger.error("âŒ OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install openai")
    openai = None

# ìºì‹œ ì„¤ì •
CACHE_DIR = Path("embeddings_cache_openai")
CACHE_DIR.mkdir(exist_ok=True)
CACHE_FILE = CACHE_DIR / "openai_embeddings.json"

# ì„ë² ë”© ìºì‹œ (ë©”ëª¨ë¦¬)
_embedding_cache = {}

def _load_cache():
    """ìºì‹œ íŒŒì¼ì—ì„œ ì„ë² ë”© ë¡œë“œ"""
    global _embedding_cache
    
    if CACHE_FILE.exists():
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                _embedding_cache = {k: np.array(v) for k, v in cache_data.items()}
            logger.info(f"ğŸ“‚ OpenAI ìºì‹œ ë¡œë“œ: {len(_embedding_cache)}ê°œ í•­ëª©")
        except Exception as e:
            logger.warning(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            _embedding_cache = {}
    else:
        _embedding_cache = {}

def _save_cache():
    """ì„ë² ë”©ì„ ìºì‹œ íŒŒì¼ì— ì €ì¥"""
    global _embedding_cache
    
    try:
        cache_data = {k: v.tolist() for k, v in _embedding_cache.items()}
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False)
        logger.info(f"ğŸ’¾ OpenAI ìºì‹œ ì €ì¥: {len(_embedding_cache)}ê°œ í•­ëª©")
    except Exception as e:
        logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def _get_cache_key(text: str) -> str:
    """í…ìŠ¤íŠ¸ì— ëŒ€í•œ ìºì‹œ í‚¤ ìƒì„±"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def _get_openai_client():
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    
    if openai is None:
        raise ImportError("OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError(
            "OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
            "Windows: set OPENAI_API_KEY=your-api-key\n"
            "Linux/Mac: export OPENAI_API_KEY=your-api-key"
        )
    
    return openai.OpenAI(api_key=api_key)

def compute_embeddings_batch(
    texts: List[str], 
    model: str = "text-embedding-3-small",
    max_batch_size: int = 100
) -> List[np.ndarray]:
    """OpenAI APIë¡œ ë°°ì¹˜ ì„ë² ë”© ìƒì„±"""
    
    if not texts:
        return []
    
    try:
        client = _get_openai_client()
        
        # ë°°ì¹˜ í¬ê¸° ì œí•œ
        all_embeddings = []
        
        for i in range(0, len(texts), max_batch_size):
            batch_texts = texts[i:i + max_batch_size]
            
            logger.info(f"ğŸ”„ OpenAI API í˜¸ì¶œ: {len(batch_texts)}ê°œ í…ìŠ¤íŠ¸ (ë°°ì¹˜ {i//max_batch_size + 1})")
            
            response = client.embeddings.create(
                model=model,
                input=batch_texts,
                encoding_format="float"
            )
            
            batch_embeddings = [np.array(item.embedding) for item in response.data]
            all_embeddings.extend(batch_embeddings)
            
            logger.info(f"âœ… OpenAI ì„ë² ë”© ìƒì„±: {len(batch_embeddings)}ê°œ â†’ ì°¨ì›: {len(batch_embeddings[0])}")
        
        return all_embeddings
        
    except Exception as e:
        logger.error(f"âŒ OpenAI ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        raise

def compute_embeddings_with_cache(
    texts: Union[str, List[str]], 
    model: str = "text-embedding-3-small",
    use_cache: bool = True
) -> Union[np.ndarray, List[np.ndarray]]:
    """ìºì‹œë¥¼ ì‚¬ìš©í•œ OpenAI ì„ë² ë”© ìƒì„±"""
    
    # ì´ˆê¸°í™”
    if not _embedding_cache and use_cache:
        _load_cache()
    
    # ë‹¨ì¼ í…ìŠ¤íŠ¸ ì²˜ë¦¬
    if isinstance(texts, str):
        texts = [texts]
        return_single = True
    else:
        return_single = False
    
    # ìºì‹œì—ì„œ ì°¾ê¸°
    cached_embeddings = {}
    missing_texts = []
    missing_indices = []
    
    if use_cache:
        for i, text in enumerate(texts):
            cache_key = _get_cache_key(text)
            if cache_key in _embedding_cache:
                cached_embeddings[i] = _embedding_cache[cache_key]
            else:
                missing_texts.append(text)
                missing_indices.append(i)
    else:
        missing_texts = texts
        missing_indices = list(range(len(texts)))
    
    # ìºì‹œ íˆíŠ¸ ë¡œê·¸
    if use_cache and cached_embeddings:
        logger.info(f"ğŸ“‚ ìºì‹œ íˆíŠ¸: {len(cached_embeddings)}ê°œ, ëˆ„ë½: {len(missing_texts)}ê°œ")
    
    # ëˆ„ë½ëœ í…ìŠ¤íŠ¸ë“¤ API í˜¸ì¶œ
    new_embeddings = {}
    if missing_texts:
        try:
            batch_embeddings = compute_embeddings_batch(missing_texts, model)
            
            for i, (idx, embedding) in enumerate(zip(missing_indices, batch_embeddings)):
                new_embeddings[idx] = embedding
                
                # ìºì‹œì— ì €ì¥
                if use_cache:
                    cache_key = _get_cache_key(missing_texts[i])
                    _embedding_cache[cache_key] = embedding
            
            # ìºì‹œ íŒŒì¼ ì €ì¥
            if use_cache and new_embeddings:
                _save_cache()
                
        except Exception as e:
            logger.error(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            raise
    
    # ê²°ê³¼ ì¡°í•©
    all_embeddings = []
    for i in range(len(texts)):
        if i in cached_embeddings:
            all_embeddings.append(cached_embeddings[i])
        elif i in new_embeddings:
            all_embeddings.append(new_embeddings[i])
        else:
            raise ValueError(f"ì„ë² ë”©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {texts[i]}")
    
    logger.info(f"âœ… OpenAI ì„ë² ë”© ì™„ë£Œ: {len(all_embeddings)}ê°œ")
    
    if return_single:
        return all_embeddings[0]
    else:
        return all_embeddings

def get_embedding_dimension(model: str = "text-embedding-3-small") -> int:
    """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
    
    dimension_map = {
        "text-embedding-3-small": 1536,
        "text-embedding-3-large": 3072,
        "text-embedding-ada-002": 1536
    }
    
    return dimension_map.get(model, 1536)

def test_openai_connection(model: str = "text-embedding-3-small") -> bool:
    """OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸"""
    
    try:
        logger.info("ğŸ” OpenAI API ì—°ê²° í…ŒìŠ¤íŠ¸...")
        
        test_embeddings = compute_embeddings_with_cache(
            ["í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤."], 
            model=model,
            use_cache=False
        )
        
        logger.info(f"âœ… OpenAI ì—°ê²° ì„±ê³µ! ì°¨ì›: {len(test_embeddings[0])}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ OpenAI ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

# ëª¨ë“ˆ ë¡œë“œì‹œ ì—°ê²° í…ŒìŠ¤íŠ¸ (ì˜µì…˜)
if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ì‹œì—ë§Œ í…ŒìŠ¤íŠ¸
    if test_openai_connection():
        print("ğŸ‰ OpenAI ì„ë² ë” ì •ìƒ ì‘ë™!")
    else:
        print("âŒ OpenAI ì„ë² ë” ì—°ê²° ì‹¤íŒ¨!")