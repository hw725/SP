"""ê°œì„ ëœ í† í¬ë‚˜ì´ì € - ê³µë°± ë³´ì¡´ ë° ì˜ë¯¸ ë‹¨ìœ„ ìµœì í™”"""

import jieba
import MeCab
import logging
import re
from typing import List, Optional, Callable
import numpy as np

logger = logging.getLogger(__name__)

# MeCab ì´ˆê¸°í™”
try:
    mecab = MeCab.Tagger('-Owakati')  # ê³µë°± ë¶„í•  ëª¨ë“œ
    logger.info("MeCab ì´ˆê¸°í™” ì„±ê³µ")
except Exception as e:
    logger.error(f"MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    mecab = None

def split_src_meaning_units(
    text: str, 
    min_tokens: int = 1, 
    max_tokens: int = 10,
    use_advanced: bool = True
) -> List[str]:
    """
    ê°œì„ ëœ ì›ë¬¸ ë¶„í•  - ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´
    """
    if not text or not text.strip():
        return []
    
    try:
        logger.debug(f"ì›ë¬¸ ë¶„í•  ì‹œì‘: {text[:50]}...")
        
        # 1ë‹¨ê³„: êµ¬ë¬¸ ê²½ê³„ ê¸°ë°˜ ë¶„í• 
        units = syntactic_src_split(text)
        
        # 2ë‹¨ê³„: ì˜ë¯¸ ë‹¨ìœ„ ì¡°ì •
        if use_advanced:
            units = adjust_semantic_units(units, min_tokens, max_tokens)
        
        logger.debug(f"ì›ë¬¸ ë¶„í•  ì™„ë£Œ: {len(units)}ê°œ ë‹¨ìœ„")
        return units
        
    except Exception as e:
        logger.error(f"ì›ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [text]

def syntactic_src_split(text: str) -> List[str]:
    """êµ¬ë¬¸ ê²½ê³„ ê¸°ë°˜ ì›ë¬¸ ë¶„í• """
    
    # ì£¼ìš” êµ¬ë¬¸ ê²½ê³„ í‘œì‹œì–´ë“¤
    primary_delimiters = [
        'ç„¶å¾Œì—', 'ç„¶å¾Œ',     # ì‹œê°„ ì ‘ì†
        'å‰‡', 'è€Œ', 'ä¸”',     # í•œë¬¸ ì ‘ì†ì‚¬
        'ì´ìš”', 'ì´ë©°',       # ë³‘ë ¬ ì ‘ì†
        'ë¼ê°€', 'ë¼ì„œ',       # ì „í™˜ ì ‘ì†
        'ë©´', 'ì´ë©´', 'í•˜ë©´'  # ì¡°ê±´ ì ‘ì†
    ]
    
    # ë³´ì¡° ê²½ê³„ í‘œì‹œì–´ë“¤
    secondary_delimiters = [
        'äº‘', 'æ›°',          # ì¸ìš©
        'è€…', 'ä¹‹',          # ê´€ê³„ì‚¬
        'ä»¥', 'æ–¼'           # ì „ì¹˜ì‚¬ë¥˜
    ]
    
    units = [text]
    
    # 1ì°¨: ì£¼ìš” êµ¬ë¶„ìë¡œ ë¶„í• 
    for delimiter in primary_delimiters:
        new_units = []
        for unit in units:
            if delimiter in unit:
                parts = split_preserving_delimiter(unit, delimiter)
                new_units.extend(parts)
            else:
                new_units.append(unit)
        units = new_units
    
    # 2ì°¨: ë„ˆë¬´ ê¸´ ë‹¨ìœ„ëŠ” ë³´ì¡° êµ¬ë¶„ìë¡œ ì¶”ê°€ ë¶„í• 
    final_units = []
    for unit in units:
        if len(unit) > 30:  # ê¸´ ë‹¨ìœ„ë§Œ ì¶”ê°€ ë¶„í• 
            sub_units = secondary_split(unit, secondary_delimiters)
            final_units.extend(sub_units)
        else:
            final_units.append(unit)
    
    return [u.strip() for u in final_units if u.strip()]

def split_preserving_delimiter(text: str, delimiter: str) -> List[str]:
    """êµ¬ë¶„ìë¥¼ í¬í•¨í•˜ì—¬ ë¶„í•  (ì˜ë¯¸ ë³´ì¡´)"""
    if delimiter not in text:
        return [text]
    
    parts = text.split(delimiter)
    result = []
    
    for i, part in enumerate(parts[:-1]):  # ë§ˆì§€ë§‰ ì œì™¸
        if part.strip():
            result.append(part + delimiter)
    
    # ë§ˆì§€ë§‰ ë¶€ë¶„ ì²˜ë¦¬
    if parts[-1].strip():
        result.append(parts[-1])
    
    return result

def secondary_split(text: str, delimiters: List[str]) -> List[str]:
    """ë³´ì¡° êµ¬ë¶„ìë¡œ ì¶”ê°€ ë¶„í• """
    
    # í•œìì–´ ë¸”ë¡ + ì¡°ì‚¬ íŒ¨í„´ ê°ì§€
    hanja_pattern = r'([\u4e00-\u9fff]{2,}(?:[\uac00-\ud7af]{1,2})?)'
    blocks = re.findall(hanja_pattern, text)
    
    if len(blocks) >= 2:
        # íŒ¨í„´ ê¸°ë°˜ ë¶„í• 
        result = []
        remaining = text
        
        for block in blocks:
            if block in remaining:
                idx = remaining.find(block)
                if idx > 0:
                    result.append(remaining[:idx + len(block)])
                else:
                    result.append(block)
                remaining = remaining[idx + len(block):]
        
        if remaining.strip():
            result.append(remaining)
        
        return [r.strip() for r in result if r.strip()]
    
    return [text]

def split_tgt_meaning_units(
    src_text: str,
    tgt_text: str,
    embed_func: Optional[Callable] = None,
    use_semantic: bool = True,
    min_tokens: int = 1,
    max_tokens: int = 10,
    similarity_threshold: float = 0.3
) -> List[str]:
    """
    ê°œì„ ëœ ë²ˆì—­ë¬¸ ë¶„í•  - ì ì‘ì  ë¶„í• 
    """
    if not tgt_text or not tgt_text.strip():
        return []
    
    try:
        logger.debug(f"ë²ˆì—­ë¬¸ ë¶„í•  ì‹œì‘: {tgt_text[:50]}...")
        
        # ì›ë¬¸ ë‹¨ìœ„ ë¶„ì„
        src_units = split_src_meaning_units(src_text, min_tokens, max_tokens)
        
        if use_semantic and embed_func is not None:
            # ì ì‘ì  ì˜ë¯¸ ê¸°ë°˜ ë¶„í• 
            units = adaptive_tgt_split(
                src_units, tgt_text, embed_func, 
                min_tokens, max_tokens, similarity_threshold
            )
        else:
            # ê°œì„ ëœ êµ¬ë¬¸ ê¸°ë°˜ ë¶„í• 
            units = improved_tgt_split(tgt_text, len(src_units), min_tokens, max_tokens)
        
        logger.debug(f"ë²ˆì—­ë¬¸ ë¶„í•  ì™„ë£Œ: {len(units)}ê°œ ë‹¨ìœ„")
        return units
        
    except Exception as e:
        logger.error(f"ë²ˆì—­ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [tgt_text]

def improved_tgt_split(
    tgt_text: str, 
    src_unit_count: int,
    min_tokens: int = 1, 
    max_tokens: int = 10
) -> List[str]:
    """ê°œì„ ëœ MeCab ê¸°ë°˜ ë²ˆì—­ë¬¸ ë¶„í• """
    
    if mecab is None:
        return basic_punctuation_split(tgt_text, src_unit_count)
    
    try:
        # MeCabìœ¼ë¡œ í˜•íƒœì†Œ ë¶„ì„
        result = mecab.parse(tgt_text).strip()
        tokens = result.split(' ') if result else []
        
        if not tokens:
            return [tgt_text]
        
        # ì˜ë¯¸ ë‹¨ìœ„ë³„ ê·¸ë£¹í™” (ê°œì„ ëœ ë°©ì‹)
        units = improved_morpheme_grouping(
            tokens, tgt_text, src_unit_count, min_tokens, max_tokens
        )
        
        return units
        
    except Exception as e:
        logger.error(f"MeCab ë¶„í•  ì‹¤íŒ¨: {e}")
        return basic_punctuation_split(tgt_text, src_unit_count)

def improved_morpheme_grouping(
    tokens: List[str], 
    original_text: str,
    src_unit_count: int,
    min_tokens: int = 1, 
    max_tokens: int = 10
) -> List[str]:
    """ê°œì„ ëœ í˜•íƒœì†Œ ê·¸ë£¹í™” - ê³µë°± ë³´ì¡´"""
    
    if not tokens:
        return [original_text]
    
    # ëª©í‘œ ë¶„í•  ìˆ˜ ê³„ì‚° (ì ì‘ì )
    target_segments = calculate_target_segments(
        len(original_text), src_unit_count, len(tokens)
    )
    
    # ì˜ë¯¸ ê²½ê³„ ê°ì§€
    boundaries = detect_semantic_boundaries(tokens, original_text)
    
    # ìµœì  ë¶„í• ì  ì„ íƒ
    optimal_boundaries = select_optimal_boundaries(
        boundaries, target_segments, len(tokens)
    )
    
    # ì‹¤ì œ ë¶„í•  ìˆ˜í–‰ (ê³µë°± ë³´ì¡´)
    units = split_by_boundaries(tokens, original_text, optimal_boundaries)
    
    return units

def calculate_target_segments(text_length: int, src_count: int, token_count: int) -> int:
    """ì ì‘ì  ëª©í‘œ ë¶„í•  ìˆ˜ ê³„ì‚°"""
    
    # ê¸°ë³¸ ì „ëµ: ì›ë¬¸ ë‹¨ìœ„ ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•˜ë˜ í…ìŠ¤íŠ¸ íŠ¹ì„± ê³ ë ¤
    base_target = max(1, src_count)
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ë³´ì •
    if text_length > 100:  # ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸
        length_factor = 1.5
    elif text_length > 50:  # ê¸´ í…ìŠ¤íŠ¸
        length_factor = 1.2
    else:  # ì¼ë°˜ í…ìŠ¤íŠ¸
        length_factor = 1.0
    
    # í† í° ë°€ë„ ë³´ì •
    density = text_length / max(1, token_count)
    if density > 3:  # ê¸´ í† í°ë“¤ (ë³µí•©ì–´ ë§ìŒ)
        density_factor = 0.8
    else:  # ì§§ì€ í† í°ë“¤
        density_factor = 1.0
    
    target = int(base_target * length_factor * density_factor)
    
    # í•©ë¦¬ì  ë²”ìœ„ë¡œ ì œí•œ
    return max(1, min(target, token_count // 2))

def detect_semantic_boundaries(tokens: List[str], original_text: str) -> List[int]:
    """ì˜ë¯¸ ê²½ê³„ ê°ì§€"""
    boundaries = []
    
    # êµ¬ë‘ì  ê²½ê³„
    for i, token in enumerate(tokens):
        if re.search(r'[.!?ã€‚ï¼ï¼Ÿ,ï¼Œ;ï¼š:]', token):
            boundaries.append(i + 1)
    
    # ì ‘ì† í‘œí˜„ ê²½ê³„
    connector_patterns = [
        r'ê·¸ëŸ°ë°|í•˜ì§€ë§Œ|ë”°ë¼ì„œ|ê·¸ëŸ¬ë¯€ë¡œ|ì¦‰|ë˜í•œ|ê·¸ë¦¬ê³ ',
        r'ì„|ë¥¼|ì´|ê°€|ì€|ëŠ”|ì—ì„œ|ìœ¼ë¡œ|ì™€|ê³¼',  # ì£¼ìš” ì¡°ì‚¬
        r'í–ˆë‹¤ê°€|ë˜ë©´|ë•Œë¬¸ì—|í•˜ì—¬|ë¯€ë¡œ'  # ì—°ê²° ì–´ë¯¸
    ]
    
    for i, token in enumerate(tokens):
        for pattern in connector_patterns:
            if re.search(pattern, token):
                boundaries.append(i + 1)
                break
    
    # í•œìì–´ ë¸”ë¡ ê²½ê³„
    for i in range(len(tokens) - 1):
        current_has_hanja = bool(re.search(r'[\u4e00-\u9fff]', tokens[i]))
        next_has_hanja = bool(re.search(r'[\u4e00-\u9fff]', tokens[i + 1]))
        
        # í•œìì–´ â†’ í•œê¸€ì–´ ë˜ëŠ” ê·¸ ë°˜ëŒ€
        if current_has_hanja != next_has_hanja:
            boundaries.append(i + 1)
    
    return sorted(set(boundaries))

def select_optimal_boundaries(
    boundaries: List[int], 
    target_segments: int, 
    total_tokens: int
) -> List[int]:
    """ìµœì  ë¶„í• ì  ì„ íƒ"""
    
    if not boundaries or target_segments <= 1:
        return [total_tokens]
    
    # ê²½ê³„ì ë“¤ì„ ê· ë“± ë¶„í¬ì— ê°€ê¹ê²Œ ì„ íƒ
    if len(boundaries) <= target_segments - 1:
        return boundaries + [total_tokens]
    
    # ë„ˆë¬´ ë§ì€ ê²½ê³„ì ì´ ìˆëŠ” ê²½ìš°, ê· ë“±í•˜ê²Œ ì„ íƒ
    selected = []
    interval = len(boundaries) / (target_segments - 1)
    
    for i in range(target_segments - 1):
        idx = int(i * interval)
        selected.append(boundaries[idx])
    
    selected.append(total_tokens)
    return sorted(set(selected))

def split_by_boundaries(
    tokens: List[str], 
    original_text: str, 
    boundaries: List[int]
) -> List[str]:
    """ê²½ê³„ì ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (ê³µë°± ë³´ì¡´)"""
    
    if not boundaries:
        return [original_text]
    
    units = []
    start = 0
    
    for boundary in boundaries:
        if boundary > start:
            # í† í° ë²”ìœ„ì˜ ì›ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            segment_tokens = tokens[start:boundary]
            if segment_tokens:
                # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë¶€ë¶„ ì°¾ê¸°
                segment_text = reconstruct_segment(segment_tokens, original_text, start)
                if segment_text.strip():
                    units.append(segment_text.strip())
            start = boundary
    
    return units

def reconstruct_segment(
    segment_tokens: List[str], 
    original_text: str, 
    token_start_idx: int
) -> str:
    """í† í°ë“¤ë¡œë¶€í„° ì›ë³¸ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ ì¬êµ¬ì„±"""
    
    if not segment_tokens:
        return ""
    
    # ë‹¨ìˆœ ê²°í•© ì‹œë„
    simple_join = ''.join(segment_tokens)
    
    # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ë¶€ë¶„ ì°¾ê¸°
    if simple_join in original_text:
        start_pos = original_text.find(simple_join)
        if start_pos != -1:
            return original_text[start_pos:start_pos + len(simple_join)]
    
    # ë¶€ë¶„ ë§¤ì¹­ìœ¼ë¡œ ì¬êµ¬ì„±
    result = ""
    remaining_text = original_text
    
    for token in segment_tokens:
        if token in remaining_text:
            pos = remaining_text.find(token)
            # í† í° ì•ì˜ ê³µë°±ê¹Œì§€ í¬í•¨
            if pos > 0 and remaining_text[pos-1] == ' ':
                result += remaining_text[:pos + len(token)]
            else:
                result += remaining_text[:pos + len(token)]
            remaining_text = remaining_text[pos + len(token):]
        else:
            result += token
    
    return result

def adaptive_tgt_split(
    src_units: List[str], 
    tgt_text: str, 
    embed_func: Callable,
    min_tokens: int,
    max_tokens: int,
    similarity_threshold: float = 0.3
) -> List[str]:
    """ì ì‘ì  ì˜ë¯¸ ê¸°ë°˜ ë²ˆì—­ë¬¸ ë¶„í• """
    
    try:
        # 1. ê¸°ë³¸ ë¶„í•  ìˆ˜í–‰
        base_units = improved_tgt_split(
            tgt_text, len(src_units), min_tokens, max_tokens
        )
        
        # 2. ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¬ì¡°í•©
        if len(src_units) > 1 and len(base_units) > 1:
            optimized_units = semantic_optimization(
                src_units, base_units, embed_func, similarity_threshold
            )
            return optimized_units
        
        return base_units
        
    except Exception as e:
        logger.error(f"ì ì‘ì  ë¶„í•  ì‹¤íŒ¨: {e}")
        return improved_tgt_split(tgt_text, len(src_units), min_tokens, max_tokens)

def semantic_optimization(
    src_units: List[str], 
    tgt_candidates: List[str], 
    embed_func: Callable,
    threshold: float = 0.3
) -> List[str]:
    """ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ ìµœì í™”"""
    
    try:
        # ì„ë² ë”© ê³„ì‚°
        all_texts = src_units + tgt_candidates
        embeddings = embed_func(all_texts)
        
        if len(embeddings) != len(all_texts):
            return tgt_candidates
        
        src_embeddings = embeddings[:len(src_units)]
        tgt_embeddings = embeddings[len(src_units):]
        
        # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤
        similarity_matrix = compute_similarity_matrix(src_embeddings, tgt_embeddings)
        
        # ë™ì  í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ìµœì  ë¶„í•  ì°¾ê¸°
        optimal_grouping = find_optimal_tgt_grouping(
            tgt_candidates, similarity_matrix, len(src_units), threshold
        )
        
        return optimal_grouping
        
    except Exception as e:
        logger.error(f"ì˜ë¯¸ ìµœì í™” ì‹¤íŒ¨: {e}")
        return tgt_candidates

def compute_similarity_matrix(src_embeddings: List, tgt_embeddings: List) -> np.ndarray:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
    
    try:
        src_matrix = np.array(src_embeddings)
        tgt_matrix = np.array(tgt_embeddings)
        
        # ì •ê·œí™”
        src_norm = src_matrix / np.linalg.norm(src_matrix, axis=1, keepdims=True)
        tgt_norm = tgt_matrix / np.linalg.norm(tgt_matrix, axis=1, keepdims=True)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        similarity = np.dot(src_norm, tgt_norm.T)
        
        # 0~1 ë²”ìœ„ë¡œ ë³€í™˜
        return (similarity + 1) / 2
        
    except Exception as e:
        logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return np.zeros((len(src_embeddings), len(tgt_embeddings)))

def find_optimal_tgt_grouping(
    tgt_candidates: List[str], 
    similarity_matrix: np.ndarray,
    target_count: int,
    threshold: float = 0.3
) -> List[str]:
    """ìµœì  íƒ€ê²Ÿ ê·¸ë£¹í™” (ë™ì  í”„ë¡œê·¸ë˜ë°)"""
    
    try:
        n = len(tgt_candidates)
        if n <= target_count:
            return tgt_candidates
        
        # DP í…Œì´ë¸”: dp[i][j] = ië²ˆì§¸ê¹Œì§€ jê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„í• í–ˆì„ ë•Œ ìµœëŒ€ ì ìˆ˜
        dp = np.full((n + 1, target_count + 1), -np.inf)
        backtrack = np.zeros((n + 1, target_count + 1, 2), dtype=int)
        
        dp[0][0] = 0
        
        for i in range(1, n + 1):
            for j in range(1, min(i, target_count) + 1):
                # kë¶€í„° i-1ê¹Œì§€ë¥¼ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œ ë§Œë“œëŠ” ê²½ìš°
                for k in range(j - 1, i):
                    if dp[k][j-1] == -np.inf:
                        continue
                    
                    # ê·¸ë£¹ [k:i]ì˜ ì ìˆ˜ ê³„ì‚°
                    group_score = calculate_group_score(
                        tgt_candidates[k:i], similarity_matrix, j-1
                    )
                    
                    total_score = dp[k][j-1] + group_score
                    
                    if total_score > dp[i][j]:
                        dp[i][j] = total_score
                        backtrack[i][j] = [k, j-1]
        
        # ë°±íŠ¸ë˜í‚¹ìœ¼ë¡œ ìµœì  ë¶„í•  ì°¾ê¸°
        groups = []
        i, j = n, target_count
        
        while j > 0:
            k, prev_j = backtrack[i][j]
            if k < i:
                group = ''.join(tgt_candidates[k:i])
                groups.append(group)
            i, j = k, prev_j
        
        groups.reverse()
        return groups if groups else tgt_candidates
        
    except Exception as e:
        logger.error(f"ìµœì  ê·¸ë£¹í™” ì‹¤íŒ¨: {e}")
        return tgt_candidates

def calculate_group_score(
    group_candidates: List[str],
    similarity_matrix: np.ndarray,
    src_idx: int
) -> float:
    """ê·¸ë£¹ ì ìˆ˜ ê³„ì‚°"""
    
    if not group_candidates or src_idx >= similarity_matrix.shape[0]:
        return 0.0
    
    # ê·¸ë£¹ ë‚´ í›„ë³´ë“¤ì˜ í‰ê·  ìœ ì‚¬ë„
    scores = []
    for i, candidate in enumerate(group_candidates):
        if src_idx < similarity_matrix.shape[0] and i < similarity_matrix.shape[1]:
            scores.append(similarity_matrix[src_idx, i])
    
    return np.mean(scores) if scores else 0.0

def basic_punctuation_split(text: str, target_count: int) -> List[str]:
    """ê¸°ë³¸ êµ¬ë‘ì  ê¸°ë°˜ ë¶„í•  (ë°±ì—…ìš©)"""
    
    # êµ¬ë‘ì ìœ¼ë¡œ 1ì°¨ ë¶„í• 
    delimiters = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', ',', 'ï¼Œ', ';', 'ï¼š', ':']
    
    units = [text]
    for delimiter in delimiters:
        new_units = []
        for unit in units:
            if delimiter in unit:
                parts = unit.split(delimiter)
                for i, part in enumerate(parts[:-1]):
                    if part.strip():
                        new_units.append(part + delimiter)
                if parts[-1].strip():
                    new_units.append(parts[-1])
            else:
                new_units.append(unit)
        units = new_units
    
    # ëª©í‘œ ê°œìˆ˜ì— ë§ì¶° ì¡°ì •
    units = [u.strip() for u in units if u.strip()]
    
    if len(units) > target_count and target_count > 1:
        # ë„ˆë¬´ ë§ìœ¼ë©´ ë³‘í•©
        merged = []
        chunk_size = len(units) // target_count
        for i in range(0, len(units), chunk_size):
            chunk = units[i:i + chunk_size]
            merged.append(' '.join(chunk))
        units = merged
    
    return units

def adjust_semantic_units(
    units: List[str], 
    min_tokens: int, 
    max_tokens: int
) -> List[str]:
    """ì˜ë¯¸ ë‹¨ìœ„ ê¸¸ì´ ì¡°ì •"""
    
    adjusted = []
    
    for unit in units:
        if len(unit) > max_tokens * 4:  # ë„ˆë¬´ ê¸´ ë‹¨ìœ„ ë¶„í• 
            # ì¤‘ê°„ ì§€ì ì—ì„œ ì ì ˆí•œ ë¶„í• ì  ì°¾ê¸°
            mid = len(unit) // 2
            for i in range(mid - 5, mid + 5):
                if i > 0 and i < len(unit) and unit[i] in ' ï¼Œ,ã€':
                    adjusted.append(unit[:i+1].strip())
                    adjusted.append(unit[i+1:].strip())
                    break
            else:
                # ì ì ˆí•œ ë¶„í• ì ì´ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ
                adjusted.append(unit)
        else:
            adjusted.append(unit)
    
    # ë„ˆë¬´ ì§§ì€ ë‹¨ìœ„ë“¤ ë³‘í•©
    if min_tokens > 1:
        merged = []
        temp = ""
        
        for unit in adjusted:
            if len(temp + unit) < min_tokens * 2:
                temp += unit
            else:
                if temp:
                    merged.append(temp.strip())
                temp = unit
        
        if temp:
            merged.append(temp.strip())
        
        adjusted = merged
    
    return [u for u in adjusted if u.strip()]

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.DEBUG)
    
    print("ğŸ§ª ê°œì„ ëœ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸")
    
    test_cases = [
        ("èˆˆä¹Ÿë¼", "èˆˆì´ë‹¤."),
        ("è’¹ì€ è–•(ë ´)ì´ìš” è‘­ëŠ” è˜†ä¹Ÿë¼", "è’¹ì€ ë¬¼ì–µìƒˆì´ê³  è‘­ëŠ” ê°ˆëŒ€ì´ë‹¤."),
        ("ç™½éœ²å‡æˆ¾çˆ²éœœç„¶å¾Œì— æ­²äº‹æˆì´ìš” åœ‹å®¶å¾…ç¦®ç„¶å¾Œèˆˆì´ë¼", 
         "ç™½éœ²ê°€ ì–¼ì–´ ì„œë¦¬ê°€ ëœ ë’¤ì—ì•¼ æ­²äº‹ê°€ ì´ë£¨ì–´ì§€ê³  åœ‹å®¶ëŠ” ç¦®ê°€ í–‰í•´ì§„ ë’¤ì—ì•¼ í¥ì„±í•œë‹¤."),
    ]
    
    for i, (src, tgt) in enumerate(test_cases, 1):
        print(f"\n=== í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i} ===")
        print(f"ì›ë¬¸: {src}")
        print(f"ë²ˆì—­: {tgt}")
        
        # ì›ë¬¸ ë¶„í• 
        src_units = split_src_meaning_units(src, min_tokens=1, max_tokens=15)
        print(f"âœ… ê°œì„ ëœ ì›ë¬¸ ë¶„í• : {src_units}")
        
        # ë²ˆì—­ë¬¸ ë¶„í•  (êµ¬ë¬¸ ê¸°ë°˜)
        tgt_units = split_tgt_meaning_units(
            src, tgt, 
            embed_func=None, 
            use_semantic=False,
            min_tokens=1, 
            max_tokens=15
        )
        print(f"âœ… ê°œì„ ëœ ë²ˆì—­ ë¶„í• : {tgt_units}")
        
        # ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  (ë”ë¯¸ ì„ë² ë”©)
        def dummy_embed(texts):
            return [np.random.randn(100) for _ in texts]
        
        tgt_units_semantic = split_tgt_meaning_units(
            src, tgt,
            embed_func=dummy_embed,
            use_semantic=True,
            min_tokens=1,
            max_tokens=15
        )
        print(f"ğŸ”— ì˜ë¯¸ ê¸°ë°˜ ë¶„í• : {tgt_units_semantic}")