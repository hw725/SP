"""í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• ëª¨ë“ˆ - ê³µë°± ë³´ì¡´ ê°œì„ """

import jieba
import logging
import re
import regex
from typing import List, Optional, Callable
import numpy as np

logger = logging.getLogger(__name__)

# MeCab ì´ˆê¸°í™”
mecab = None
try:
    import MeCab
    mecab = MeCab.Tagger()
    logger.info("âœ… MeCab-ko ì´ˆê¸°í™” ì„±ê³µ")
except ImportError:
    logger.warning("âš ï¸ MeCab ì—†ìŒ, ê¸°ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©")
    mecab = None
except Exception as e:
    logger.warning(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    mecab = None

def split_src_meaning_units(
    text: str, 
    min_tokens: int = 1, 
    max_tokens: int = 10,
    use_advanced: bool = True
) -> List[str]:
    """ì›ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
    
    if not text or not text.strip():
        return []
    
    try:
        # í•œë¬¸ êµ¬ë¬¸ ê²½ê³„ íŒ¨í„´
        patterns = [
            r'ç„¶å¾Œì—',
            r'ì´ìš”(?=\s|\p{Han}|\p{Hangul}|$)',
            r'ì´ë¼ê°€',
            r'ì´ë©´',
            r'í•˜ë©´',
            r'å‰‡(?=\s|\p{Han}|\p{Hangul})',
            r'è€Œ(?=\s|\p{Han}|\p{Hangul})',
            r'ä¸”(?=\s|\p{Han}|\p{Hangul})'
        ]
        
        units = [text]
        
        for pattern in patterns:
            new_units = []
            for unit in units:
                if regex.search(pattern, unit):
                    parts = regex.split(f'({pattern})', unit)
                    current = ""
                    for part in parts:
                        if regex.match(pattern, part):
                            if current:
                                new_units.append(current + part)
                                current = ""
                        else:
                            current += part
                    if current:
                        new_units.append(current)
                else:
                    new_units.append(unit)
            units = [u.strip() for u in new_units if u.strip()]
        
        if use_advanced:
            units = _advanced_han_split(units)
        
        return units
        
    except Exception as e:
        logger.error(f"âŒ ì›ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [text]

def _advanced_han_split(units: List[str]) -> List[str]:
    """í•œìì–´ + ì¡°ì‚¬ ë‹¨ìœ„ë¡œ ê³ ê¸‰ ë¶„í• """
    
    advanced_units = []
    
    for unit in units:
        if len(unit) > 15:
            pattern = r'(\p{Han}+\p{Hangul}*(?:ì´ë¼|ì´ìš”|ì—ì„œ|ë¼ì„œ|í•˜ì—¬|ë©´ì„œ|ì—|ëŠ”|ì€|ì´|ê°€)?)'
            matches = regex.findall(pattern, unit)
            
            if len(matches) > 1:
                remaining = unit
                for match in matches:
                    if match in remaining:
                        pos = remaining.find(match)
                        if pos > 0:
                            advanced_units.append(remaining[:pos].strip())
                        advanced_units.append(match)
                        remaining = remaining[pos + len(match):]
                
                if remaining.strip():
                    advanced_units.append(remaining.strip())
            else:
                advanced_units.append(unit)
        else:
            advanced_units.append(unit)
    
    return [u for u in advanced_units if u.strip()]

def split_tgt_meaning_units(
    src_text: str,
    tgt_text: str,
    embed_func: Optional[Callable] = None,
    use_semantic: bool = True,
    min_tokens: int = 1,
    max_tokens: int = 10,
    similarity_threshold: float = 0.3
) -> List[str]:
    """ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í•  - ê³µë°± ë³´ì¡´ ê°œì„ """
    
    if not tgt_text or not tgt_text.strip():
        return []
    
    try:
        if mecab:
            return _mecab_split_preserve_spaces(tgt_text, max_tokens)
        else:
            return _basic_split_preserve_spaces(tgt_text, max_tokens)
        
    except Exception as e:
        logger.error(f"âŒ ë²ˆì—­ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [tgt_text]

def _mecab_split_preserve_spaces(text: str, max_tokens: int) -> List[str]:
    """ğŸ”§ MeCab ë¶„í•  + ê³µë°± ë³´ì¡´"""
    
    try:
        # ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ê³µë°± ìœ„ì¹˜ ê¸°ë¡
        space_positions = []
        for i, char in enumerate(text):
            if char.isspace():
                space_positions.append(i)
        
        # MeCab í˜•íƒœì†Œ ë¶„ì„
        result = mecab.parse(text)
        morphemes = []
        position = 0
        
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    pos = parts[1].split(',')[0]
                    morphemes.append({
                        'surface': surface,
                        'pos': pos,
                        'start': position,
                        'end': position + len(surface)
                    })
                    position += len(surface)
        
        # ğŸ”§ ê³µë°±ì„ ê³ ë ¤í•œ ì˜ë¯¸ ë‹¨ìœ„ ê·¸ë£¹í™”
        units = []
        current_unit = ""
        current_start = 0
        
        for i, morph in enumerate(morphemes):
            surface = morph['surface']
            pos = morph['pos']
            start_pos = morph['start']
            
            # ì´ì „ í˜•íƒœì†Œì™€ í˜„ì¬ í˜•íƒœì†Œ ì‚¬ì´ì˜ ê³µë°± ì¶”ê°€
            if current_unit and start_pos > current_start:
                spaces_between = text[current_start:start_pos]
                current_unit += spaces_between
            
            current_unit += surface
            current_start = morph['end']
            
            # í•œì + ì¡°ì‚¬ íŠ¹ë³„ ì²˜ë¦¬
            is_han_particle_boundary = False
            if _is_han(surface) and i + 1 < len(morphemes):
                next_morph = morphemes[i + 1]
                if _is_particle_pos(next_morph['pos']):
                    # ë‹¤ìŒ ì¡°ì‚¬ê¹Œì§€ í¬í•¨
                    next_surface = next_morph['surface']
                    next_start = next_morph['start']
                    
                    # ê³µë°± í¬í•¨
                    if next_start > current_start:
                        spaces_between = text[current_start:next_start]
                        current_unit += spaces_between
                    
                    current_unit += next_surface
                    current_start = next_morph['end']
                    morphemes[i + 1]['surface'] = ''  # ìŠ¤í‚µ í‘œì‹œ
                    is_han_particle_boundary = True
            
            # ê²½ê³„ ì¡°ê±´
            is_boundary = (
                pos in ['JKS', 'JKO', 'JKC', 'JX', 'SF', 'SP'] or
                pos.startswith('E') or
                is_han_particle_boundary or
                len(current_unit.replace(' ', '')) >= max_tokens * 2 or  # ê³µë°± ì œì™¸í•œ ê¸¸ì´
                _is_natural_boundary(surface, pos)
            )
            
            if is_boundary and current_unit.strip():
                units.append(current_unit.strip())
                current_unit = ""
                current_start = morph['end']
        
        if current_unit.strip():
            units.append(current_unit.strip())
        
        return [u for u in units if u.strip()]
        
    except Exception as e:
        logger.error(f"âŒ MeCab ê³µë°± ë³´ì¡´ ë¶„í•  ì‹¤íŒ¨: {e}")
        return _basic_split_preserve_spaces(text, max_tokens)

def _basic_split_preserve_spaces(text: str, max_tokens: int) -> List[str]:
    """ğŸ”§ ê¸°ë³¸ ë¶„í•  + ê³µë°± ë³´ì¡´"""
    
    try:
        # ê³µë°±ìœ¼ë¡œ ì¼ì°¨ ë¶„í• 
        words = text.split()
        if not words:
            return [text]
        
        # ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì¬ê·¸ë£¹í™”
        units = []
        current_unit = []
        current_length = 0
        
        for word in words:
            current_unit.append(word)
            current_length += len(word)
            
            # ê²½ê³„ ì¡°ê±´
            is_boundary = (
                regex.search(r'[.!?ã€‚ï¼ï¼Ÿ]$', word) or  # êµ¬ë‘ì ìœ¼ë¡œ ëë‚¨
                _ends_with_korean_ending(word) or      # í•œêµ­ì–´ ì–´ë¯¸ë¡œ ëë‚¨
                current_length >= max_tokens * 3 or   # ê¸¸ì´ ì œí•œ
                len(current_unit) >= 5                 # ë‹¨ì–´ ìˆ˜ ì œí•œ
            )
            
            if is_boundary and current_unit:
                units.append(' '.join(current_unit))
                current_unit = []
                current_length = 0
        
        if current_unit:
            units.append(' '.join(current_unit))
        
        return [u.strip() for u in units if u.strip()]
        
    except Exception as e:
        logger.error(f"âŒ ê¸°ë³¸ ê³µë°± ë³´ì¡´ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [text]

def _is_natural_boundary(surface: str, pos: str) -> bool:
    """ìì—°ìŠ¤ëŸ¬ìš´ ê²½ê³„ì¸ì§€ íŒë‹¨"""
    
    # êµ¬ë‘ì 
    if pos in ['SF', 'SP'] or surface in ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', ',', 'ï¼Œ']:
        return True
    
    # í•œêµ­ì–´ ì–´ë¯¸
    if _ends_with_korean_ending(surface):
        return True
    
    # ì ‘ì†ì‚¬
    connectives = ['ê·¸ëŸ°ë°', 'í•˜ì§€ë§Œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ']
    if surface in connectives:
        return True
    
    return False

def _ends_with_korean_ending(word: str) -> bool:
    """í•œêµ­ì–´ ì–´ë¯¸ë¡œ ëë‚˜ëŠ”ì§€ í™•ì¸"""
    
    endings = ['ë‹¤', 'ê³ ', 'ë©°', 'ë©´ì„œ', 'ì§€ë§Œ', 'í•˜ì—¬', 'í•˜ê³ ', 'í•œë‹¤', 'ì˜€ë‹¤', 'ìŠµë‹ˆë‹¤']
    return any(word.endswith(ending) for ending in endings)

def _is_han(token: str) -> bool:
    """í•œì í¬í•¨ ì—¬ë¶€"""
    return bool(regex.search(r'\p{Han}', token))

def _is_hangul(token: str) -> bool:
    """í•œê¸€ í¬í•¨ ì—¬ë¶€"""
    return bool(regex.search(r'\p{Hangul}', token))

def _is_particle_pos(pos: str) -> bool:
    """í’ˆì‚¬ê°€ ì¡°ì‚¬ì¸ì§€ í™•ì¸"""
    particle_pos = ['JKS', 'JKO', 'JKC', 'JX']
    return pos in particle_pos

def _is_particle(token: str) -> bool:
    """ì¡°ì‚¬/ì–´ë¯¸ ì—¬ë¶€"""
    particles = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 
                'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë¼', 'ì´ë¼']
    return token in particles or (len(token) <= 2 and _is_hangul(token))

if __name__ == "__main__":
    # ê³µë°± ë³´ì¡´ í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸ§ª ê³µë°± ë³´ì¡´ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    test_cases = [
        "è’¹ì€ ë¬¼ì–µìƒˆì´ê³  è‘­ëŠ” ê°ˆëŒ€ì´ë‹¤.",
        "ç™½éœ²ê°€ ì–¼ì–´ ì„œë¦¬ê°€ ëœ ë’¤ì—ì•¼ æ­²äº‹ê°€ ì´ë£¨ì–´ì§€ê³ ",
        "ì—¬ëŸ¬ í’€ ê°€ìš´ë°ì— í‘¸ë¥´ê²Œ ë¬´ì„±í–ˆë‹¤ê°€ ç™½éœ²ê°€ ì–¼ì–´ ì„œë¦¬ê°€ ë˜ë©´"
    ]
    
    for text in test_cases:
        print(f"\nì›ë³¸: {text}")
        units = split_tgt_meaning_units("", text, use_semantic=False)
        print(f"ë¶„í• : {units}")
        
        # ê³µë°± ë³´ì¡´ í™•ì¸
        for unit in units:
            spaces = unit.count(' ')
            print(f"  '{unit}' (ê³µë°± {spaces}ê°œ)")