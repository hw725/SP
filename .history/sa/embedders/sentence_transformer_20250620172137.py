"""SentenceTransformer ê¸°ë°˜ ì„ë² ë”© ëª¨ë“ˆ"""

import logging
import numpy as np
from typing import List, Dict, Any, Optional
import hashlib
import pickle
import os
from sentence_transformers import SentenceTransformer

logger = logging.getLogger(__name__)

# ì „ì—­ ë³€ìˆ˜ë“¤
_model = None
_cache = {}
_cache_file = "embedding_cache.pkl"

def get_model():
    """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ë° ìºì‹œ"""
    global _model
    if _model is None:
        logger.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        device = "cpu"  # GPU ì‚¬ìš© ì‹œ "cuda"
        logger.info(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
        
        model_name = "paraphrase-multilingual-MiniLM-L12-v2"
        _model = SentenceTransformer(model_name, device=device)
        logger.info(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")
    
    return _model

def _get_cache_key(texts: List[str]) -> str:
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ìºì‹œ í‚¤ ìƒì„±"""
    text_str = '|'.join(sorted(texts))
    return hashlib.md5(text_str.encode()).hexdigest()

def _load_cache():
    """ìºì‹œ íŒŒì¼ ë¡œë“œ"""
    global _cache
    if os.path.exists(_cache_file):
        try:
            with open(_cache_file, 'rb') as f:
                _cache = pickle.load(f)
            logger.info(f"ğŸ“‚ ìºì‹œ ë¡œë“œ: {len(_cache)}ê°œ í•­ëª©")
        except Exception as e:
            logger.warning(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            _cache = {}

def _save_cache():
    """ìºì‹œ íŒŒì¼ ì €ì¥"""
    try:
        with open(_cache_file, 'wb') as f:
            pickle.dump(_cache, f)
        logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥: {len(_cache)}ê°œ í•­ëª©")
    except Exception as e:
        logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

def compute_embeddings_with_cache(
    texts: List[str],
    model_name: Optional[str] = None,
    cache_enabled: bool = True
) -> np.ndarray:
    """ìºì‹œë¥¼ í™œìš©í•œ ì„ë² ë”© ê³„ì‚°"""
    
    if not texts:
        return np.array([])
    
    # ìºì‹œ í‚¤ ìƒì„±
    cache_key = _get_cache_key(texts) if cache_enabled else None
    
    # ìºì‹œ í™•ì¸
    if cache_enabled:
        _load_cache()
        if cache_key in _cache:
            logger.info(f"ğŸ¯ ìºì‹œ íˆíŠ¸: {len(texts)}ê°œ í…ìŠ¤íŠ¸")
            return _cache[cache_key]
    
    # ì„ë² ë”© ê³„ì‚°
    model = get_model()
    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)
    
    logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(texts)}ê°œ â†’ {embeddings.shape}")
    
    # ìºì‹œ ì €ì¥
    if cache_enabled and cache_key:
        _cache[cache_key] = embeddings
        _save_cache()
    
    return embeddings

def compute_similarity(text1: str, text2: str) -> float:
    """ë‘ í…ìŠ¤íŠ¸ ê°„ ìœ ì‚¬ë„ ê³„ì‚°"""
    embeddings = compute_embeddings_with_cache([text1, text2])
    
    if len(embeddings) != 2:
        return 0.0
        
    emb1, emb2 = embeddings[0], embeddings[1]
    similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-8)
    
    return float(similarity)

def clear_cache():
    """ìºì‹œ ì´ˆê¸°í™”"""
    global _cache
    _cache = {}
    if os.path.exists(_cache_file):
        os.remove(_cache_file)
    logger.info("ğŸ—‘ï¸ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")