"""í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• ëª¨ë“ˆ - ë§¤ê°œë³€ìˆ˜ ì¶”ê°€"""

import jieba
import MeCab
import logging
import re
from typing import List, Optional, Callable
import numpy as np

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# MeCab ì´ˆê¸°í™”
try:
    mecab = MeCab.Tagger()
    logger.info("MeCab ì´ˆê¸°í™” ì„±ê³µ")
except Exception as e:
    logger.error(f"MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    mecab = None

def split_src_meaning_units(
    text: str, 
    min_tokens: int = 1, 
    max_tokens: int = 10,
    use_advanced: bool = True
) -> List[str]:
    """
    ì›ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 
    
    Args:
        text: ì›ë¬¸ í…ìŠ¤íŠ¸
        min_tokens: ìµœì†Œ í† í° ìˆ˜
        max_tokens: ìµœëŒ€ í† í° ìˆ˜  
        use_advanced: ê³ ê¸‰ ë¶„í•  ê¸°ë²• ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        List[str]: ë¶„í• ëœ ì˜ë¯¸ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸
    """
    if not text or not text.strip():
        return []
    
    try:
        logger.debug(f"ì›ë¬¸ ë¶„í•  ì‹œì‘: {text[:50]}...")
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ë¬¸ë²• êµ¬ì¡° ë¶„í• 
        units = basic_src_split(text)
        
        # 2ë‹¨ê³„: ê³ ê¸‰ ë¶„í•  (ì˜µì…˜)
        if use_advanced:
            units = advanced_src_split(units)
        
        # 3ë‹¨ê³„: ê¸¸ì´ ì œí•œ ì ìš©
        units = apply_length_constraints(units, min_tokens, max_tokens, is_src=True)
        
        logger.debug(f"ì›ë¬¸ ë¶„í•  ì™„ë£Œ: {len(units)}ê°œ ë‹¨ìœ„")
        return units
        
    except Exception as e:
        logger.error(f"ì›ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [text]  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

def split_tgt_meaning_units(
    src_text: str,
    tgt_text: str,
    embed_func: Optional[Callable] = None,
    use_semantic: bool = True,
    min_tokens: int = 1,
    max_tokens: int = 10,
    similarity_threshold: float = 0.3
) -> List[str]:
    """
    ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 
    
    Args:
        src_text: ì›ë¬¸
        tgt_text: ë²ˆì—­ë¬¸
        embed_func: ì„ë² ë”© í•¨ìˆ˜
        use_semantic: ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ì‚¬ìš© ì—¬ë¶€
        min_tokens: ìµœì†Œ í† í° ìˆ˜
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
    
    Returns:
        List[str]: ë¶„í• ëœ ì˜ë¯¸ ë‹¨ìœ„ ë¦¬ìŠ¤íŠ¸
    """
    if not tgt_text or not tgt_text.strip():
        return []
    
    try:
        logger.debug(f"ë²ˆì—­ë¬¸ ë¶„í•  ì‹œì‘: {tgt_text[:50]}...")
        
        if use_semantic and embed_func is not None:
            # ì˜ë¯¸ ê¸°ë°˜ ë¶„í• 
            units = semantic_tgt_split(
                src_text, tgt_text, embed_func, 
                similarity_threshold, min_tokens, max_tokens
            )
        else:
            # ë‹¨ìˆœ ë¶„í• 
            units = simple_tgt_split(tgt_text, min_tokens, max_tokens)
        
        logger.debug(f"ë²ˆì—­ë¬¸ ë¶„í•  ì™„ë£Œ: {len(units)}ê°œ ë‹¨ìœ„")
        return units
        
    except Exception as e:
        logger.error(f"ë²ˆì—­ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [tgt_text]  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

def basic_src_split(text: str) -> List[str]:
    """ê¸°ë³¸ ì›ë¬¸ ë¶„í• """
    
    # 1. ëª…í™•í•œ êµ¬ë¶„ìë¡œ ë¶„í• 
    delimiters = [
        'ç„¶å¾Œì—',  # ì‹œê°„ ì ‘ì†
        'ç„¶å¾Œ',
        'ì´ìš”',    # ë³‘ë ¬ ì ‘ì†
        'ì´ë©°',
        'ì´ê³ ',
        'ë¼ê°€',    # ì „í™˜
        'ë¼ì„œ',
        'ë©´',      # ì¡°ê±´
        'ì´ë©´',
        'í•˜ë©´',
        'å‰‡',      # í•œë¬¸ ì ‘ì†ì‚¬
        'è€Œ',
        'ä¸”',
        'ë˜',
        'ê·¸ë¦¬ê³ ',
        'í•˜ì§€ë§Œ',
        'ê·¸ëŸ¬ë‚˜'
    ]
    
    units = [text]
    
    for delimiter in delimiters:
        new_units = []
        for unit in units:
            parts = re.split(f'({re.escape(delimiter)})', unit)
            current = ""
            
            for part in parts:
                if part == delimiter:
                    if current:
                        new_units.append(current + part)
                        current = ""
                else:
                    current += part
            
            if current:
                new_units.append(current)
        
        units = [u.strip() for u in new_units if u.strip()]
    
    return units

def advanced_src_split(units: List[str]) -> List[str]:
    """ê³ ê¸‰ ì›ë¬¸ ë¶„í• """
    
    advanced_units = []
    
    for unit in units:
        # ë„ˆë¬´ ê¸´ ë‹¨ìœ„ëŠ” ì¶”ê°€ ë¶„í• 
        if len(unit) > 30:
            # í•œìì–´ + ì¡°ì‚¬ íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
            pattern = r'([\u4e00-\u9fff]+[\uac00-\ud7af]*)'
            parts = re.findall(pattern, unit)
            
            if len(parts) > 1:
                # íŒ¨í„´ ê¸°ë°˜ ë¶„í•  ì„±ê³µ
                start = 0
                for part in parts:
                    pos = unit.find(part, start)
                    if pos > start:
                        # íŒ¨í„´ ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ë„ í¬í•¨
                        advanced_units.append(unit[start:pos + len(part)])
                    else:
                        advanced_units.append(part)
                    start = pos + len(part)
                
                if start < len(unit):
                    advanced_units.append(unit[start:])
            else:
                # íŒ¨í„´ ë¶„í•  ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
                advanced_units.append(unit)
        else:
            advanced_units.append(unit)
    
    return [u.strip() for u in advanced_units if u.strip()]

def simple_tgt_split(text: str, min_tokens: int = 1, max_tokens: int = 10) -> List[str]:
    """ë‹¨ìˆœ ë²ˆì—­ë¬¸ ë¶„í•  (MeCab ê¸°ë°˜)"""
    
    if mecab is None:
        # MeCab ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„í• 
        return basic_text_split(text, min_tokens, max_tokens)
    
    try:
        # MeCabìœ¼ë¡œ í˜•íƒœì†Œ ë¶„ì„
        result = mecab.parse(text)
        morphemes = []
        
        for line in result.split('\n'):
            if line and line != 'EOS':
                parts = line.split('\t')
                if len(parts) >= 2:
                    surface = parts[0]
                    features = parts[1].split(',')
                    pos = features[0] if features else ''
                    
                    morphemes.append({
                        'surface': surface,
                        'pos': pos,
                        'features': features
                    })
        
        # ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
        units = group_morphemes_by_meaning(morphemes, min_tokens, max_tokens)
        
        return units
        
    except Exception as e:
        logger.error(f"MeCab ë¶„í•  ì‹¤íŒ¨: {e}")
        return basic_text_split(text, min_tokens, max_tokens)

def semantic_tgt_split(
    src_text: str, 
    tgt_text: str, 
    embed_func: Callable,
    similarity_threshold: float = 0.3,
    min_tokens: int = 1,
    max_tokens: int = 10
) -> List[str]:
    """ì˜ë¯¸ ê¸°ë°˜ ë²ˆì—­ë¬¸ ë¶„í• """
    
    try:
        # 1. ì›ë¬¸ ë‹¨ìœ„ ë¨¼ì € ë¶„í• 
        src_units = split_src_meaning_units(src_text, min_tokens, max_tokens)
        
        # 2. ë²ˆì—­ë¬¸ ê¸°ë³¸ ë¶„í• 
        tgt_candidates = simple_tgt_split(tgt_text, min_tokens, max_tokens)
        
        # 3. ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¬ì¡°í•©
        if len(src_units) > 1 and len(tgt_candidates) > 1:
            tgt_units = semantic_regrouping(
                src_units, tgt_candidates, embed_func, similarity_threshold
            )
        else:
            tgt_units = tgt_candidates
        
        return tgt_units
        
    except Exception as e:
        logger.error(f"ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ì‹¤íŒ¨: {e}")
        return simple_tgt_split(tgt_text, min_tokens, max_tokens)

def semantic_regrouping(
    src_units: List[str], 
    tgt_candidates: List[str], 
    embed_func: Callable,
    similarity_threshold: float = 0.3
) -> List[str]:
    """ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì¬ì¡°í•©"""
    
    try:
        # ëª¨ë“  í…ìŠ¤íŠ¸ ì„ë² ë”©
        all_texts = src_units + tgt_candidates
        embeddings = embed_func(all_texts)
        
        if len(embeddings) != len(all_texts):
            logger.warning("ì„ë² ë”© ìˆ˜ ë¶ˆì¼ì¹˜, ê¸°ë³¸ ë¶„í•  ì‚¬ìš©")
            return tgt_candidates
        
        src_embeddings = embeddings[:len(src_units)]
        tgt_embeddings = embeddings[len(src_units):]
        
        # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
        similarity_matrix = calculate_similarity_matrix(src_embeddings, tgt_embeddings)
        
        # ìµœì  ê·¸ë£¹í™” ì°¾ê¸°
        tgt_groups = find_optimal_grouping(
            tgt_candidates, similarity_matrix, similarity_threshold
        )
        
        return tgt_groups
        
    except Exception as e:
        logger.error(f"ì˜ë¯¸ ì¬ì¡°í•© ì‹¤íŒ¨: {e}")
        return tgt_candidates

def group_morphemes_by_meaning(
    morphemes: List[dict], 
    min_tokens: int = 1, 
    max_tokens: int = 10
) -> List[str]:
    """í˜•íƒœì†Œë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”"""
    
    if not morphemes:
        return []
    
    units = []
    current_unit = ""
    current_count = 0
    
    # ì£¼ìš” í’ˆì‚¬ ê¸°ì¤€ ê·¸ë£¹í™”
    boundary_pos = ['SF', 'SP', 'SS', 'VCP', 'VCN', 'EC', 'EF']  # ë¬¸ì¥ ê²½ê³„ í’ˆì‚¬
    
    for morph in morphemes:
        surface = morph['surface']
        pos = morph['pos']
        
        current_unit += surface
        current_count += 1
        
        # ê²½ê³„ ì¡°ê±´ í™•ì¸
        is_boundary = (
            pos in boundary_pos or  # í’ˆì‚¬ ê²½ê³„
            current_count >= max_tokens or  # ìµœëŒ€ ê¸¸ì´
            (current_count >= min_tokens and pos in ['NNG', 'NNP', 'VV', 'VA'])  # ì˜ë¯¸ ì™„ë£Œ
        )
        
        if is_boundary and current_count >= min_tokens:
            units.append(current_unit.strip())
            current_unit = ""
            current_count = 0
    
    # ë§ˆì§€ë§‰ ë‹¨ìœ„ ì²˜ë¦¬
    if current_unit.strip():
        if units and current_count < min_tokens:
            # ë„ˆë¬´ ì§§ìœ¼ë©´ ì´ì „ ë‹¨ìœ„ì™€ í•©ì¹˜ê¸°
            units[-1] += current_unit
        else:
            units.append(current_unit.strip())
    
    return [u for u in units if u]

def basic_text_split(text: str, min_tokens: int = 1, max_tokens: int = 10) -> List[str]:
    """ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„í•  (ë°±ì—…ìš©)"""
    
    # êµ¬ë‘ì  ê¸°ì¤€ ë¶„í• 
    delimiters = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', ',', 'ï¼Œ', ';', 'ï¼š', ':']
    
    units = [text]
    
    for delimiter in delimiters:
        new_units = []
        for unit in units:
            if delimiter in unit:
                parts = unit.split(delimiter)
                for i, part in enumerate(parts):
                    if i < len(parts) - 1:  # ë§ˆì§€ë§‰ì´ ì•„ë‹ˆë©´ êµ¬ë¶„ì í¬í•¨
                        new_units.append(part + delimiter)
                    else:
                        if part.strip():  # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´
                            new_units.append(part)
            else:
                new_units.append(unit)
        units = [u.strip() for u in new_units if u.strip()]
    
    return apply_length_constraints(units, min_tokens, max_tokens, is_src=False)

def apply_length_constraints(
    units: List[str], 
    min_tokens: int, 
    max_tokens: int, 
    is_src: bool = True
) -> List[str]:
    """ê¸¸ì´ ì œí•œ ì ìš©"""
    
    if min_tokens <= 1 and max_tokens >= 50:
        return units  # ì œí•œì´ ëŠìŠ¨í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    constrained_units = []
    
    for unit in units:
        unit_len = len(unit)
        
        if unit_len > max_tokens * 3:  # ëŒ€ëµì ì¸ ê¸€ì ìˆ˜ ê¸°ì¤€
            # ë„ˆë¬´ ê¸´ ë‹¨ìœ„ëŠ” ë¶„í• 
            mid = len(unit) // 2
            # ì ì ˆí•œ ë¶„í• ì  ì°¾ê¸°
            for i in range(mid - 5, mid + 5):
                if i > 0 and i < len(unit) and unit[i] in ' ï¼Œ,ã€':
                    constrained_units.append(unit[:i+1].strip())
                    constrained_units.append(unit[i+1:].strip())
                    break
            else:
                # ì ì ˆí•œ ë¶„í• ì  ì—†ìœ¼ë©´ ì¤‘ê°„ì—ì„œ ë¶„í• 
                constrained_units.append(unit[:mid].strip())
                constrained_units.append(unit[mid:].strip())
        else:
            constrained_units.append(unit)
    
    # ë„ˆë¬´ ì§§ì€ ë‹¨ìœ„ëŠ” í•©ì¹˜ê¸°
    if min_tokens > 1:
        merged_units = []
        temp_unit = ""
        
        for unit in constrained_units:
            if len(temp_unit + unit) < min_tokens * 2:  # ëŒ€ëµì ì¸ ê¸°ì¤€
                temp_unit += unit
            else:
                if temp_unit:
                    merged_units.append(temp_unit.strip())
                temp_unit = unit
        
        if temp_unit:
            merged_units.append(temp_unit.strip())
        
        constrained_units = merged_units
    
    return [u for u in constrained_units if u.strip()]

def calculate_similarity_matrix(embeddings1: List, embeddings2: List) -> np.ndarray:
    """ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"""
    
    try:
        emb1 = np.array(embeddings1)
        emb2 = np.array(embeddings2)
        
        # ì •ê·œí™”
        emb1_norm = emb1 / np.linalg.norm(emb1, axis=1, keepdims=True)
        emb2_norm = emb2 / np.linalg.norm(emb2, axis=1, keepdims=True)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        similarity = np.dot(emb1_norm, emb2_norm.T)
        
        # 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”
        similarity = (similarity + 1) / 2
        
        return similarity
        
    except Exception as e:
        logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        return np.zeros((len(embeddings1), len(embeddings2)))

def find_optimal_grouping(
    candidates: List[str], 
    similarity_matrix: np.ndarray,
    threshold: float = 0.3
) -> List[str]:
    """ìµœì  ê·¸ë£¹í™” ì°¾ê¸°"""
    
    try:
        # ë‹¨ìˆœ ê·¸ë¦¬ë”” ê·¸ë£¹í™”
        groups = []
        used = set()
        
        # ìœ ì‚¬ë„ê°€ ë†’ì€ ìˆœì„œë¡œ ê·¸ë£¹í™”
        for i in range(len(candidates)):
            if i not in used:
                current_group = [candidates[i]]
                used.add(i)
                
                # ìœ ì‚¬í•œ í›„ë³´ë“¤ ì°¾ê¸°
                for j in range(i + 1, len(candidates)):
                    if j not in used:
                        # ê·¸ë£¹ ë‚´ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
                        avg_sim = np.mean([similarity_matrix[k % similarity_matrix.shape[0], j % similarity_matrix.shape[1]] 
                                         for k in range(len(current_group))])
                        
                        if avg_sim >= threshold:
                            current_group.append(candidates[j])
                            used.add(j)
                
                # ê·¸ë£¹ì„ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ í•©ì¹˜ê¸°
                if len(current_group) > 1:
                    groups.append(''.join(current_group))
                else:
                    groups.append(current_group[0])
        
        return groups
        
    except Exception as e:
        logger.error(f"ìµœì  ê·¸ë£¹í™” ì‹¤íŒ¨: {e}")
        return candidates

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.DEBUG)
    
    print("ğŸ§ª í† í¬ë‚˜ì´ì € ë§¤ê°œë³€ìˆ˜ í…ŒìŠ¤íŠ¸")
    
    test_src = "èˆˆè€…ëŠ” å–©è¡†æ°‘ä¹‹ä¸å¾è¥„å…¬æ”¿ä»¤è€…ëŠ” å¾—å‘¨ç¦®ä»¥æ•ä¹‹ë©´ å‰‡æœì´ë¼"
    test_tgt = "èˆˆí•œ ê²ƒì€ è¥„å…¬ì˜ æ”¿ä»¤ì„ ë”°ë¥´ì§€ ì•ŠëŠ” ë°±ì„±ë“¤ì€ <êµ°ì£¼ê°€> å‘¨ç¦®ë¥¼ ë”°ë¼ êµí™”ì‹œí‚¤ë©´ ë³µì¢…í•œë‹¤ëŠ” ê²ƒì„ ë¹„ìœ í•œ ê²ƒì´ë‹¤."
    
    print(f"\nì›ë¬¸: {test_src}")
    print(f"ë²ˆì—­: {test_tgt}")
    
    # ë‹¤ì–‘í•œ ë§¤ê°œë³€ìˆ˜ë¡œ í…ŒìŠ¤íŠ¸
    for min_tok, max_tok in [(1, 5), (2, 8), (1, 15)]:
        print(f"\n--- min_tokens={min_tok}, max_tokens={max_tok} ---")
        
        src_units = split_src_meaning_units(test_src, min_tok, max_tok)
        print(f"ì›ë¬¸ ë¶„í• : {src_units}")
        
        tgt_units = split_tgt_meaning_units(
            test_src, test_tgt, 
            embed_func=None, use_semantic=False,
            min_tokens=min_tok, max_tokens=max_tok
        )
        print(f"ë²ˆì—­ ë¶„í• : {tgt_units}")