"""ì‹¤ì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import time
from io_utils import load_excel, save_excel
from processor import process_file
from tokenizers import split_src_meaning_units, split_tgt_meaning_units

def create_test_data():
    """ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
    print("ğŸ”¬ ì‹¤ì œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 80)
    
    # ì‹¤ì œ ë°ì´í„°ì…‹ì—ì„œ ë‹¤ì–‘í•œ ê¸¸ì´ì™€ íŠ¹ì„±ì˜ ë¬¸ì¥ ì„ ë³„
    test_data = [
        {"id": 14, "ì›ë¬¸": "èˆˆä¹Ÿë¼", "ë²ˆì—­ë¬¸": "èˆˆì´ë‹¤."},
        {"id": 15, "ì›ë¬¸": "è’¹ì€ è–•(ë ´)ì´ìš” è‘­ëŠ” è˜†ä¹Ÿë¼", "ë²ˆì—­ë¬¸": "è’¹ì€ ë¬¼ì–µìƒˆì´ê³  è‘­ëŠ” ê°ˆëŒ€ì´ë‹¤."},
        {"id": 17, "ì›ë¬¸": "ç™½éœ²å‡æˆ¾çˆ²éœœç„¶å¾Œì— æ­²äº‹æˆì´ìš” åœ‹å®¶å¾…ç¦®ç„¶å¾Œèˆˆì´ë¼", 
         "ë²ˆì—­ë¬¸": "ç™½éœ²ê°€ ì–¼ì–´ ì„œë¦¬ê°€ ëœ ë’¤ì—ì•¼ æ­²äº‹ê°€ ì´ë£¨ì–´ì§€ê³  åœ‹å®¶ëŠ” ç¦®ê°€ í–‰í•´ì§„ ë’¤ì—ì•¼ í¥ì„±í•œë‹¤."},
        {"id": 18, "ì›ë¬¸": "ç®‹äº‘ è’¹è‘­åœ¨è¡†è‰ä¹‹ä¸­ì— è’¼è’¼ç„¶å½Šç››ì´ë¼ê°€ è‡³ç™½éœ²å‡æˆ¾çˆ²éœœì´ë©´ å‰‡æˆè€Œé»ƒì´ë¼", 
         "ë²ˆì—­ë¬¸": "ç®‹äº‘ï¼š ê°ˆëŒ€ëŠ” ì—¬ëŸ¬ í’€ ê°€ìš´ë°ì— í‘¸ë¥´ê²Œ ë¬´ì„±í–ˆë‹¤ê°€ ç™½éœ²ê°€ ì–¼ì–´ ì„œë¦¬ê°€ ë˜ë©´ ë‹¤ ìë¼ ëˆ„ë˜ì§„ë‹¤."},
        {"id": 19, "ì›ë¬¸": "èˆˆè€…ëŠ” å–©è¡†æ°‘ä¹‹ä¸å¾è¥„å…¬æ”¿ä»¤è€…ëŠ” å¾—å‘¨ç¦®ä»¥æ•ä¹‹ë©´ å‰‡æœì´ë¼", 
         "ë²ˆì—­ë¬¸": "èˆˆí•œ ê²ƒì€ è¥„å…¬ì˜ æ”¿ä»¤ì„ ë”°ë¥´ì§€ ì•ŠëŠ” ë°±ì„±ë“¤ì€ <êµ°ì£¼ê°€> å‘¨ç¦®ë¥¼ ë”°ë¼ êµí™”ì‹œí‚¤ë©´ ë³µì¢…í•œë‹¤ëŠ” ê²ƒì„ ë¹„ìœ í•œ ê²ƒì´ë‹¤."},
        {"id": 20, "ì›ë¬¸": "è’¹è‘­è’¼è’¼ì´ëŸ¬ë‹ˆ ç™½éœ²çˆ²éœœì´ë¡œë‹¤", "ë²ˆì—­ë¬¸": "ê°ˆëŒ€ ë¬´ì„±í•˜ë”ë‹ˆ ç™½éœ² ì„œë¦¬ê°€ ë˜ì—ˆë„¤"},
        {"id": 21, "ì›ë¬¸": "æ‰€è¬‚ä¼Šäººì´ åœ¨æ°´ä¸€æ–¹ì´ì–¸ë§ˆëŠ”", "ë²ˆì—­ë¬¸": "ì´ë¥¸ë°” ê·¸ ë¶„ì´ ê°•ë¬¼ ì €ìª½ì— ìˆë‹¤ì§€ë§Œ"},
        {"id": 22, "ì›ë¬¸": "è‹¥é€†æµé¡æ´„è€Œå¾€å¾ä¹‹, å‰‡é“éšªé˜»ä¸”é•·é , ä¸å¯å¾—è‡³, è¨€é€†ç¦®ä»¥æ²»åœ‹, å‰‡ç„¡å¾—äººé“, çµ‚ä¸å¯è‡³. è‹¥é †æµé¡æ¸¸è€Œå¾€å¾ä¹‹, å‰‡å®›ç„¶åœ¨æ–¼æ°´ä¹‹ä¸­å¤®, è¨€é †ç¦®æ²»åœ‹, å‰‡å¾—äººä¹‹é“, è‡ªä¾†è¿å·±, æ­£è¿‘åœ¨ç¦®æ¨‚ä¹‹å…§.", 
         "ë²ˆì—­ë¬¸": "ë§Œì¼ ë¬¼ì‚´ì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ì„œ ë”°ë¥¸ë‹¤ë©´ ê¸¸ì´ í—˜í•˜ê³  ë§‰íˆë©° ë˜í•œ ë©€ì–´ì„œ ì´ë¥¼ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì€ ç¦®ì— ê±°ìŠ¬ëŸ¬ ë‚˜ë¼ë¥¼ ë‹¤ìŠ¤ë¦¬ë©´ ì‚¬ëŒì˜ ë„ë¦¬ë¥¼ ì–»ì§€ ëª»í•˜ì—¬ ëë‚´ ì´ë¥¼ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì„ ë§í•œ ê²ƒì´ë‹¤. ë§Œì¼ ë¬¼ì‚´ì„ ë”°ë¼ ë‚´ë ¤ê°€ì„œ ë”°ë¥¸ë‹¤ë©´ ì™„ì—°íˆ ë¬¼ ê°€ìš´ë°ì— ìˆë‹¤ëŠ” ê²ƒì€ ç¦®ë¥¼ ë”°ë¼ ë‚˜ë¼ë¥¼ ë‹¤ìŠ¤ë¦¬ë©´ ì‚¬ëŒì˜ ë„ë¦¬ë¥¼ ì–»ì–´ì„œ ì €ì ˆë¡œ ì™€ì„œ ìê¸°ë¥¼ ë§ì´í•˜ë‹ˆ ë°”ë¡œ ç¦®æ¨‚ ì•ˆì— ê°€ê¹Œì´ ìˆë‹¤ëŠ” ê²ƒì„ ë§í•œ ê²ƒì´ë‹¤."},
        {"id": 23, "ì›ë¬¸": "ç„¶å‰‡éç¦®, å¿…ä¸å¾—äºº, å¾—äºº, å¿…èƒ½å›ºåœ‹, å›ä½•ä»¥ä¸æ±‚ç”¨å‘¨ç¦®ä¹.", 
         "ë²ˆì—­ë¬¸": "ê·¸ëŸ¬ë‹ˆ ç¦®ê°€ ì•„ë‹ˆë©´ ë°˜ë“œì‹œ ì‚¬ëŒì„ ì–»ì§€ ëª»í•˜ê³ , ì‚¬ëŒì„ ì–»ìœ¼ë©´ ë°˜ë“œì‹œ ë‚˜ë¼ë¥¼ ê²¬ê³ íˆ í•  ìˆ˜ ìˆìœ¼ë‹ˆ, ì„ê¸ˆê»˜ì„œëŠ” ì–´ì°Œí•˜ì—¬ å‘¨ç¦®ì˜ ì‚¬ìš©ì„ êµ¬í•˜ì§€ ì•Šìœ¼ì‹œëŠ”ê°€."},
        {"id": 24, "ì›ë¬¸": "æ­£ç¾©æ›°ï¼š 'è’¹, è–•', 'è‘­, è˜†', é‡‹è‰æ–‡, éƒ­ç’æ›°\"è’¹, ä¼¼è‘è€Œç´°, é«˜æ•¸å°º.", 
         "ë²ˆì—­ë¬¸": "æ­£ç¾©æ›°ï¼š'è’¹, è–•', 'è‘­, è˜†'ëŠ” â‰ªçˆ¾é›…â‰« <é‡‹è‰>ì˜ ê¸€ì´ê³ , éƒ­ç’ì´ ë§í•˜ê¸°ë¥¼ \"è’¹ì€ è‘ê³¼ ë¹„ìŠ·í•˜ë‚˜ ê°€ëŠ˜ê³  ë†’ì´ê°€ ëª‡ ìì´ë‹¤."}
    ]
    
    df = pd.DataFrame(test_data)
    
    # ê¸°ë³¸ í†µê³„ ì¶œë ¥
    print("âœ… ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: real_test_data.xlsx")
    print(f"ğŸ“Š ë¬¸ì¥ ìˆ˜: {len(df)}ê°œ")
    print(f"ğŸ“ ê¸¸ì´ ë¶„í¬:")
    print(f"   ì›ë¬¸ ê¸¸ì´: ìµœì†Œ {df['ì›ë¬¸'].str.len().min()}, ìµœëŒ€ {df['ì›ë¬¸'].str.len().max()}, í‰ê·  {df['ì›ë¬¸'].str.len().mean():.1f}")
    print(f"   ë²ˆì—­ ê¸¸ì´: ìµœì†Œ {df['ë²ˆì—­ë¬¸'].str.len().min()}, ìµœëŒ€ {df['ë²ˆì—­ë¬¸'].str.len().max()}, í‰ê·  {df['ë²ˆì—­ë¬¸'].str.len().mean():.1f}")
    print(f"   í‰ê·  í™•ì¥ ë¹„ìœ¨: {(df['ë²ˆì—­ë¬¸'].str.len().mean() / df['ì›ë¬¸'].str.len().mean()):.2f}")
    
    # ê¸¸ì´ë³„ ë¶„ë¥˜
    short = len(df[df['ì›ë¬¸'].str.len() <= 10])
    medium = len(df[(df['ì›ë¬¸'].str.len() > 10) & (df['ì›ë¬¸'].str.len() <= 50)])
    long = len(df[(df['ì›ë¬¸'].str.len() > 50) & (df['ì›ë¬¸'].str.len() <= 100)])
    very_long = len(df[df['ì›ë¬¸'].str.len() > 100])
    
    print(f"\nğŸ“ ê¸¸ì´ë³„ ë¶„ë¥˜:")
    print(f"   â€¢ ì§§ì€ ë¬¸ì¥ (â‰¤10ì): {short}ê°œ")
    print(f"   â€¢ ì¤‘ê°„ ë¬¸ì¥ (11-50ì): {medium}ê°œ") 
    print(f"   â€¢ ê¸´ ë¬¸ì¥ (51-100ì): {long}ê°œ")
    print(f"   â€¢ ë§¤ìš° ê¸´ ë¬¸ì¥ (>100ì): {very_long}ê°œ")
    
    # íŠ¹ì§•ë³„ ë¶„ë¥˜
    features = {
        "í•œì+ì¡°ì‚¬ í˜¼í•©": len([s for s in df['ì›ë¬¸'] if any(c in s for c in ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼'])]),
        "ì‹œë¬¸/ìš´ìœ¨": len([s for s in df['ì›ë¬¸'] if any(c in s for c in ['ì´ë¡œë‹¤', 'ì´ëŸ¬ë‹ˆ', 'ì´ì–¸ë§ˆëŠ”'])]),
        "ì„¤ëª…ë¬¸": len([s for s in df['ì›ë¬¸'] if 'äº‘' in s or 'æ›°' in s]),
        "ì¸ìš©ë¬¸": len([s for s in df['ë²ˆì—­ë¬¸'] if '<' in s or '>' in s]),
        "ì „ë¬¸ìš©ì–´": len([s for s in df['ì›ë¬¸'] if 'é‡‹è‰' in s or 'æ­£ç¾©' in s]),
        "ì˜ë¬¸ë¬¸": len([s for s in df['ë²ˆì—­ë¬¸'] if '?' in s or 'ê°€.' in s]),
        "ë³µí•©ë¬¸": len([s for s in df['ì›ë¬¸'] if len(s.split()) >= 3])
    }
    
    print(f"\nğŸ¯ íŠ¹ì§•ë³„ ë¶„ë¥˜:")
    for feature, count in features.items():
        print(f"   â€¢ {feature}: {count}ê°œ")
    
    # ë¯¸ë¦¬ë³´ê¸°
    print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    for i, row in df.head(3).iterrows():
        print(f"{i+1}. [ID {row['id']}] ì›ë¬¸: {row['ì›ë¬¸']}")
        print(f"   ë²ˆì—­: {row['ë²ˆì—­ë¬¸']}")
        print()
    
    # íŒŒì¼ ì €ì¥
    save_excel(df, "real_test_data.xlsx")
    print("âœ… ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
    
    return "real_test_data.xlsx"

def test_individual_tokenizer():
    """ê°œë³„ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ê°œë³„ í† í¬ë‚˜ì´ì € ë‹¨ìœ„ í…ŒìŠ¤íŠ¸")
    print("-" * 60)
    
    test_cases = [
        ("èˆˆä¹Ÿë¼", "èˆˆì´ë‹¤."),
        ("è’¹ì€ è–•(ë ´)ì´ìš” è‘­ëŠ” è˜†ä¹Ÿë¼", "è’¹ì€ ë¬¼ì–µìƒˆì´ê³  è‘­ëŠ” ê°ˆëŒ€ì´ë‹¤."),
        ("ç™½éœ²å‡æˆ¾çˆ²éœœç„¶å¾Œì— æ­²äº‹æˆì´ìš”", "ç™½éœ²ê°€ ì–¼ì–´ ì„œë¦¬ê°€ ëœ ë’¤ì—ì•¼ æ­²äº‹ê°€ ì´ë£¨ì–´ì§€ê³ ")
    ]
    
    for i, (src, tgt) in enumerate(test_cases, 1):
        print(f"\ní…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}:")
        print(f"ì›ë¬¸: {src}")
        print(f"ë²ˆì—­: {tgt}")
        
        try:
            src_units = split_src_meaning_units(src)
            tgt_units = split_tgt_meaning_units(src, tgt, use_semantic=False)
            
            print(f"âœ… ì›ë¬¸ ë¶„í• : {src_units}")
            print(f"âœ… ë²ˆì—­ ë¶„í• : {tgt_units}")
        except Exception as e:
            print(f"âŒ ë¶„í•  ì‹¤íŒ¨: {e}")

def test_sentence_tokenization(file_path):
    """ë¬¸ì¥ë³„ í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”¤ ê°œë³„ ë¬¸ì¥ í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸")
    print("-" * 60)
    
    try:
        df = load_excel(file_path)
        print(f"ğŸ“‹ ë°ì´í„° ì»¬ëŸ¼: {list(df.columns)}")
        print(f"ğŸ“Š ë°ì´í„° í–‰ ìˆ˜: {len(df)}")
        
        for idx, row in df.head(5).iterrows():
            src = row['ì›ë¬¸']
            tgt = row['ë²ˆì—­ë¬¸']
            
            print(f"\nğŸ“ ë¬¸ì¥ {idx+1} [ID {row['id']}]:")
            print(f"ì›ë¬¸: {src}")
            print(f"ë²ˆì—­: {tgt}")
            
            try:
                src_units = split_src_meaning_units(src)
                tgt_units = split_tgt_meaning_units(src, tgt, use_semantic=False)
                
                print(f"âœ… ì›ë¬¸ ë¶„í• : {src_units}")
                print(f"âœ… ë²ˆì—­ ë¶„í• : {tgt_units}")
                
            except Exception as e:
                print(f"âŒ í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨: {e}")
                
        print("\nâœ… í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")

def preprocess_data(input_file):
    """ë°ì´í„° ì „ì²˜ë¦¬ (ì»¬ëŸ¼ëª… ë³€í™˜)"""
    print("\nğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ (ì»¬ëŸ¼ëª… ë³€í™˜)")
    print("-" * 60)
    
    try:
        df = load_excel(input_file)
        
        # ì»¬ëŸ¼ëª… ë³€í™˜
        if 'ì›ë¬¸' in df.columns and 'ë²ˆì—­ë¬¸' in df.columns:
            df = df.rename(columns={'ì›ë¬¸': 'src', 'ë²ˆì—­ë¬¸': 'tgt'})
            
        processed_file = input_file.replace('.xlsx', '_processed.xlsx')
        save_excel(df, processed_file)
        
        print(f"âœ… ì»¬ëŸ¼ëª… ë³€í™˜ ì™„ë£Œ: {processed_file}")
        print(f"ğŸ“‹ ë³€í™˜ëœ ì»¬ëŸ¼: {list(df.columns)}")
        
        return processed_file
        
    except Exception as e:
        print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return None

def run_full_pipeline(processed_file):
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("\nğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    print("-" * 60)
    
    print("ğŸ“Š íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘... (ì‹¤ì œ ì„ë² ë”© ì‚¬ìš©)")
    
    start_time = time.time()
    
    try:
        results = process_file(
            processed_file,
            use_semantic=True,  # ì˜ë¯¸ ê¸°ë°˜ ë§¤ì¹­ ì‚¬ìš©
            save_results=True
        )
        
        end_time = time.time()
        
        if results is not None:
            print("âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ")
            print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
            return "real_test_results.xlsx"
        else:
            print("âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            return None
            
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
        return None

def analyze_results(results_file):
    """ê²°ê³¼ ë¶„ì„"""
    print("\nğŸ“Š ê²°ê³¼ ë¶„ì„")
    print("-" * 60)
    
    try:
        df = load_excel(results_file)
        
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œëœ ë¬¸ì¥ ìˆ˜: {len(df)}")
        
        print(f"\nğŸ“ ê¸¸ì´ë³„ ì²˜ë¦¬ ê²°ê³¼:")
        for idx, row in df.iterrows():
            src_units = eval(row['src_units']) if isinstance(row['src_units'], str) else row['src_units']
            tgt_units = eval(row['tgt_units']) if isinstance(row['tgt_units'], str) else row['tgt_units']
            alignments = eval(row['alignments']) if isinstance(row['alignments'], str) else row['alignments']
            
            src_len = len(row['src']) if 'src' in row else 0
            
            print(f"ë¬¸ì¥ {idx+1} (ê¸¸ì´ {src_len}ì):")
            print(f"  ì›ë¬¸ ë¶„í• : {len(src_units) if src_units else 0}ê°œ ë‹¨ìœ„")
            print(f"  ë²ˆì—­ ë¶„í• : {len(tgt_units) if tgt_units else 0}ê°œ ë‹¨ìœ„")
            
            if src_units and len(src_units) <= 5:  # ì§§ì€ ë¬¸ì¥ë§Œ ìƒì„¸ ì¶œë ¥
                print(f"  ì›ë¬¸ ë‹¨ìœ„: {src_units}")
                print(f"  ë²ˆì—­ ë‹¨ìœ„: {tgt_units[:3] + ['...'] if len(tgt_units) > 3 else tgt_units}")
            
            print(f"  ì •ë ¬ ê²°ê³¼: {len(alignments) if alignments else 0}ê°œ")
            print()
        
        # ì „ì²´ í†µê³„
        total_src_units = sum(len(eval(row['src_units']) if isinstance(row['src_units'], str) else row['src_units']) 
                             for _, row in df.iterrows() if row['src_units'])
        total_tgt_units = sum(len(eval(row['tgt_units']) if isinstance(row['tgt_units'], str) else row['tgt_units']) 
                             for _, row in df.iterrows() if row['tgt_units'])
        
        avg_src_len = df['src'].str.len().mean() if 'src' in df.columns else 0
        
        print(f"\nğŸ“ˆ ì „ë°˜ì ì¸ í†µê³„:")
        print(f"  ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ë¬¸ì¥: {len(df[df['src_units'].notna()])}/{len(df)} ({len(df[df['src_units'].notna()])/len(df)*100:.1f}%)")
        print(f"  í‰ê·  ì›ë¬¸ ê¸¸ì´: {avg_src_len:.1f}ì")
        print(f"  í‰ê·  ì›ë¬¸ ë¶„í•  ìˆ˜: {total_src_units/len(df):.1f}ê°œ/ë¬¸ì¥")
        print(f"  í‰ê·  ë²ˆì—­ ë¶„í•  ìˆ˜: {total_tgt_units/len(df):.1f}ê°œ/ë¬¸ì¥")
        if total_src_units > 0:
            print(f"  ë¶„í•  ë¹„ìœ¨ (ë²ˆì—­/ì›ë¬¸): {total_tgt_units/total_src_units:.2f}")
        
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ë¶„ì„ ì‹¤íŒ¨: {e}")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    
    # 1. ê°œë³„ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    test_individual_tokenizer()
    
    # 2. ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    test_file = create_test_data()
    
    # 3. ê°œë³„ ë¬¸ì¥ í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸
    test_sentence_tokenization(test_file)
    
    # 4. ë°ì´í„° ì „ì²˜ë¦¬
    processed_file = preprocess_data(test_file)
    if not processed_file:
        return
    
    # 5. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    results_file = run_full_pipeline(processed_file)
    if not results_file:
        return
    
    # 6. ê²°ê³¼ ë¶„ì„
    analyze_results(results_file)
    
    print("\n" + "=" * 80)
    print("ğŸ ì‹¤ì œ ë°ì´í„° í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("=" * 80)
    print("ğŸ‰ ì‹¤ì œ ë°ì´í„° í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {results_file}")
    print("ğŸ” ìƒì„¸ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    
    print(f"\nğŸ—‚ï¸ ìƒì„±ëœ íŒŒì¼ë“¤:")
    print(f"  â€¢ {test_file} - ì›ë³¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°")
    print(f"  â€¢ {processed_file} - ì „ì²˜ë¦¬ëœ ë°ì´í„°")
    print(f"  â€¢ {results_file} - ìµœì¢… ê²°ê³¼")

if __name__ == "__main__":
    main()