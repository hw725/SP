"""í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• ëª¨ë“ˆ - regex ëª¨ë“ˆ ì‚¬ìš©"""

import jieba
import MeCab
import logging
import re
import regex  # ğŸ†• ìœ ë‹ˆì½”ë“œ ì†ì„± ì •ê·œì‹ ì§€ì›
from typing import List, Optional, Callable
import numpy as np

logger = logging.getLogger(__name__)

# MeCab ì´ˆê¸°í™”
try:
    mecab = MeCab.Tagger()
    logger.info("âœ… MeCab ì´ˆê¸°í™” ì„±ê³µ")
except Exception as e:
    logger.warning(f"âš ï¸ MeCab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    mecab = None

def split_src_meaning_units(
    text: str, 
    min_tokens: int = 1, 
    max_tokens: int = 10,
    use_advanced: bool = True
) -> List[str]:
    """ì›ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
    
    if not text or not text.strip():
        return []
    
    try:
        # ğŸ¯ í•œë¬¸ êµ¬ë¬¸ ê²½ê³„ íŒ¨í„´
        patterns = [
            r'ç„¶å¾Œì—',
            r'ì´ìš”(?=\s|\p{Han}|\p{Hangul}|$)',  # ë’¤ì— ê³µë°±, í•œì, í•œê¸€, ë
            r'ì´ë¼ê°€',
            r'ì´ë©´',
            r'í•˜ë©´',
            r'å‰‡(?=\s|\p{Han}|\p{Hangul})',      # í•œë¬¸ ì ‘ì†ì‚¬
            r'è€Œ(?=\s|\p{Han}|\p{Hangul})',
            r'ä¸”(?=\s|\p{Han}|\p{Hangul})'
        ]
        
        units = [text]
        
        for pattern in patterns:
            new_units = []
            for unit in units:
                if regex.search(pattern, unit):
                    parts = regex.split(f'({pattern})', unit)
                    current = ""
                    for part in parts:
                        if regex.match(pattern, part):
                            if current:
                                new_units.append(current + part)
                                current = ""
                        else:
                            current += part
                    if current:
                        new_units.append(current)
                else:
                    new_units.append(unit)
            units = [u.strip() for u in new_units if u.strip()]
        
        # ğŸ†• ê³ ê¸‰ ë¶„í• : í•œìì–´ + ì¡°ì‚¬ ë‹¨ìœ„
        if use_advanced:
            units = _advanced_han_split(units)
        
        return units
        
    except Exception as e:
        logger.error(f"âŒ ì›ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [text]

def _advanced_han_split(units: List[str]) -> List[str]:
    """í•œìì–´ + ì¡°ì‚¬ ë‹¨ìœ„ë¡œ ê³ ê¸‰ ë¶„í• """
    
    advanced_units = []
    
    for unit in units:
        if len(unit) > 15:  # ê¸´ ë‹¨ìœ„ë§Œ ì¶”ê°€ ë¶„í• 
            # ğŸ†• í•œìì–´ + í•œê¸€ ì¡°ì‚¬/ì–´ë¯¸ íŒ¨í„´
            pattern = r'(\p{Han}+\p{Hangul}*(?:ì´ë¼|ì´ìš”|ì—ì„œ|ë¼ì„œ|í•˜ì—¬|ë©´ì„œ|ì—|ëŠ”|ì€|ì´|ê°€)?)'
            matches = regex.findall(pattern, unit)
            
            if len(matches) > 1:
                remaining = unit
                for match in matches:
                    if match in remaining:
                        pos = remaining.find(match)
                        if pos > 0:
                            advanced_units.append(remaining[:pos].strip())
                        advanced_units.append(match)
                        remaining = remaining[pos + len(match):]
                
                if remaining.strip():
                    advanced_units.append(remaining.strip())
            else:
                advanced_units.append(unit)
        else:
            advanced_units.append(unit)
    
    return [u for u in advanced_units if u.strip()]

def split_tgt_meaning_units(
    src_text: str,
    tgt_text: str,
    embed_func: Optional[Callable] = None,
    use_semantic: bool = True,
    min_tokens: int = 1,
    max_tokens: int = 10,
    similarity_threshold: float = 0.3
) -> List[str]:
    """ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
    
    if not tgt_text or not tgt_text.strip():
        return []
    
    try:
        if mecab:
            return _mecab_split_with_han_awareness(tgt_text, max_tokens)
        else:
            return _basic_split_with_regex(tgt_text, max_tokens)
        
    except Exception as e:
        logger.error(f"âŒ ë²ˆì—­ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return [tgt_text]

def _mecab_split_with_han_awareness(text: str, max_tokens: int) -> List[str]:
    """MeCab ë¶„í•  + í•œì ì¸ì‹"""
    
    result = mecab.parse(text)
    morphemes = []
    
    for line in result.split('\n'):
        if line and line != 'EOS':
            parts = line.split('\t')
            if len(parts) >= 2:
                surface = parts[0]
                pos = parts[1].split(',')[0]
                morphemes.append((surface, pos))
    
    # ì˜ë¯¸ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
    units = []
    current_unit = ""
    
    for i, (surface, pos) in enumerate(morphemes):
        current_unit += surface
        
        # ğŸ†• í•œì + ì¡°ì‚¬ íŠ¹ë³„ ì²˜ë¦¬
        is_han_particle_boundary = False
        if _is_han(surface) and i + 1 < len(morphemes):
            next_surface, next_pos = morphemes[i + 1]
            if _is_particle(next_surface):
                # ë‹¤ìŒì´ ì¡°ì‚¬ë©´ í•¨ê»˜ ë¬¶ê¸°
                current_unit += next_surface
                morphemes[i + 1] = ('', '')  # ë‹¤ìŒ í† í° ìŠ¤í‚µ í‘œì‹œ
                is_han_particle_boundary = True
        
        # ê²½ê³„ ì¡°ê±´
        is_boundary = (
            pos in ['JKS', 'JKO', 'JKC', 'JX', 'SF', 'SP'] or  # ì¡°ì‚¬, êµ¬ë‘ì 
            (pos.startswith('V') and surface in ['ë‹¤', 'ê³ ', 'ë©°']) or  # ì–´ë¯¸
            is_han_particle_boundary or
            len(current_unit) >= max_tokens * 2
        )
        
        if is_boundary and current_unit.strip():
            units.append(current_unit.strip())
            current_unit = ""
    
    if current_unit.strip():
        units.append(current_unit.strip())
    
    return [u for u in units if u]

def _basic_split_with_regex(text: str, max_tokens: int) -> List[str]:
    """regex ê¸°ë°˜ ê¸°ë³¸ ë¶„í• """
    
    # ğŸ†• í•œì+í•œê¸€, êµ¬ë‘ì  íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
    patterns = [
        r'([.!?ã€‚ï¼ï¼Ÿ]+)',                    # êµ¬ë‘ì 
        r'(\p{Han}+\p{Hangul}*)',            # í•œì+í•œê¸€ ì¡°í•©
        r'(\p{Hangul}+(?:ë‹¤|ê³ |ë©°|ì§€ë§Œ))',    # í•œê¸€ ì–´ë¯¸
        r'([,ï¼Œ;ï¼š:]+)'                       # ì‰¼í‘œë¥˜
    ]
    
    units = [text]
    
    for pattern in patterns:
        new_units = []
        for unit in units:
            if regex.search(pattern, unit):
                parts = regex.split(pattern, unit)
                current = ""
                for part in parts:
                    if regex.match(pattern, part):
                        current += part
                        if len(current) >= max_tokens or part in ['.', '!', '?', 'ã€‚']:
                            new_units.append(current.strip())
                            current = ""
                    else:
                        current += part
                if current.strip():
                    new_units.append(current.strip())
            else:
                new_units.append(unit)
        units = [u.strip() for u in new_units if u.strip()]
    
    return units

def _is_han(token: str) -> bool:
    """í•œì í¬í•¨ ì—¬ë¶€ - regex ì‚¬ìš©"""
    return bool(regex.search(r'\p{Han}', token))

def _is_hangul(token: str) -> bool:
    """í•œê¸€ í¬í•¨ ì—¬ë¶€ - regex ì‚¬ìš©"""
    return bool(regex.search(r'\p{Hangul}', token))

def _is_particle(token: str) -> bool:
    """ì¡°ì‚¬/ì–´ë¯¸ ì—¬ë¶€"""
    particles = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¡œ', 
                'ì™€', 'ê³¼', 'ì˜', 'ë„', 'ë§Œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë¼', 'ì´ë¼']
    return token in particles or (len(token) <= 2 and _is_hangul(token))

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    logging.basicConfig(level=logging.INFO)
    
    test_cases = [
        "èˆˆä¹Ÿë¼",
        "è’¹ì€ è–•(ë ´)ì´ìš” è‘­ëŠ” è˜†ä¹Ÿë¼", 
        "ç™½éœ²å‡æˆ¾çˆ²éœœç„¶å¾Œì— æ­²äº‹æˆì´ìš”"
    ]
    
    for text in test_cases:
        print(f"\nì›ë¬¸: {text}")
        units = split_src_meaning_units(text)
        print(f"ë¶„í• : {units}")
        
        # í•œì/í•œê¸€ ê²€ì¶œ í…ŒìŠ¤íŠ¸
        print(f"í•œì í¬í•¨: {_is_han(text)}")
        print(f"í•œê¸€ í¬í•¨: {_is_hangul(text)}")