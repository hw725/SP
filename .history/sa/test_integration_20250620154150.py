"""mecab + jieba í†µí•© í…ŒìŠ¤íŠ¸"""

def test_tokenizer_integration():
    """í† í¬ë‚˜ì´ì € í†µí•© í…ŒìŠ¤íŠ¸"""
    from tokenizer import split_src_meaning_units, split_tgt_meaning_units
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    src_text = "å­æ›°å­¸è€Œæ™‚ç¿’ä¹‹ä¸äº¦èªªä¹"
    tgt_text = "ê³µìê»˜ì„œ ë§ì”€í•˜ì…¨ë‹¤ ë°°ìš°ê³  ë•Œë•Œë¡œ ìµíˆë©´ ë˜í•œ ê¸°ì˜ì§€ ì•„ë‹ˆí•œê°€"
    
    print("ğŸ§ª í† í¬ë‚˜ì´ì € í†µí•© í…ŒìŠ¤íŠ¸")
    print(f"ì›ë¬¸: {src_text}")
    print(f"ë²ˆì—­: {tgt_text}")
    
    # ì›ë¬¸ ë¶„í•  í…ŒìŠ¤íŠ¸
    try:
        src_units = split_src_meaning_units(src_text)
        print(f"âœ… ì›ë¬¸ ë¶„í•  ì„±ê³µ: {src_units}")
    except Exception as e:
        print(f"âŒ ì›ë¬¸ ë¶„í•  ì‹¤íŒ¨: {e}")
        return False
    
    # ë²ˆì—­ë¬¸ ë¶„í•  í…ŒìŠ¤íŠ¸ (ë”ë¯¸ ì„ë² ë”© ì‚¬ìš©)
    try:
        def dummy_embed_func(texts):
            import numpy as np
            return [np.random.randn(100) for _ in texts]
        
        tgt_units = split_tgt_meaning_units(
            src_text, tgt_text, 
            embed_func=dummy_embed_func,
            use_semantic=False  # ë‹¨ìˆœ ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸
        )
        print(f"âœ… ë²ˆì—­ ë¶„í•  ì„±ê³µ: {tgt_units}")
    except Exception as e:
        print(f"âŒ ë²ˆì—­ ë¶„í•  ì‹¤íŒ¨: {e}")
        return False
    
    return True

def test_mecab_functionality():
    """MeCab ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    from tokenizer import calculate_mecab_completeness
    
    test_texts = [
        "ê³µìê»˜ì„œ",
        "ë§ì”€í•˜ì…¨ë‹¤", 
        "ë°°ìš°ê³ ",
        "ë•Œë•Œë¡œ ìµíˆë©´"
    ]
    
    print("\nğŸ§ª MeCab ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    for text in test_texts:
        try:
            score = calculate_mecab_completeness(text)
            print(f"âœ… '{text}' â†’ ì™„ì „ì„± ì ìˆ˜: {score:.3f}")
        except Exception as e:
            print(f"âŒ '{text}' â†’ ì˜¤ë¥˜: {e}")
            return False
    
    return True

def test_jieba_functionality():
    """jieba ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
    from tokenizer import get_jieba_boundaries
    
    test_texts = [
        "å­æ›°å­¸è€Œæ™‚ç¿’ä¹‹",
        "ä¸äº¦èªªä¹",
        "å­¸è€Œæ™‚ç¿’"
    ]
    
    print("\nğŸ§ª jieba ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸")
    for text in test_texts:
        try:
            boundaries = get_jieba_boundaries(text)
            print(f"âœ… '{text}' â†’ ê²½ê³„: {boundaries}")
        except Exception as e:
            print(f"âŒ '{text}' â†’ ì˜¤ë¥˜: {e}")
            return False
    
    return True

def main():
    """ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸"""
    print("ğŸ”¬ MeCab + jieba í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    tests = [
        ("í† í¬ë‚˜ì´ì € í†µí•©", test_tokenizer_integration),
        ("MeCab ê¸°ëŠ¥", test_mecab_functionality), 
        ("jieba ê¸°ëŠ¥", test_jieba_functionality),
    ]
    
    results = []
    for name, test_func in tests:
        print(f"\n{'='*50}")
        print(f"í…ŒìŠ¤íŠ¸: {name}")
        print(f"{'='*50}")
        
        try:
            result = test_func()
            results.append((name, result))
            print(f"âœ… {name}: {'í†µê³¼' if result else 'ì‹¤íŒ¨'}")
        except Exception as e:
            results.append((name, False))
            print(f"âŒ {name}: ì˜ˆì™¸ ë°œìƒ - {e}")
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*50}")
    print("ğŸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*50}")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨" 
        print(f"{name}: {status}")
    
    print(f"\nì´ {total}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ {passed}ê°œ í†µê³¼ ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return True
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return False

if __name__ == "__main__":
    main()