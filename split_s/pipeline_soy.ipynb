{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 원문-번역문 의미 기반 구 단위 매칭 파이프라인\n",
        "\n",
        "이 노트북은 한문 원문과 번역문 간의 구 단위 정렬을 위한 파이프라인을 구현합니다. 다음과 같은 주요 기능을 포함합니다:\n",
        "\n",
        "1. **텍스트 토크나이징**: 원문과 번역문을 의미 단위로 분할\n",
        "2. **임베딩 계산**: 각 의미 단위에 대한 벡터 표현 생성\n",
        "3. **구 단위 정렬**: 동적 프로그래밍 기반 정렬 알고리즘\n",
        "4. **파일 입출력**: Excel 파일 처리\n",
        "\n",
        "이 파이프라인은 원문-번역문 쌍을 입력으로 받아 구 단위로 정렬된 결과를 출력합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 필요한 패키지 설치 및 임포트\n",
        "\n",
        "아래 셀을 실행하여 필요한 패키지를 설치하고 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary packages (GPU version with CUDA 12.1)\n",
        "%pip install regex pandas numpy tqdm openpyxl soynlp\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install sentence-transformers FlagEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import torch\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Dict, Any, Optional, Callable\n",
        "from tqdm.notebook import tqdm\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "from soynlp.tokenizer import LTokenizer  # Using soynlp tokenizer\n",
        "import sentencepiece as spm\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from transformers import XLMRobertaTokenizerFast\n",
        "\n",
        "\n",
        "# Initialize soynlp tokenizer\n",
        "tokenizer = LTokenizer()\n",
        "\n",
        "# Load BGE model\n",
        "model = BGEM3FlagModel(\n",
        "    'BAAI/bge-m3',\n",
        "    use_fp16=True\n",
        ")\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(\n",
        "    format=\"[%(levelname)s] %(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 임베딩 모듈 (embedder.py)\n",
        "\n",
        "텍스트 구에 대한 임베딩을 계산하고 캐시를 관리하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding module\n",
        "_embedding_cache = {}\n",
        "\n",
        "# compute_embeddings_with_cache 함수 교체\n",
        "def compute_embeddings_with_cache(\n",
        "    texts: List[str],\n",
        "    batch_size: int = 20,\n",
        "    show_batch_progress: bool = False\n",
        ") -> np.ndarray:\n",
        "    \"\"\"향상된 임베딩 캐싱 기능\"\"\"\n",
        "    global _embedding_cache\n",
        "    \n",
        "    result_list: List[Optional[np.ndarray]] = [None] * len(texts)\n",
        "    to_embed: List[str] = []\n",
        "    indices_to_embed: List[int] = []\n",
        "\n",
        "    # 캐시 확인\n",
        "    for i, txt in enumerate(texts):\n",
        "        if txt in _embedding_cache:\n",
        "            result_list[i] = _embedding_cache[txt]\n",
        "        else:\n",
        "            to_embed.append(txt)\n",
        "            indices_to_embed.append(i)\n",
        "\n",
        "    # 새 임베딩 계산 필요시\n",
        "    if to_embed:\n",
        "        embeddings = []\n",
        "        it = range(0, len(to_embed), batch_size)\n",
        "        if show_batch_progress:\n",
        "            it = tqdm(it, desc=\"Embedding batches\", ncols=80)\n",
        "        \n",
        "        # GPU 메모리 정리\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "        for start in it:\n",
        "            batch = to_embed[start:start + batch_size]\n",
        "            try:\n",
        "                output = model.encode(\n",
        "                    batch,\n",
        "                    return_dense=True,\n",
        "                    return_sparse=False,  # 필요한 출력만 계산\n",
        "                    return_colbert_vecs=False  # 필요한 출력만 계산\n",
        "                )\n",
        "                dense = output['dense_vecs']\n",
        "            except RuntimeError as e:\n",
        "                # OOM 오류 시 배치 크기 줄여서 재시도\n",
        "                if \"out of memory\" in str(e) and batch_size > 1:\n",
        "                    reduced_batch = batch_size // 2\n",
        "                    logger.warning(f\"메모리 부족, 배치 크기 축소: {batch_size} -> {reduced_batch}\")\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    \n",
        "                    # 재귀적으로 더 작은 배치로 처리\n",
        "                    return compute_embeddings_with_cache(\n",
        "                        texts, \n",
        "                        batch_size=reduced_batch,\n",
        "                        show_batch_progress=show_batch_progress\n",
        "                    )\n",
        "                else:\n",
        "                    raise e\n",
        "                    \n",
        "            embeddings.extend(dense)\n",
        "\n",
        "        # 캐시 업데이트\n",
        "        for i, (txt, emb) in enumerate(zip(to_embed, embeddings)):\n",
        "            _embedding_cache[txt] = emb\n",
        "            result_list[indices_to_embed[i]] = emb\n",
        "\n",
        "    return np.array(result_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 구두점 처리 모듈 (punctuation.py)\n",
        "\n",
        "괄호 처리 및 마스킹 기능을 제공하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Punctuation handling module\n",
        "MASK_TEMPLATE = '[MASK{}]'\n",
        "\n",
        "HALF_WIDTH_BRACKETS = [\n",
        "    ('(', ')'),\n",
        "    ('[', ']'),\n",
        "]\n",
        "FULL_WIDTH_BRACKETS = [\n",
        "    ('（', '）'),\n",
        "    ('［', '］'),\n",
        "]\n",
        "TRANS_BRACKETS = [\n",
        "    ('<', '>'),\n",
        "    ('《', '》'),\n",
        "    ('〈', '〉'),\n",
        "    ('「', '」'),\n",
        "    ('『', '』'),\n",
        "    ('〔', '〕'),\n",
        "    ('【', '】'),\n",
        "    ('〖', '〗'),\n",
        "    ('〘', '〙'),\n",
        "    ('〚', '〛'),\n",
        "]\n",
        "\n",
        "ALL_BRACKETS = HALF_WIDTH_BRACKETS + FULL_WIDTH_BRACKETS + TRANS_BRACKETS\n",
        "\n",
        "def mask_brackets(text: str, text_type: str) -> Tuple[str, List[str]]:\n",
        "    \"\"\"Mask content within brackets according to rules.\"\"\"\n",
        "    assert text_type in {'source', 'target'}, \"text_type must be 'source' or 'target'\"\n",
        "\n",
        "    masks: List[str] = []\n",
        "    mask_id = [0]\n",
        "\n",
        "    def safe_sub(pattern, repl, s):\n",
        "        def safe_replacer(m):\n",
        "            if '[MASK' in m.group(0):\n",
        "                return m.group(0)\n",
        "            return repl(m)\n",
        "        return pattern.sub(safe_replacer, s)\n",
        "\n",
        "    patterns: List[Tuple[re.Pattern, bool]] = []\n",
        "\n",
        "    if text_type == 'source':\n",
        "        for left, right in HALF_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left) + r'[^' + re.escape(left + right) + r']*?' + re.escape(right)), True))\n",
        "        for left, right in FULL_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left)), False))\n",
        "            patterns.append((re.compile(re.escape(right)), False))\n",
        "    elif text_type == 'target':\n",
        "        for left, right in HALF_WIDTH_BRACKETS + FULL_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left) + r'[^' + re.escape(left + right) + r']*?' + re.escape(right)), True))\n",
        "        for left, right in TRANS_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left)), False))\n",
        "            patterns.append((re.compile(re.escape(right)), False))\n",
        "\n",
        "    def mask_content(s: str, pattern: re.Pattern, content_mask: bool) -> str:\n",
        "        def replacer(match: re.Match) -> str:\n",
        "            token = MASK_TEMPLATE.format(mask_id[0])\n",
        "            masks.append(match.group())\n",
        "            mask_id[0] += 1\n",
        "            return token\n",
        "        return safe_sub(pattern, replacer, s)\n",
        "\n",
        "    for pattern, content_mask in patterns:\n",
        "        if content_mask:\n",
        "            text = mask_content(text, pattern, content_mask)\n",
        "    for pattern, content_mask in patterns:\n",
        "        if not content_mask:\n",
        "            text = mask_content(text, pattern, content_mask)\n",
        "\n",
        "    return text, masks\n",
        "\n",
        "def restore_masks(text: str, masks: List[str]) -> str:\n",
        "    \"\"\"Restore masked tokens to their original content.\"\"\"\n",
        "    for i, original in enumerate(masks):\n",
        "        text = text.replace(MASK_TEMPLATE.format(i), original)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 토크나이저 모듈 (tokenizer.py)\n",
        "\n",
        "원문과 번역문을 의미 단위로 분할하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import regex\n",
        "from soynlp.tokenizer import LTokenizer\n",
        "\n",
        "# SoyNLP tokenizer\n",
        "tokenizer = LTokenizer()\n",
        "\n",
        "# 미리 컴파일된 정규식\n",
        "hanja_re    = regex.compile(r'\\p{Han}+')\n",
        "hangul_re   = regex.compile(r'^\\p{Hangul}+$')\n",
        "combined_re = regex.compile(\n",
        "    r'(\\p{Han}+)+(?:\\p{Hangul}+)(?:은|는|이|가|을|를|에|에서|으로|로|와|과|도|만|며|고|하고|의|때)?'\n",
        ")\n",
        "\n",
        "\n",
        "def split_src_meaning_units(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    한문 텍스트를 '한자+조사+어미' 단위로 묶어서 분할\n",
        "    - 줄바꿈 제거, 콜론 뒤 공백 처리\n",
        "    - SoyNLP 기반 한글 형태소 분절\n",
        "    \"\"\"\n",
        "    text = text.replace('\\n', ' ').replace('：', '： ')\n",
        "    tokens = regex.findall(r'\\S+', text)\n",
        "    units: list[str] = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        tok = tokens[i]\n",
        "\n",
        "        # 1) 한자+한글+조사 어미 복합패턴 우선 매칭\n",
        "        m = combined_re.match(tok)\n",
        "        if m:\n",
        "            units.append(m.group(0))\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # 2) 순수 한자 토큰\n",
        "        if hanja_re.search(tok):\n",
        "            unit = tok\n",
        "            j = i + 1\n",
        "            # 뒤따르는 순수 한글 토큰이 있으면 묶기\n",
        "            while j < len(tokens) and hangul_re.match(tokens[j]):\n",
        "                unit += tokens[j]\n",
        "                j += 1\n",
        "            units.append(unit)\n",
        "            i = j\n",
        "            continue\n",
        "\n",
        "        # 3) 순수 한글 토큰: SoyNLP LTokenizer 사용\n",
        "        if hangul_re.match(tok):\n",
        "            korean_tokens = tokenizer.tokenize(tok)\n",
        "            units.extend(korean_tokens)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # 4) 기타 토큰 (숫자, 로마자 등) 그대로 보존\n",
        "        units.append(tok)\n",
        "        i += 1\n",
        "\n",
        "    return units\n",
        "\n",
        "\n",
        "def split_inside_chunk(chunk: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    조사, 어미, 그리고 '：' 기준으로 의미 단위 분할\n",
        "    원형 보존, 공백 삽입 없이 분리\n",
        "    \"\"\"\n",
        "    delimiters = ['을', '를', '이', '가', '은', '는', '에', '에서', '로', '으로',\n",
        "                  '와', '과', '고', '며', '하고', '때', '의', '도', '만', '：']\n",
        "    \n",
        "    # lookbehind 패턴 생성\n",
        "    pattern = '|'.join([f'(?<={re.escape(d)})' for d in delimiters])\n",
        "    try:\n",
        "        parts = re.split(pattern, chunk)\n",
        "        return [p.strip() for p in parts if p.strip()]\n",
        "    except:\n",
        "        return [p for p in chunk.split() if p.strip()]\n",
        "\n",
        "\n",
        "def find_target_span_end_simple(src_unit: str, remaining_tgt: str) -> int:\n",
        "    hanja_chars = regex.findall(r'\\p{Han}+', src_unit)\n",
        "    if not hanja_chars:\n",
        "        return 0\n",
        "    last = hanja_chars[-1]\n",
        "    idx = remaining_tgt.rfind(last)\n",
        "    if idx == -1:\n",
        "        return len(remaining_tgt)\n",
        "    end = idx + len(last)\n",
        "    next_space = remaining_tgt.find(' ', end)\n",
        "    return next_space + 1 if next_space != -1 else len(remaining_tgt)\n",
        "\n",
        "\n",
        "def find_target_span_end_semantic(\n",
        "    src_unit: str,\n",
        "    remaining_tgt: str,\n",
        "    embed_func=compute_embeddings_with_cache,\n",
        "    min_tokens: int = 1,\n",
        "    max_tokens: int = 50,\n",
        "    similarity_threshold: float = 0.4\n",
        ") -> int:\n",
        "    \"\"\"최적화된 타겟 스팬 탐색 함수\"\"\"\n",
        "    # 예외 처리 강화\n",
        "    if not src_unit or not remaining_tgt:\n",
        "        return 0\n",
        "        \n",
        "    try:\n",
        "        # 1) 원문 임베딩 (단일 계산)\n",
        "        src_emb = embed_func([src_unit])[0]\n",
        "        \n",
        "        # 2) 번역문 토큰 분리 및 누적 길이 계산\n",
        "        tgt_tokens = remaining_tgt.split()\n",
        "        if not tgt_tokens:\n",
        "            return 0\n",
        "            \n",
        "        upper = min(len(tgt_tokens), max_tokens)\n",
        "        cumulative_lengths = [0]\n",
        "        current_length = 0\n",
        "        \n",
        "        for tok in tgt_tokens:\n",
        "            current_length += len(tok) + 1  # 토큰 + 공백\n",
        "            cumulative_lengths.append(current_length)\n",
        "            \n",
        "        # 3) 후보 세그먼트 생성 (메모리 효율적 방식)\n",
        "        candidates = []\n",
        "        candidate_indices = []\n",
        "        \n",
        "        # 적은 수의 후보만 생성하여 효율성 향상\n",
        "        step_size = 1 if upper <= 10 else 2  # 토큰 10개 이하면 모든 후보 검사, 이상이면 2칸씩\n",
        "        \n",
        "        for end_i in range(min_tokens-1, upper, step_size):\n",
        "            cand = \" \".join(tgt_tokens[:end_i+1])\n",
        "            candidates.append(cand)\n",
        "            candidate_indices.append(end_i)\n",
        "            \n",
        "        # 4) 배치 임베딩 (한 번에 계산)\n",
        "        cand_embs = embed_func(candidates)\n",
        "        \n",
        "        # 5) 최적 매칭 탐색 (코사인 유사도 + 길이 패널티)\n",
        "        best_score = -1.0\n",
        "        best_end_idx = cumulative_lengths[-1]  # 기본값은 전체 길이\n",
        "        \n",
        "        for i, emb in enumerate(cand_embs):\n",
        "            # 코사인 유사도 계산\n",
        "            score = np.dot(src_emb, emb) / (np.linalg.norm(src_emb) * np.linalg.norm(emb) + 1e-8)\n",
        "            \n",
        "            # 길이 패널티 (너무 짧은 매칭 방지)\n",
        "            end_i = candidate_indices[i]\n",
        "            length_ratio = (end_i + 1) / len(tgt_tokens)\n",
        "            length_penalty = min(1.0, length_ratio * 2)  # 최대 1.0\n",
        "            \n",
        "            adjusted_score = score * length_penalty\n",
        "            \n",
        "            # 임계값 확인 및 최대값 갱신\n",
        "            if adjusted_score > best_score and score >= similarity_threshold:\n",
        "                best_score = adjusted_score\n",
        "                best_end_idx = cumulative_lengths[end_i + 1]\n",
        "                \n",
        "        return best_end_idx\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"의미 매칭 오류, 단순 매칭으로 대체: {e}\")\n",
        "        return find_target_span_end_simple(src_unit, remaining_tgt)\n",
        "\n",
        "\n",
        "def split_tgt_by_src_units(src_units: list[str], tgt_text: str) -> list[str]:\n",
        "    results = []\n",
        "    cursor = 0\n",
        "    total = len(tgt_text)\n",
        "    for src_u in src_units:\n",
        "        remaining = tgt_text[cursor:]\n",
        "        end_len = find_target_span_end_simple(src_u, remaining)\n",
        "        chunk = tgt_text[cursor:cursor+end_len]\n",
        "        results.extend(split_inside_chunk(chunk))\n",
        "        cursor += end_len\n",
        "    if cursor < total:\n",
        "        results.extend(split_inside_chunk(tgt_text[cursor:]))\n",
        "    return results\n",
        "\n",
        "def split_tgt_by_src_units_semantic(src_units, tgt_text, embed_func=compute_embeddings_with_cache, min_tokens=1):\n",
        "    tgt_tokens = tgt_text.split()\n",
        "    N, T = len(src_units), len(tgt_tokens)\n",
        "    if N == 0 or T == 0:\n",
        "        return []\n",
        "\n",
        "    dp = np.full((N+1, T+1), -np.inf)\n",
        "    back = np.zeros((N+1, T+1), dtype=int)\n",
        "    dp[0, 0] = 0.0\n",
        "\n",
        "    # 원문 임베딩 계산\n",
        "    src_embs = embed_func(src_units)\n",
        "\n",
        "    # DP 테이블 채우기 (j 루프 범위 주의!)\n",
        "    for i in range(1, N+1):\n",
        "        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):\n",
        "            for k in range((i-1)*min_tokens, j-min_tokens+1):\n",
        "                span = \" \".join(tgt_tokens[k:j])\n",
        "                tgt_emb = embed_func([span])[0]\n",
        "                sim = float(np.dot(src_embs[i-1], tgt_emb)/((np.linalg.norm(src_embs[i-1])*np.linalg.norm(tgt_emb))+1e-8))\n",
        "                score = dp[i-1, k] + sim\n",
        "                if score > dp[i, j]:\n",
        "                    dp[i, j] = score\n",
        "                    back[i, j] = k\n",
        "\n",
        "    # Traceback\n",
        "    cuts = [T]\n",
        "    curr = T\n",
        "    for i in range(N, 0, -1):\n",
        "        prev = int(back[i, curr])\n",
        "        cuts.append(prev)\n",
        "        curr = prev\n",
        "    cuts = cuts[::-1]\n",
        "    assert cuts[0] == 0 and cuts[-1] == T and len(cuts) == N + 1\n",
        "\n",
        "    # Build actual spans\n",
        "    tgt_spans = []\n",
        "    for i in range(N):\n",
        "        span = \" \".join(tgt_tokens[cuts[i]:cuts[i+1]]).strip()\n",
        "        tgt_spans.append(span)\n",
        "    return tgt_spans\n",
        "\n",
        "def split_tgt_meaning_units(\n",
        "    src_text: str,\n",
        "    tgt_text: str,\n",
        "    use_semantic: bool = True,\n",
        "    min_tokens: int = 1,\n",
        "    max_tokens: int = 50\n",
        ") -> list[str]:\n",
        "    src_units = split_src_meaning_units(src_text)\n",
        "\n",
        "    if use_semantic:\n",
        "        return split_tgt_by_src_units_semantic(\n",
        "            src_units,\n",
        "            tgt_text,\n",
        "            embed_func=compute_embeddings_with_cache,\n",
        "            min_tokens=min_tokens\n",
        "        )\n",
        "    else:\n",
        "        return split_tgt_by_src_units(src_units, tgt_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 정렬 모듈 (aligner.py)\n",
        "\n",
        "원문과 번역문 구 간의 정렬을 위한 알고리즘입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aligner module\n",
        "def cosine_similarity(vec1: Any, vec2: Any) -> float:\n",
        "    \"\"\"Calculate cosine similarity (handling zero vectors).\"\"\"\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(vec1, vec2) / (norm1 * norm2))\n",
        "\n",
        "def align_src_tgt(src_units, tgt_units, embed_func=compute_embeddings_with_cache):\n",
        "    \"\"\"Align source and target units.\"\"\"\n",
        "    logger.info(f\"Source units: {len(src_units)} items, Target units: {len(tgt_units)} items\")\n",
        "\n",
        "    if len(src_units) != len(tgt_units):\n",
        "        try:\n",
        "            flatten_tgt = \" \".join(tgt_units)\n",
        "            new_tgt_units = split_tgt_by_src_units_semantic(src_units, flatten_tgt, embed_func, min_tokens=1)\n",
        "            if len(new_tgt_units) == len(src_units):\n",
        "                logger.info(\"Semantic re-alignment successful\")\n",
        "                return list(zip(src_units, new_tgt_units))\n",
        "            else:\n",
        "                logger.warning(f\"Length mismatch after re-alignment: Source={len(src_units)}, Target={len(new_tgt_units)}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during semantic re-alignment: {e}\")\n",
        "\n",
        "        if len(src_units) > len(tgt_units):\n",
        "            tgt_units.extend([\"\"] * (len(src_units) - len(tgt_units)))\n",
        "        else:\n",
        "            src_units.extend([\"\"] * (len(tgt_units) - len(src_units)))\n",
        "\n",
        "    return list(zip(src_units, tgt_units))\n",
        "\n",
        "def calculate_alignment_matrix(src_embs, tgt_embs, batch_size=512):\n",
        "    \"\"\"Optimized function for calculating large similarity matrices.\"\"\"\n",
        "    src_len, tgt_len = len(src_embs), len(tgt_embs)\n",
        "    similarity_matrix = np.zeros((src_len, tgt_len))\n",
        "\n",
        "    for i in range(0, src_len, batch_size):\n",
        "        batch_src = src_embs[i:i + batch_size]\n",
        "        for j in range(0, tgt_len, batch_size):\n",
        "            batch_tgt = tgt_embs[j:j + batch_size]\n",
        "            batch_src_norm = np.linalg.norm(batch_src, axis=1, keepdims=True)\n",
        "            batch_tgt_norm = np.linalg.norm(batch_tgt, axis=1, keepdims=True)\n",
        "\n",
        "            dots = np.matmul(batch_src, batch_tgt.T)\n",
        "            norms = np.matmul(batch_src_norm, batch_tgt_norm.T)\n",
        "            batch_sim = dots / (norms + 1e-8)\n",
        "\n",
        "            similarity_matrix[i:i + batch_size, j:j + batch_size] = batch_sim\n",
        "\n",
        "    return similarity_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. I/O 모듈 (io_manager.py)\n",
        "\n",
        "Excel 파일을 읽고 처리한 후 결과를 저장하는 모듈입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I/O module\n",
        "def process_file(input_path: str, output_path: str, batch_size: int = 128, verbose: bool = False) -> None:\n",
        "    \"\"\"청크 단위로 최적화된 파일 처리 함수\"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(input_path, engine='openpyxl')\n",
        "    except Exception as e:\n",
        "        logger.error(f\"[IO] Failed to read Excel file: {e}\")\n",
        "        return\n",
        "\n",
        "    if '원문' not in df.columns or '번역문' not in df.columns:\n",
        "        logger.error(\"[IO] Missing '원문' or '번역문' columns.\")\n",
        "        return\n",
        "\n",
        "    outputs: List[Dict[str, Any]] = []\n",
        "    total_rows = len(df)\n",
        "    \n",
        "    # 처리할 행을 작은 청크로 분할\n",
        "    chunk_size = min(50, total_rows)  # 최대 50행씩 처리\n",
        "    \n",
        "    # 임베딩 캐시 초기화 - 메모리 관리\n",
        "    global _embedding_cache\n",
        "    \n",
        "    for chunk_start in tqdm(range(0, total_rows, chunk_size), desc=\"Processing chunks\"):\n",
        "        chunk_end = min(chunk_start + chunk_size, total_rows)\n",
        "        chunk_df = df.iloc[chunk_start:chunk_end]\n",
        "        \n",
        "        # 청크별 임베딩 캐시 관리 (메모리 효율성)\n",
        "        if len(_embedding_cache) > 10000:  # 캐시가 너무 크면\n",
        "            _embedding_cache = {}  # 초기화\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        # 청크 내 행 처리\n",
        "        for idx, row in enumerate(chunk_df.itertuples(index=False), start=chunk_start+1):\n",
        "            src_text = str(getattr(row, '원문', '') or '')\n",
        "            tgt_text = str(getattr(row, '번역문', '') or '')\n",
        "            if verbose:\n",
        "                print(f\"\\n[========= ROW {idx} =========]\")\n",
        "                print(\"Source (input):\", src_text)\n",
        "                print(\"Target (input):\", tgt_text)\n",
        "\n",
        "            try:\n",
        "                masked_src, src_masks = mask_brackets(src_text, text_type=\"source\")\n",
        "                masked_tgt, tgt_masks = mask_brackets(tgt_text, text_type=\"target\")\n",
        "\n",
        "                src_units = split_src_meaning_units(masked_src)\n",
        "                tgt_units = split_tgt_meaning_units(masked_src, masked_tgt, use_semantic=True, min_tokens=1)\n",
        "\n",
        "                restored_src_units = [restore_masks(unit, src_masks) for unit in src_units]\n",
        "                restored_tgt_units = [restore_masks(unit, tgt_masks) for unit in tgt_units]\n",
        "\n",
        "                aligned_pairs = align_src_tgt(restored_src_units, restored_tgt_units, compute_embeddings_with_cache)\n",
        "                aligned_src_units, aligned_tgt_units = zip(*aligned_pairs)\n",
        "\n",
        "                if verbose:\n",
        "                    print(\"Alignment result: (source to target comparison)\")\n",
        "                    for src_gu, tgt_gu in zip(aligned_src_units, aligned_tgt_units):\n",
        "                        print(f\"SRC: {src_gu} | TGT: {tgt_gu}\")\n",
        "\n",
        "                for gu_idx, (src_gu, tgt_gu) in enumerate(zip(aligned_src_units, aligned_tgt_units), start=1):\n",
        "                    outputs.append({\n",
        "                        \"문장식별자\": idx,\n",
        "                        \"구식별자\": gu_idx,\n",
        "                        \"원문구\": src_gu,\n",
        "                        \"번역구\": tgt_gu,\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"[IO] Failed to process row {idx}: {e}\")\n",
        "                outputs.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "\n",
        "    try:\n",
        "        output_df = pd.DataFrame(outputs, columns=[\"문장식별자\", \"구식별자\", \"원문구\", \"번역구\"])\n",
        "        output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "        if verbose:\n",
        "            logger.info(f\"[IO] Results saved successfully: {output_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"[IO] Failed to save results: {e}\")\n",
        "        \n",
        "\n",
        "def process_file_parallel(input_path: str, output_path: str, num_workers: int = None, chunk_size: int = 20, gpu_strategy: str = \"single\") -> None:\n",
        "    \"\"\"\n",
        "    병렬 처리를 사용한 파일 처리 함수 - GPU 최적화 버전\n",
        "    \n",
        "    Args:\n",
        "        input_path: 입력 Excel 파일 경로\n",
        "        output_path: 출력 Excel 파일 경로\n",
        "        num_workers: 병렬 처리에 사용할 워커 수 (None이면 CPU 코어 수에 따라 자동 설정)\n",
        "        chunk_size: 각 워커에게 할당할 행 수\n",
        "        gpu_strategy: GPU 활용 전략 \n",
        "            - \"single\": 첫 번째 워커만 GPU 사용 (안전)\n",
        "            - \"shared\": 모든 워커가 GPU 공유 (고성능 GPU 필요)\n",
        "            - \"multi\": 여러 GPU에 분산 (다중 GPU 환경)\n",
        "            - \"none\": GPU 사용 안 함 (CPU만 사용)\n",
        "    \"\"\"\n",
        "    import concurrent.futures\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "    from tqdm import tqdm\n",
        "    import logging\n",
        "    \n",
        "    # 로거 사용 (기존 코드와 동일한 로거 사용)\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    try:\n",
        "        # 파일 읽기 (기존 코드와 동일한 방식)\n",
        "        try:\n",
        "            df = pd.read_excel(input_path, engine='openpyxl')\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[IO] Failed to read Excel file: {e}\")\n",
        "            return\n",
        "\n",
        "        if '원문' not in df.columns or '번역문' not in df.columns:\n",
        "            logger.error(\"[IO] Missing '원문' or '번역문' columns.\")\n",
        "            return\n",
        "        \n",
        "        # 워커 수 결정\n",
        "        if num_workers is None:\n",
        "            cpu_cores = os.cpu_count() or 4\n",
        "            if gpu_strategy == \"multi\" and torch.cuda.is_available():\n",
        "                # 멀티 GPU 환경에서는 GPU 수에 맞게 조정\n",
        "                gpu_count = torch.cuda.device_count()\n",
        "                num_workers = max(min(cpu_cores, gpu_count * 2 if gpu_count > 0 else 4), 1)\n",
        "            else:\n",
        "                # 일반 환경에서는 CPU 코어 수 기준으로 설정\n",
        "                num_workers = min(cpu_cores, 4)\n",
        "        \n",
        "        # GPU 설정 준비\n",
        "        gpu_count = 0\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_count = torch.cuda.device_count()\n",
        "            logger.info(f\"Available GPUs: {gpu_count}\")\n",
        "            # GPU 메모리 정리\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            logger.info(\"No GPU available, using CPU only\")\n",
        "            if gpu_strategy != \"none\":\n",
        "                logger.warning(\"GPU strategy requested but no GPU available, falling back to CPU\")\n",
        "                gpu_strategy = \"none\"\n",
        "        \n",
        "        # 데이터 분할\n",
        "        total_rows = len(df)\n",
        "        row_indices = list(range(1, total_rows + 1))  # 1부터 시작하는 인덱스\n",
        "        \n",
        "        # 데이터 준비 - 작업량이 균등하도록 분배\n",
        "        data_chunks = []\n",
        "        for i in range(0, total_rows, chunk_size):\n",
        "            chunk_indices = row_indices[i:i + chunk_size]\n",
        "            chunk_data = []\n",
        "            for idx in chunk_indices:\n",
        "                row_idx = idx - 1  # 0-based 인덱스로 변환\n",
        "                if row_idx < len(df):  # 인덱스 범위 체크\n",
        "                    row = df.iloc[row_idx]\n",
        "                    # None 체크 및 문자열 변환\n",
        "                    src_text = str(row['원문']) if pd.notnull(row['원문']) else \"\"\n",
        "                    tgt_text = str(row['번역문']) if pd.notnull(row['번역문']) else \"\"\n",
        "                    \n",
        "                    # 텍스트 길이 기반 가중치 계산\n",
        "                    text_length = len(src_text) + len(tgt_text)\n",
        "                    chunk_data.append((idx, src_text, tgt_text, text_length))\n",
        "            \n",
        "            if chunk_data:  # 빈 청크 방지\n",
        "                data_chunks.append(chunk_data)\n",
        "        \n",
        "        # 청크별 작업량 계산 및 텍스트 길이 정보 제거\n",
        "        processed_chunks = []\n",
        "        for chunk in data_chunks:\n",
        "            # 텍스트 길이 정보는 제거하고 원래 데이터만 유지\n",
        "            processed_chunks.append([(item[0], item[1], item[2]) for item in chunk])\n",
        "        \n",
        "        # 작업량 기준으로 청크 정렬 (내림차순)\n",
        "        chunks_with_size = [(i, sum(len(item[1])+len(item[2]) for item in chunk)) for i, chunk in enumerate(data_chunks)]\n",
        "        chunks_with_size.sort(key=lambda x: x[1], reverse=True)\n",
        "        sorted_chunk_indices = [i for i, _ in chunks_with_size]\n",
        "        \n",
        "        # GPU 할당 전략 설정\n",
        "        if gpu_strategy == \"single\" and gpu_count > 0:\n",
        "            # 첫 번째 워커만 GPU 사용\n",
        "            gpu_assignments = [0 if i == 0 else -1 for i in range(num_workers)]\n",
        "        elif gpu_strategy == \"shared\" and gpu_count > 0:\n",
        "            # 모든 워커가 첫 번째 GPU 공유\n",
        "            gpu_assignments = [0 for _ in range(num_workers)]\n",
        "        elif gpu_strategy == \"multi\" and gpu_count > 1:\n",
        "            # 다중 GPU에 워커 분산\n",
        "            gpu_assignments = [i % gpu_count for i in range(num_workers)]\n",
        "        else:\n",
        "            # GPU 사용 안 함\n",
        "            gpu_assignments = [-1 for _ in range(num_workers)]\n",
        "        \n",
        "        logger.info(f\"Starting parallel processing with {num_workers} workers, {len(processed_chunks)} chunks\")\n",
        "        logger.info(f\"GPU strategy: {gpu_strategy}, GPU assignments: {gpu_assignments}\")\n",
        "        \n",
        "        # 청크와 워커 매핑 최적화 (긴 텍스트 = 강력한 GPU에 할당)\n",
        "        chunk_worker_assignments = {}\n",
        "        for i, chunk_idx in enumerate(sorted_chunk_indices):\n",
        "            worker_idx = i % num_workers\n",
        "            chunk_worker_assignments[chunk_idx] = worker_idx\n",
        "        \n",
        "        # 병렬 실행\n",
        "        all_results = []\n",
        "        with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "            future_to_chunk = {}\n",
        "            \n",
        "            for chunk_idx, chunk_data in enumerate(processed_chunks):\n",
        "                worker_idx = chunk_worker_assignments.get(chunk_idx, chunk_idx % num_workers)\n",
        "                gpu_idx = gpu_assignments[worker_idx]\n",
        "                \n",
        "                # 워커별 메모리 사용량 조정\n",
        "                memory_fraction = 0.8 if gpu_strategy == \"shared\" else 0.95\n",
        "                \n",
        "                future = executor.submit(\n",
        "                    process_chunk_safe,  # 안전한 청크 처리 함수 사용\n",
        "                    chunk_data,\n",
        "                    gpu_idx=gpu_idx,\n",
        "                    memory_fraction=memory_fraction\n",
        "                )\n",
        "                future_to_chunk[future] = chunk_idx\n",
        "            \n",
        "            # 결과 수집\n",
        "            for future in tqdm(concurrent.futures.as_completed(future_to_chunk), \n",
        "                              total=len(future_to_chunk), \n",
        "                              desc=\"Processing chunks\"):\n",
        "                chunk_idx = future_to_chunk[future]\n",
        "                try:\n",
        "                    chunk_results = future.result()\n",
        "                    if isinstance(chunk_results, list):  # 타입 체크 추가\n",
        "                        all_results.extend(chunk_results)\n",
        "                    else:\n",
        "                        logger.error(f\"Invalid result type from chunk {chunk_idx}: {type(chunk_results)}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing chunk {chunk_idx}: {e}\")\n",
        "        \n",
        "        # 결과가 비어있는지 확인\n",
        "        if not all_results:\n",
        "            logger.error(\"No results were produced by parallel processing\")\n",
        "            return\n",
        "            \n",
        "        # 결과 정렬 (문장식별자, 구식별자 순)\n",
        "        all_results.sort(key=lambda x: (x['문장식별자'], x['구식별자']))\n",
        "        \n",
        "        # 결과 저장\n",
        "        try:\n",
        "            output_df = pd.DataFrame(all_results, columns=[\"문장식별자\", \"구식별자\", \"원문구\", \"번역구\"])\n",
        "            output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "            logger.info(f\"[IO] Results saved successfully: {output_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[IO] Failed to save results: {e}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Parallel processing failed: {e}\")\n",
        "        raise\n",
        "    \n",
        "\n",
        "def process_chunk_safe(chunk_data, gpu_idx=-1, memory_fraction=0.95):\n",
        "    \"\"\"\n",
        "    예외 처리 기능이 강화된 안전한 청크 처리 래퍼 함수\n",
        "    \n",
        "    Args:\n",
        "        chunk_data: (인덱스, 원문, 번역문) 튜플의 리스트\n",
        "        gpu_idx: 사용할 GPU 인덱스 (-1: CPU만 사용)\n",
        "        memory_fraction: GPU 메모리 사용 비율 (0.0 ~ 1.0)\n",
        "    \n",
        "    Returns:\n",
        "        처리된 결과 딕셔너리 리스트 또는 오류 시 빈 리스트\n",
        "    \"\"\"\n",
        "    import logging\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    try:\n",
        "        # 실제 처리 함수 호출\n",
        "        return process_chunk(chunk_data, gpu_idx, memory_fraction)\n",
        "    except Exception as e:\n",
        "        # 치명적 오류 발생 시 기본 처리\n",
        "        logger.error(f\"Critical error in process_chunk: {e}\")\n",
        "        \n",
        "        # 오류 발생 시 원본 텍스트만 결과로 반환\n",
        "        fallback_results = []\n",
        "        try:\n",
        "            for idx, src_text, tgt_text in chunk_data:\n",
        "                fallback_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "        except:\n",
        "            logger.error(\"Failed to create fallback results\")\n",
        "        \n",
        "        return fallback_results\n",
        "\n",
        "\n",
        "def process_chunk(chunk_data, gpu_idx=-1, memory_fraction=0.95):\n",
        "    \"\"\"\n",
        "    하나의 데이터 청크를 처리하는 워커 함수 - GPU 최적화 버전\n",
        "    \n",
        "    Args:\n",
        "        chunk_data: (인덱스, 원문, 번역문) 튜플의 리스트\n",
        "        gpu_idx: 사용할 GPU 인덱스 (-1: CPU만 사용)\n",
        "        memory_fraction: GPU 메모리 사용 비율 (0.0 ~ 1.0)\n",
        "    \n",
        "    Returns:\n",
        "        처리된 결과 딕셔너리 리스트\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import torch\n",
        "    import logging\n",
        "    import gc\n",
        "    \n",
        "    # 로거 초기화 (프로세스별)\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    # 사용 가능한지 확인하고 필요한 모듈 임포트\n",
        "    required_modules = {}\n",
        "    \n",
        "    try:\n",
        "        from colbert.modeling.checkpoint import Checkpoint\n",
        "        required_modules['colbert'] = True\n",
        "    except ImportError:\n",
        "        logger.error(\"colbert 모듈을 불러올 수 없습니다.\")\n",
        "        required_modules['colbert'] = False\n",
        "    \n",
        "    # 마스킹 및 분할 함수 사용 가능성 확인\n",
        "    try:\n",
        "        from your_module import mask_brackets, split_src_meaning_units, split_tgt_meaning_units, restore_masks, align_src_tgt, compute_embeddings_with_cache\n",
        "        required_modules['custom_functions'] = True\n",
        "    except ImportError:\n",
        "        # 모듈 이름이 다를 수 있으므로 전역 네임스페이스에서 찾아봄\n",
        "        functions_available = True\n",
        "        for func_name in ['mask_brackets', 'split_src_meaning_units', 'split_tgt_meaning_units', 'restore_masks', 'align_src_tgt', 'compute_embeddings_with_cache']:\n",
        "            if func_name not in globals():\n",
        "                logger.error(f\"필수 함수 {func_name}를 찾을 수 없습니다.\")\n",
        "                functions_available = False\n",
        "        required_modules['custom_functions'] = functions_available\n",
        "    \n",
        "    # 필수 모듈/함수가 없으면 기본 처리만 수행\n",
        "    if not all(required_modules.values()):\n",
        "        logger.error(\"필수 모듈 또는 함수가 누락되었습니다. 원본 텍스트를 그대로 반환합니다.\")\n",
        "        return [{\"문장식별자\": idx, \"구식별자\": 1, \"원문구\": src, \"번역구\": tgt} for idx, src, tgt in chunk_data]\n",
        "    \n",
        "    # 프로세스별 GPU 설정\n",
        "    if gpu_idx < 0:\n",
        "        # CPU 모드\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "        logger.info(f\"Worker using CPU mode\")\n",
        "    else:\n",
        "        # GPU 모드 - 특정 GPU 지정\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_idx)\n",
        "        logger.info(f\"Worker using GPU {gpu_idx} with memory fraction {memory_fraction}\")\n",
        "        \n",
        "        # GPU 메모리 설정\n",
        "        if torch.cuda.is_available():\n",
        "            # 메모리 효율성 개선\n",
        "            torch.cuda.empty_cache()\n",
        "            try:\n",
        "                torch.cuda.set_per_process_memory_fraction(memory_fraction, 0)\n",
        "            except (AttributeError, RuntimeError) as e:\n",
        "                logger.warning(f\"GPU 메모리 설정 실패: {e}\")\n",
        "                # 대체 메모리 관리\n",
        "                gc.collect()\n",
        "    \n",
        "    # 모델 로드 최적화 (프로세스별로 새로 로드)\n",
        "    global model, _embedding_cache\n",
        "    \n",
        "    # 임베딩 캐시 초기화\n",
        "    _embedding_cache = {}\n",
        "    \n",
        "    try:\n",
        "        # 모델 로드 경로가 올바른지 확인\n",
        "        model_path = './model/original_colbert_bm25_dense_hybrid_for_pairmining_checkpoint_512x8.torch'\n",
        "        if not os.path.exists(model_path):\n",
        "            logger.error(f\"모델 파일을 찾을 수 없습니다: {model_path}\")\n",
        "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "            \n",
        "        # 모델 로드\n",
        "        from colbert.modeling.checkpoint import Checkpoint\n",
        "        model = Checkpoint.load(model_path)\n",
        "        model.query_tokenizer.query_maxlen = 512\n",
        "        model.doc_tokenizer.doc_maxlen = 512\n",
        "        \n",
        "        # GPU 메모리 최적화\n",
        "        if gpu_idx >= 0 and torch.cuda.is_available():\n",
        "            try:\n",
        "                # 혼합 정밀도 사용 (Float16)\n",
        "                model.half()  # Float16 변환으로 메모리 사용량 절반으로 감소\n",
        "                model.cuda()\n",
        "                # 그래디언트 계산 비활성화 (추론 모드)\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for param in model.parameters():\n",
        "                        param.requires_grad = False\n",
        "            except RuntimeError as e:\n",
        "                logger.error(f\"GPU 모드 설정 오류: {e}\")\n",
        "                # GPU 오류 시 CPU로 폴백\n",
        "                model.cpu()\n",
        "                model.eval()\n",
        "        else:\n",
        "            # CPU 모드\n",
        "            model.cpu()\n",
        "            model.eval()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"모델 로드 오류: {e}\")\n",
        "        # 모델 로드 실패 시 원본 텍스트 반환\n",
        "        return [{\"문장식별자\": idx, \"구식별자\": 1, \"원문구\": src, \"번역구\": tgt} for idx, src, tgt in chunk_data]\n",
        "    \n",
        "    # 결과 저장 리스트\n",
        "    chunk_results = []\n",
        "    \n",
        "    # 청크 내 각 행 처리\n",
        "    for idx, src_text, tgt_text in chunk_data:\n",
        "        try:\n",
        "            # 빈 텍스트 체크\n",
        "            if not src_text or not tgt_text:\n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "                \n",
        "            # 괄호 마스킹\n",
        "            masked_src, src_masks = mask_brackets(src_text, text_type=\"source\")\n",
        "            masked_tgt, tgt_masks = mask_brackets(tgt_text, text_type=\"target\")\n",
        "            \n",
        "            # 의미 단위 분할\n",
        "            src_units = split_src_meaning_units(masked_src)\n",
        "            tgt_units = split_tgt_meaning_units(masked_src, masked_tgt, use_semantic=True, min_tokens=1)\n",
        "            \n",
        "            # 분할 결과 체크\n",
        "            if not src_units or not tgt_units:\n",
        "                logger.warning(f\"행 {idx}: 의미 단위 분할 결과가 비어 있습니다.\")\n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "            \n",
        "            # 마스크 복원\n",
        "            restored_src_units = [restore_masks(unit, src_masks) for unit in src_units]\n",
        "            restored_tgt_units = [restore_masks(unit, tgt_masks) for unit in tgt_units]\n",
        "            \n",
        "            # 정렬\n",
        "            try:\n",
        "                # compute_embeddings_with_cache 함수는 _embedding_cache를 참조\n",
        "                aligned_pairs = align_src_tgt(restored_src_units, restored_tgt_units, compute_embeddings_with_cache)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"행 {idx} 정렬 오류: {e}\")\n",
        "                # 정렬 실패 시 원본 텍스트 사용\n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "            \n",
        "            # 결과가 없으면 원본 텍스트로 대체\n",
        "            if not aligned_pairs:\n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "                \n",
        "            # 정렬 결과 언패킹\n",
        "            aligned_src_units, aligned_tgt_units = zip(*aligned_pairs)\n",
        "            \n",
        "            # 결과 저장\n",
        "            for gu_idx, (src_gu, tgt_gu) in enumerate(zip(aligned_src_units, aligned_tgt_units), start=1):\n",
        "                # 빈 결과 방지\n",
        "                if not src_gu.strip() or not tgt_gu.strip():\n",
        "                    continue\n",
        "                    \n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": gu_idx,\n",
        "                    \"원문구\": src_gu,\n",
        "                    \"번역구\": tgt_gu,\n",
        "                })\n",
        "            \n",
        "            # 결과가 생성되지 않았으면 원본 사용\n",
        "            if not any(r[\"문장식별자\"] == idx for r in chunk_results):\n",
        "                chunk_results.append({\n",
        "                    \"문장식별자\": idx,\n",
        "                    \"구식별자\": 1,\n",
        "                    \"원문구\": src_text,\n",
        "                    \"번역구\": tgt_text,\n",
        "                })\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"행 {idx} 처리 오류: {e}\")\n",
        "            # 오류 발생 시 원본 텍스트 저장\n",
        "            chunk_results.append({\n",
        "                \"문장식별자\": idx,\n",
        "                \"구식별자\": 1,\n",
        "                \"원문구\": src_text,\n",
        "                \"번역구\": tgt_text,\n",
        "            })\n",
        "    \n",
        "    # 프로세스 종료 시 메모리 정리\n",
        "    try:\n",
        "        # 메모리 정리\n",
        "        _embedding_cache.clear()\n",
        "        del model\n",
        "        gc.collect()\n",
        "        \n",
        "        # GPU 메모리 정리\n",
        "        if gpu_idx >= 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return chunk_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 메인 실행 함수 (main.py)\n",
        "\n",
        "파이프라인 전체를 실행하는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_with_options(\n",
        "    input_path: str, \n",
        "    output_path: str, \n",
        "    use_parallel: bool = False,\n",
        "    num_workers: int = None,\n",
        "    chunk_size: int = 20,\n",
        "    gpu_strategy: str = \"single\",\n",
        "    verbose: bool = False\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    옵션에 따라 병렬 또는 비병렬 처리를 선택하는 통합 인터페이스\n",
        "    \n",
        "    Args:\n",
        "        input_path: 입력 파일 경로\n",
        "        output_path: 출력 파일 경로\n",
        "        use_parallel: 병렬 처리 사용 여부\n",
        "        num_workers: 병렬 워커 수 (None=자동)\n",
        "        chunk_size: 데이터 청크 크기\n",
        "        gpu_strategy: GPU 활용 전략\n",
        "            - \"single\": 첫 번째 워커만 GPU 사용 (안전)\n",
        "            - \"shared\": 모든 워커가 GPU 공유 (고성능 GPU 필요)\n",
        "            - \"multi\": 여러 GPU에 분산 (다중 GPU 환경)\n",
        "            - \"none\": GPU 사용 안 함 (CPU만 사용)\n",
        "        verbose: 상세 로깅 여부\n",
        "    \"\"\"\n",
        "    import time\n",
        "    import logging\n",
        "    import os\n",
        "    \n",
        "    # 로거 확인\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    # 시작 시간\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # 파일 경로 검증\n",
        "    if not os.path.exists(input_path):\n",
        "        logger.error(f\"입력 파일을 찾을 수 없습니다: {input_path}\")\n",
        "        return\n",
        "        \n",
        "    # 출력 디렉토리 존재 확인\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    if output_dir and not os.path.exists(output_dir):\n",
        "        try:\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "            logger.info(f\"출력 디렉토리 생성: {output_dir}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"출력 디렉토리 생성 실패: {e}\")\n",
        "            return\n",
        "    \n",
        "    try:\n",
        "        # GPU 상태 확인\n",
        "        gpu_available = False\n",
        "        gpu_count = 0\n",
        "        gpu_info = []\n",
        "        \n",
        "        try:\n",
        "            import torch\n",
        "            gpu_available = torch.cuda.is_available()\n",
        "            if gpu_available:\n",
        "                gpu_count = torch.cuda.device_count()\n",
        "                for i in range(gpu_count):\n",
        "                    try:\n",
        "                        gpu_name = torch.cuda.get_device_name(i)\n",
        "                        total_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)  # GB\n",
        "                        gpu_info.append(f\"GPU {i}: {gpu_name} ({total_memory:.2f} GB)\")\n",
        "                    except:\n",
        "                        gpu_info.append(f\"GPU {i}: Information unavailable\")\n",
        "                \n",
        "                logger.info(f\"Available GPUs: {gpu_count}\")\n",
        "                for info in gpu_info:\n",
        "                    logger.info(info)\n",
        "            else:\n",
        "                logger.info(\"No GPU available, using CPU only\")\n",
        "                if gpu_strategy != \"none\":\n",
        "                    logger.warning(\"GPU strategy requested but no GPU available, falling back to CPU\")\n",
        "                    gpu_strategy = \"none\"\n",
        "        except ImportError:\n",
        "            logger.warning(\"torch module not available, using CPU only\")\n",
        "            gpu_strategy = \"none\"\n",
        "        \n",
        "        # 처리 방식 선택\n",
        "        if use_parallel:\n",
        "            # 병렬 처리 설정 로깅\n",
        "            worker_info = \"auto\" if num_workers is None else str(num_workers)\n",
        "            logger.info(f\"Using parallel processing with {worker_info} workers, chunk_size={chunk_size}, gpu_strategy={gpu_strategy}\")\n",
        "            \n",
        "            # 병렬 처리 실행\n",
        "            process_file_parallel(\n",
        "                input_path=input_path,\n",
        "                output_path=output_path,\n",
        "                num_workers=num_workers,\n",
        "                chunk_size=chunk_size,\n",
        "                gpu_strategy=gpu_strategy\n",
        "            )\n",
        "        else:\n",
        "            # 비병렬 처리 로깅\n",
        "            logger.info(\"Using sequential processing\")\n",
        "            \n",
        "            # 기존 process_file 함수가 있는지 확인\n",
        "            if 'process_file' in globals():\n",
        "                # 기존 함수 호출\n",
        "                process_file(\n",
        "                    input_path=input_path,\n",
        "                    output_path=output_path,\n",
        "                    batch_size=128,\n",
        "                    verbose=verbose\n",
        "                )\n",
        "            else:\n",
        "                logger.error(\"Sequential processing function (process_file) not found\")\n",
        "                return\n",
        "        \n",
        "        # 처리 시간 기록\n",
        "        elapsed_time = time.time() - start_time\n",
        "        logger.info(f\"Processing completed in {elapsed_time:.2f} seconds\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Processing failed: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "        raise\n",
        "\n",
        "def main(input_file, output_file, verbose=False, use_parallel=False, num_workers=None, chunk_size=20, gpu_strategy=\"single\"):\n",
        "    \"\"\"\n",
        "    Execute the pipeline with optional parallel processing.\n",
        "    \n",
        "    Args:\n",
        "        input_file: Path to input Excel file\n",
        "        output_file: Path to output Excel file\n",
        "        verbose: Enable verbose logging\n",
        "        use_parallel: Enable parallel processing\n",
        "        num_workers: Number of worker processes (None = auto)\n",
        "        chunk_size: Size of data chunks for parallel processing\n",
        "        gpu_strategy: GPU strategy (\"single\", \"shared\", \"multi\", or \"none\")\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(input_file):\n",
        "        logger.error(f\"Input file does not exist: {input_file}\")\n",
        "        return\n",
        "    if not input_file.lower().endswith(('.xls', '.xlsx')):\n",
        "        logger.error(\"Input file must have .xls/.xlsx extension.\")\n",
        "        return\n",
        "    if not output_file.lower().endswith('.xlsx'):\n",
        "        logger.error(\"Output file must have .xlsx extension.\")\n",
        "        return\n",
        "\n",
        "    if verbose:\n",
        "        logging.getLogger().setLevel(logging.INFO)\n",
        "        logger.info(\"Verbose mode activated: INFO level logging enabled.\")\n",
        "    \n",
        "    # Log parallel processing options if enabled\n",
        "    if use_parallel:\n",
        "        worker_info = \"auto\" if num_workers is None else num_workers\n",
        "        logger.info(f\"Parallel processing enabled: workers={worker_info}, chunk_size={chunk_size}, gpu_strategy={gpu_strategy}\")\n",
        "\n",
        "    try:\n",
        "        # Use the unified interface function instead of direct process_file call\n",
        "        process_with_options(\n",
        "            input_path=input_file,\n",
        "            output_path=output_file,\n",
        "            use_parallel=use_parallel,\n",
        "            num_workers=num_workers,\n",
        "            chunk_size=chunk_size,\n",
        "            gpu_strategy=gpu_strategy,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        logger.info(f\"Processing completed: {output_file}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Critical error occurred during pipeline execution: {e}\", exc_info=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 테스트\n",
        "\n",
        "아래 셀을 실행하여 간단한 예제로 파이프라인을 테스트해 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing function\n",
        "def create_test_data(file_path=\"test_input.xlsx\"):\n",
        "    \"\"\"Generate test data.\"\"\"\n",
        "    test_data = [\n",
        "        {\n",
        "            \"원문\": \"作詁訓傳時에 移其篇第하고 因改之耳라\",\n",
        "            \"번역문\": \"주석과 해설을 작성할 때에 그 편과 장을 옮기고 그에 따라 고쳤을 뿐이다.\"\n",
        "        },\n",
        "        {\n",
        "            \"원문\": \"古來相傳하야 學者가 於其說에 未嘗致疑하니라\",\n",
        "            \"번역문\": \"예로부터 서로 전해져 학자들은 그 설에 대해 의심을 품은 적이 없었다.\"\n",
        "        },\n",
        "        {\n",
        "            \"원문\": \"夫雅頌之作也 詩人各有所屬者也\",\n",
        "            \"번역문\": \"무릇 아송의 창작은 시인마다 각자 속한 바가 있었다.\"\n",
        "        },\n",
        "        {    \n",
        "            \"원문\": \"然而孔子取而次之者하야 則有家國之次矣\",\n",
        "            \"번역문\": \"그런데 공자가 이것을 취하여 차례를 매긴 것은 가국의 순서에 따른 것이다.\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    df = pd.DataFrame(test_data)\n",
        "    df.to_excel(file_path, index=False, engine='openpyxl')\n",
        "    return file_path\n",
        "\n",
        "# Testing execution\n",
        "test_input = create_test_data()\n",
        "test_output = \"test_output.xlsx\"\n",
        "\n",
        "# Run the pipeline\n",
        "main(test_input, test_output, verbose=True)\n",
        "\n",
        "# Check results\n",
        "try:\n",
        "    result_df = pd.read_excel(test_output)\n",
        "    display(result_df)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to read result file: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 단일 문장 테스트\n",
        "\n",
        "특정 문장 쌍에 대해 빠르게 테스트해볼 수 있는 기능입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single sentence test function\n",
        "def test_single_alignment(src_text, tgt_text):\n",
        "    \"\"\"Test alignment for a single sentence pair.\"\"\"\n",
        "    print(\"=== Input Sentences ===\")\n",
        "    print(f\"Source: {src_text}\")\n",
        "    print(f\"Target: {tgt_text}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    try:\n",
        "        # Preprocessing\n",
        "        masked_src, src_masks = mask_brackets(src_text, text_type=\"source\")\n",
        "        masked_tgt, tgt_masks = mask_brackets(tgt_text, text_type=\"target\")\n",
        "\n",
        "        # Split into units\n",
        "        src_units = split_src_meaning_units(masked_src)\n",
        "        print(\"=== Source Units ===\")\n",
        "        for i, unit in enumerate(src_units):\n",
        "            print(f\"[{i+1}] {unit}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        tgt_units = split_tgt_meaning_units(\n",
        "            masked_src,\n",
        "            masked_tgt,\n",
        "            use_semantic=True,\n",
        "            min_tokens=1,\n",
        "            max_tokens=50\n",
        "        )\n",
        "        tgt_units = [restore_masks(unit, tgt_masks) for unit in tgt_units]\n",
        "        print(\"=== Target Units ===\")\n",
        "        for i, unit in enumerate(tgt_units):\n",
        "            print(f\"[{i+1}] {unit}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        # Alignment\n",
        "        aligned_pairs = align_src_tgt(src_units, tgt_units, compute_embeddings_with_cache)\n",
        "        aligned_src_units, aligned_tgt_units = zip(*aligned_pairs)\n",
        "        aligned_src_units = [restore_masks(unit, src_masks) for unit in aligned_src_units]\n",
        "        aligned_tgt_units = [restore_masks(unit, tgt_masks) for unit in aligned_tgt_units]\n",
        "\n",
        "        print(\"=== Alignment Results ===\")\n",
        "        for i, (src_gu, tgt_gu) in enumerate(zip(aligned_src_units, aligned_tgt_units), 1):\n",
        "            print(f\"[{i}] Source: {src_gu}\")\n",
        "            print(f\"    Target: {tgt_gu}\")\n",
        "            print()\n",
        "\n",
        "        return aligned_src_units, aligned_tgt_units\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return [], []\n",
        "\n",
        "# Single sentence test execution\n",
        "src_example = \"作詁訓傳時에 移其篇第하고 因改之耳라\"\n",
        "tgt_example = \"주석과 해설을 작성할 때에 그 편과 장을 옮기고 그에 따라 고쳤을 뿐이다.\"\n",
        "\n",
        "test_single_alignment(src_example, tgt_example)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
