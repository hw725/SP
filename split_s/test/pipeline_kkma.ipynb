{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ì›ë¬¸-ë²ˆì—­ë¬¸ ì˜ë¯¸ ê¸°ë°˜ êµ¬ ë‹¨ìœ„ ë§¤ì¹­ íŒŒì´í”„ë¼ì¸\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ í•œë¬¸ ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ ê°„ì˜ êµ¬ ë‹¨ìœ„ ì •ë ¬ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ì£¼ìš” ê¸°ëŠ¥ì„ í¬í•¨í•©ë‹ˆë‹¤:\n",
        "\n",
        "1. **í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•**: ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• \n",
        "2. **ì„ë² ë”© ê³„ì‚°**: ê° ì˜ë¯¸ ë‹¨ìœ„ì— ëŒ€í•œ ë²¡í„° í‘œí˜„ ìƒì„±\n",
        "3. **êµ¬ ë‹¨ìœ„ ì •ë ¬**: ë™ì  í”„ë¡œê·¸ë˜ë° ê¸°ë°˜ ì •ë ¬ ì•Œê³ ë¦¬ì¦˜\n",
        "4. **íŒŒì¼ ì…ì¶œë ¥**: Excel íŒŒì¼ ì²˜ë¦¬\n",
        "\n",
        "ì´ íŒŒì´í”„ë¼ì¸ì€ ì›ë¬¸-ë²ˆì—­ë¬¸ ìŒì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ êµ¬ ë‹¨ìœ„ë¡œ ì •ë ¬ëœ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì„í¬íŠ¸\n",
        "\n",
        "ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³  ì„í¬íŠ¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install necessary packages (GPU version with CUDA 12.1)\n",
        "%pip install regex pandas numpy tqdm openpyxl konlpy\n",
        "%pip install torch>=2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install sentence-transformers FlagEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import torch\n",
        "import regex as re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Dict, Any, Optional, Callable\n",
        "from tqdm.notebook import tqdm\n",
        "from konlpy.tag import Kkma\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "import sentencepiece as spm\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from transformers import XLMRobertaTokenizerFast\n",
        "\n",
        "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
        "os.environ['PYTORCH_DISABLE_TORCH_LOAD_SECURITY_CHECK'] = '1'\n",
        "\n",
        "# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
        "tokenizer = Kkma()\n",
        "\n",
        "# ëª¨ë¸ ë¡œë”© with ë‹¤ì¤‘ fallback\n",
        "def load_embedding_model():\n",
        "    \"\"\"ì—¬ëŸ¬ ì˜µì…˜ì„ ì‹œë„í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\"\"\"\n",
        "    \n",
        "    # Option 1: BGE ëª¨ë¸\n",
        "    try:\n",
        "        from FlagEmbedding import BGEM3FlagModel\n",
        "        model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
        "        print(\"âœ… BGE model loaded successfully\")\n",
        "        return model, 'bge'\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ BGE model failed: {e}\")\n",
        "    \n",
        "    # Option 2: SentenceTransformers\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        print(\"âœ… SentenceTransformer model loaded successfully\")\n",
        "        return model, 'sentence_transformer'\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ SentenceTransformer failed: {e}\")\n",
        "    \n",
        "    # Option 3: ë”ë¯¸ ëª¨ë¸\n",
        "    print(\"ğŸ”„ Using dummy embeddings\")\n",
        "    return None, 'dummy'\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "model, model_type = load_embedding_model()\n",
        "\n",
        "# ë²”ìš© ì„ë² ë”© í•¨ìˆ˜\n",
        "def compute_embeddings(texts, model=model, model_type=model_type):\n",
        "    \"\"\"ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì„ë² ë”© ê³„ì‚°\"\"\"\n",
        "    if model_type == 'bge':\n",
        "        output = model.encode(texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
        "        return output['dense_vecs']\n",
        "    elif model_type == 'sentence_transformer':\n",
        "        return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    else:  # dummy\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            import hashlib\n",
        "            text_hash = hashlib.md5(str(text).encode()).hexdigest()\n",
        "            seed = int(text_hash[:8], 16) % (2**31)\n",
        "            np.random.seed(seed)\n",
        "            dummy_emb = np.random.randn(384).astype(np.float32)\n",
        "            dummy_emb = dummy_emb / (np.linalg.norm(dummy_emb) + 1e-8)\n",
        "            embeddings.append(dummy_emb)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "# Logging setup\n",
        "logging.basicConfig(\n",
        "    format=\"[%(levelname)s] %(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "print(f\"âœ… Setup complete. Using {model_type} embeddings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. ì„ë² ë”© ëª¨ë“ˆ (embedder.py)\n",
        "\n",
        "í…ìŠ¤íŠ¸ êµ¬ì— ëŒ€í•œ ì„ë² ë”©ì„ ê³„ì‚°í•˜ê³  ìºì‹œë¥¼ ê´€ë¦¬í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from konlpy.tag import Kkma\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "# ë¡œê±° ì„¤ì • (ë¨¼ì € ì„¤ì •í•´ì•¼ í•¨)\n",
        "logging.basicConfig(\n",
        "    format=\"[%(levelname)s] %(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
        "os.environ['PYTORCH_DISABLE_TORCH_LOAD_SECURITY_CHECK'] = '1'\n",
        "\n",
        "# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
        "tokenizer = Kkma()\n",
        "\n",
        "# ëª¨ë¸ ë¡œë”© with ë‹¤ì¤‘ fallback\n",
        "def load_embedding_model():\n",
        "    \"\"\"ì—¬ëŸ¬ ì˜µì…˜ì„ ì‹œë„í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\"\"\"\n",
        "    \n",
        "    # Option 1: BGE ëª¨ë¸\n",
        "    try:\n",
        "        from FlagEmbedding import BGEM3FlagModel\n",
        "        model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
        "        logger.info(\"âœ… BGE model loaded successfully\")\n",
        "        return model, 'bge'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"âŒ BGE model failed: {e}\")\n",
        "    \n",
        "    # Option 2: SentenceTransformers\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        logger.info(\"âœ… SentenceTransformer model loaded successfully\")\n",
        "        return model, 'sentence_transformer'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"âŒ SentenceTransformer failed: {e}\")\n",
        "    \n",
        "    # Option 3: ë”ë¯¸ ëª¨ë¸\n",
        "    logger.info(\"ğŸ”„ Using dummy embeddings\")\n",
        "    return None, 'dummy'\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "model, model_type = load_embedding_model()\n",
        "\n",
        "# ë²”ìš© ì„ë² ë”© í•¨ìˆ˜\n",
        "def compute_embeddings(texts, model=model, model_type=model_type):\n",
        "    \"\"\"ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì„ë² ë”© ê³„ì‚°\"\"\"\n",
        "    if model_type == 'bge':\n",
        "        output = model.encode(texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
        "        return output['dense_vecs']\n",
        "    elif model_type == 'sentence_transformer':\n",
        "        return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    else:  # dummy\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            import hashlib\n",
        "            text_hash = hashlib.md5(str(text).encode()).hexdigest()\n",
        "            seed = int(text_hash[:8], 16) % (2**31)\n",
        "            np.random.seed(seed)\n",
        "            dummy_emb = np.random.randn(384).astype(np.float32)\n",
        "            dummy_emb = dummy_emb / (np.linalg.norm(dummy_emb) + 1e-8)\n",
        "            embeddings.append(dummy_emb)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "logger.info(f\"âœ… Setup complete. Using {model_type} embeddings.\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸\n",
        "test_texts = [\"í…ŒìŠ¤íŠ¸ ë¬¸ì¥\", \"ë˜ ë‹¤ë¥¸ ë¬¸ì¥\"]\n",
        "test_embeddings = compute_embeddings(test_texts)\n",
        "logger.info(f\"Test embeddings shape: {test_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. êµ¬ë‘ì  ì²˜ë¦¬ ëª¨ë“ˆ (punctuation.py)\n",
        "\n",
        "ê´„í˜¸ ì²˜ë¦¬ ë° ë§ˆìŠ¤í‚¹ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Punctuation handling module\n",
        "MASK_TEMPLATE = '[MASK{}]'\n",
        "\n",
        "HALF_WIDTH_BRACKETS = [\n",
        "    ('(', ')'),\n",
        "    ('[', ']'),\n",
        "]\n",
        "FULL_WIDTH_BRACKETS = [\n",
        "    ('ï¼ˆ', 'ï¼‰'),\n",
        "    ('ï¼»', 'ï¼½'),\n",
        "]\n",
        "TRANS_BRACKETS = [\n",
        "    ('<', '>'),\n",
        "    ('ã€Š', 'ã€‹'),\n",
        "    ('ã€ˆ', 'ã€‰'),\n",
        "    ('ã€Œ', 'ã€'),\n",
        "    ('ã€', 'ã€'),\n",
        "    ('ã€”', 'ã€•'),\n",
        "    ('ã€', 'ã€‘'),\n",
        "    ('ã€–', 'ã€—'),\n",
        "    ('ã€˜', 'ã€™'),\n",
        "    ('ã€š', 'ã€›'),\n",
        "]\n",
        "\n",
        "ALL_BRACKETS = HALF_WIDTH_BRACKETS + FULL_WIDTH_BRACKETS + TRANS_BRACKETS\n",
        "\n",
        "def mask_brackets(text: str, text_type: str) -> Tuple[str, List[str]]:\n",
        "    \"\"\"Mask content within brackets according to rules.\"\"\"\n",
        "    assert text_type in {'source', 'target'}, \"text_type must be 'source' or 'target'\"\n",
        "\n",
        "    masks: List[str] = []\n",
        "    mask_id = [0]\n",
        "\n",
        "    def safe_sub(pattern, repl, s):\n",
        "        def safe_replacer(m):\n",
        "            if '[MASK' in m.group(0):\n",
        "                return m.group(0)\n",
        "            return repl(m)\n",
        "        return pattern.sub(safe_replacer, s)\n",
        "\n",
        "    patterns: List[Tuple[re.Pattern, bool]] = []\n",
        "\n",
        "    if text_type == 'source':\n",
        "        for left, right in HALF_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left) + r'[^' + re.escape(left + right) + r']*?' + re.escape(right)), True))\n",
        "        for left, right in FULL_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left)), False))\n",
        "            patterns.append((re.compile(re.escape(right)), False))\n",
        "    elif text_type == 'target':\n",
        "        for left, right in HALF_WIDTH_BRACKETS + FULL_WIDTH_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left) + r'[^' + re.escape(left + right) + r']*?' + re.escape(right)), True))\n",
        "        for left, right in TRANS_BRACKETS:\n",
        "            patterns.append((re.compile(re.escape(left)), False))\n",
        "            patterns.append((re.compile(re.escape(right)), False))\n",
        "\n",
        "    def mask_content(s: str, pattern: re.Pattern, content_mask: bool) -> str:\n",
        "        def replacer(match: re.Match) -> str:\n",
        "            token = MASK_TEMPLATE.format(mask_id[0])\n",
        "            masks.append(match.group())\n",
        "            mask_id[0] += 1\n",
        "            return token\n",
        "        return safe_sub(pattern, replacer, s)\n",
        "\n",
        "    for pattern, content_mask in patterns:\n",
        "        if content_mask:\n",
        "            text = mask_content(text, pattern, content_mask)\n",
        "    for pattern, content_mask in patterns:\n",
        "        if not content_mask:\n",
        "            text = mask_content(text, pattern, content_mask)\n",
        "\n",
        "    return text, masks\n",
        "\n",
        "def restore_masks(text: str, masks: List[str]) -> str:\n",
        "    \"\"\"Restore masked tokens to their original content.\"\"\"\n",
        "    for i, original in enumerate(masks):\n",
        "        text = text.replace(MASK_TEMPLATE.format(i), original)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. í† í¬ë‚˜ì´ì € ëª¨ë“ˆ (tokenizer.py)\n",
        "\n",
        "ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ëª¨ë“ˆ\"\"\"\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import regex\n",
        "import re\n",
        "from typing import List, Callable\n",
        "from konlpy.tag import Kkma\n",
        "\n",
        "# Kkma í˜•íƒœì†Œ ë¶„ì„ê¸°\n",
        "kkma = Kkma()\n",
        "\n",
        "# ë¯¸ë¦¬ ì»´íŒŒì¼ëœ ì •ê·œì‹\n",
        "hanja_re    = regex.compile(r'\\p{Han}+')\n",
        "hangul_re   = regex.compile(r'^\\p{Hangul}+$')\n",
        "combined_re = regex.compile(\n",
        "    r'(\\p{Han}+)+(?:\\p{Hangul}+)(?:ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ìœ¼ë¡œ|ë¡œ|ì™€|ê³¼|ë„|ë§Œ|ìœ¼ë©°|ê³ |í•˜ê³ |ì˜|ë•Œ)?'\n",
        ")\n",
        "\n",
        "def split_src_meaning_units(text: str) -> list[str]:\n",
        "    text = text.replace('\\n', ' ').replace('ï¼š', 'ï¼š ')\n",
        "    tokens = regex.findall(r'\\S+', text)\n",
        "    units: list[str] = []\n",
        "    i = 0\n",
        "\n",
        "    while i < len(tokens):\n",
        "        tok = tokens[i]\n",
        "        m = combined_re.match(tok)\n",
        "        if m:\n",
        "            units.append(m.group(0))\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        if hanja_re.search(tok):\n",
        "            unit = tok\n",
        "            j = i + 1\n",
        "            while j < len(tokens) and hangul_re.match(tokens[j]):\n",
        "                unit += tokens[j]\n",
        "                j += 1\n",
        "            units.append(unit)\n",
        "            i = j\n",
        "            continue\n",
        "\n",
        "        if hangul_re.match(tok):\n",
        "            korean_tokens = kkma.morphs(tok)\n",
        "            units.extend(korean_tokens)\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        units.append(tok)\n",
        "        i += 1\n",
        "\n",
        "    return units\n",
        "\n",
        "def split_inside_chunk(chunk: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    ì¡°ì‚¬, ì–´ë¯¸, ê·¸ë¦¬ê³  'ï¼š' ê¸°ì¤€ìœ¼ë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• \n",
        "    ì›í˜• ë³´ì¡´, ê³µë°± ì‚½ì… ì—†ì´ ë¶„ë¦¬\n",
        "    \"\"\"\n",
        "    delimiters = ['ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ',\n",
        "                  'ì™€', 'ê³¼', 'ê³ ', 'ë©°', 'í•˜ê³ ', 'ë•Œ', 'ì˜', 'ë„', 'ë§Œ', 'ï¼š']\n",
        "    \n",
        "    # lookbehind íŒ¨í„´ ìƒì„±\n",
        "    pattern = '|'.join([f'(?<={re.escape(d)})' for d in delimiters])\n",
        "    try:\n",
        "        parts = re.split(pattern, chunk)\n",
        "        return [p.strip() for p in parts if p.strip()]\n",
        "    except:\n",
        "        return [p for p in chunk.split() if p.strip()]\n",
        "\n",
        "def find_target_span_end_simple(src_unit: str, remaining_tgt: str) -> int:\n",
        "    hanja_chars = regex.findall(r'\\p{Han}+', src_unit)\n",
        "    if not hanja_chars:\n",
        "        return 0\n",
        "    last = hanja_chars[-1]\n",
        "    idx = remaining_tgt.rfind(last)\n",
        "    if idx == -1:\n",
        "        return len(remaining_tgt)\n",
        "    end = idx + len(last)\n",
        "    next_space = remaining_tgt.find(' ', end)\n",
        "    return next_space + 1 if next_space != -1 else len(remaining_tgt)\n",
        "\n",
        "def find_target_span_end_semantic(\n",
        "    src_unit: str,\n",
        "    remaining_tgt: str,\n",
        "    embed_func=compute_embeddings_with_cache,\n",
        "    min_tokens: int = 1,\n",
        "    max_tokens: int = 50,\n",
        "    similarity_threshold: float = 0.4\n",
        ") -> int:\n",
        "    \"\"\"ìµœì í™”ëœ íƒ€ê²Ÿ ìŠ¤íŒ¬ íƒìƒ‰ í•¨ìˆ˜\"\"\"\n",
        "    # ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”\n",
        "    if not src_unit or not remaining_tgt:\n",
        "        return 0\n",
        "        \n",
        "    try:\n",
        "        # 1) ì›ë¬¸ ì„ë² ë”© (ë‹¨ì¼ ê³„ì‚°)\n",
        "        src_emb = embed_func([src_unit])[0]\n",
        "        \n",
        "        # 2) ë²ˆì—­ë¬¸ í† í° ë¶„ë¦¬ ë° ëˆ„ì  ê¸¸ì´ ê³„ì‚°\n",
        "        tgt_tokens = remaining_tgt.split()\n",
        "        if not tgt_tokens:\n",
        "            return 0\n",
        "            \n",
        "        upper = min(len(tgt_tokens), max_tokens)\n",
        "        cumulative_lengths = [0]\n",
        "        current_length = 0\n",
        "        \n",
        "        for tok in tgt_tokens:\n",
        "            current_length += len(tok) + 1  # í† í° + ê³µë°±\n",
        "            cumulative_lengths.append(current_length)\n",
        "            \n",
        "        # 3) í›„ë³´ ì„¸ê·¸ë¨¼íŠ¸ ìƒì„± (ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°©ì‹)\n",
        "        candidates = []\n",
        "        candidate_indices = []\n",
        "        \n",
        "        # ì ì€ ìˆ˜ì˜ í›„ë³´ë§Œ ìƒì„±í•˜ì—¬ íš¨ìœ¨ì„± í–¥ìƒ\n",
        "        step_size = 1 if upper <= 10 else 2  # í† í° 10ê°œ ì´í•˜ë©´ ëª¨ë“  í›„ë³´ ê²€ì‚¬, ì´ìƒì´ë©´ 2ì¹¸ì”©\n",
        "        \n",
        "        for end_i in range(min_tokens-1, upper, step_size):\n",
        "            cand = \" \".join(tgt_tokens[:end_i+1])\n",
        "            candidates.append(cand)\n",
        "            candidate_indices.append(end_i)\n",
        "            \n",
        "        # 4) ë°°ì¹˜ ì„ë² ë”© (í•œ ë²ˆì— ê³„ì‚°)\n",
        "        cand_embs = embed_func(candidates)\n",
        "        \n",
        "        # 5) ìµœì  ë§¤ì¹­ íƒìƒ‰ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ + ê¸¸ì´ íŒ¨ë„í‹°)\n",
        "        best_score = -1.0\n",
        "        best_end_idx = cumulative_lengths[-1]  # ê¸°ë³¸ê°’ì€ ì „ì²´ ê¸¸ì´\n",
        "        \n",
        "        for i, emb in enumerate(cand_embs):\n",
        "            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
        "            score = np.dot(src_emb, emb) / (np.linalg.norm(src_emb) * np.linalg.norm(emb) + 1e-8)\n",
        "            \n",
        "            # ê¸¸ì´ íŒ¨ë„í‹° (ë„ˆë¬´ ì§§ì€ ë§¤ì¹­ ë°©ì§€)\n",
        "            end_i = candidate_indices[i]\n",
        "            length_ratio = (end_i + 1) / len(tgt_tokens)\n",
        "            length_penalty = min(1.0, length_ratio * 2)  # ìµœëŒ€ 1.0\n",
        "            \n",
        "            adjusted_score = score * length_penalty\n",
        "            \n",
        "            # ì„ê³„ê°’ í™•ì¸ ë° ìµœëŒ€ê°’ ê°±ì‹ \n",
        "            if adjusted_score > best_score and score >= similarity_threshold:\n",
        "                best_score = adjusted_score\n",
        "                best_end_idx = cumulative_lengths[end_i + 1]\n",
        "                \n",
        "        return best_end_idx\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.warning(f\"ì˜ë¯¸ ë§¤ì¹­ ì˜¤ë¥˜, ë‹¨ìˆœ ë§¤ì¹­ìœ¼ë¡œ ëŒ€ì²´: {e}\")\n",
        "        return find_target_span_end_simple(src_unit, remaining_tgt)\n",
        "\n",
        "\n",
        "def split_tgt_by_src_units(src_units: list[str], tgt_text: str) -> list[str]:\n",
        "    results = []\n",
        "    cursor = 0\n",
        "    total = len(tgt_text)\n",
        "    for src_u in src_units:\n",
        "        remaining = tgt_text[cursor:]\n",
        "        end_len = find_target_span_end_simple(src_u, remaining)\n",
        "        chunk = tgt_text[cursor:cursor+end_len]\n",
        "        results.extend(split_inside_chunk(chunk))\n",
        "        cursor += end_len\n",
        "    if cursor < total:\n",
        "        results.extend(split_inside_chunk(tgt_text[cursor:]))\n",
        "    return results\n",
        "\n",
        "def split_tgt_by_src_units_semantic(src_units, tgt_text, embed_func=compute_embeddings_with_cache, min_tokens=1):\n",
        "    tgt_tokens = tgt_text.split()\n",
        "    N, T = len(src_units), len(tgt_tokens)\n",
        "    if N == 0 or T == 0:\n",
        "        return []\n",
        "\n",
        "    dp = np.full((N+1, T+1), -np.inf)\n",
        "    back = np.zeros((N+1, T+1), dtype=int)\n",
        "    dp[0, 0] = 0.0\n",
        "\n",
        "    # ì›ë¬¸ ì„ë² ë”© ê³„ì‚°\n",
        "    src_embs = embed_func(src_units)\n",
        "\n",
        "    # DP í…Œì´ë¸” ì±„ìš°ê¸° (j ë£¨í”„ ë²”ìœ„ ì£¼ì˜!)\n",
        "    for i in range(1, N+1):\n",
        "        for j in range(i*min_tokens, T-(N-i)*min_tokens+1):\n",
        "            for k in range((i-1)*min_tokens, j-min_tokens+1):\n",
        "                span = \" \".join(tgt_tokens[k:j])\n",
        "                tgt_emb = embed_func([span])[0]\n",
        "                sim = float(np.dot(src_embs[i-1], tgt_emb)/((np.linalg.norm(src_embs[i-1])*np.linalg.norm(tgt_emb))+1e-8))\n",
        "                score = dp[i-1, k] + sim\n",
        "                if score > dp[i, j]:\n",
        "                    dp[i, j] = score\n",
        "                    back[i, j] = k\n",
        "\n",
        "    # Traceback\n",
        "    cuts = [T]\n",
        "    curr = T\n",
        "    for i in range(N, 0, -1):\n",
        "        prev = int(back[i, curr])\n",
        "        cuts.append(prev)\n",
        "        curr = prev\n",
        "    cuts = cuts[::-1]\n",
        "    assert cuts[0] == 0 and cuts[-1] == T and len(cuts) == N + 1\n",
        "\n",
        "    # Build actual spans\n",
        "    tgt_spans = []\n",
        "    for i in range(N):\n",
        "        span = \" \".join(tgt_tokens[cuts[i]:cuts[i+1]]).strip()\n",
        "        tgt_spans.append(span)\n",
        "    return tgt_spans\n",
        "\n",
        "def split_tgt_meaning_units(\n",
        "    src_text: str,\n",
        "    tgt_text: str,\n",
        "    use_semantic: bool = True,\n",
        "    min_tokens: int = 1,\n",
        "    max_tokens: int = 50\n",
        ") -> list[str]:\n",
        "    src_units = split_src_meaning_units(src_text)\n",
        "\n",
        "    if use_semantic:\n",
        "        return split_tgt_by_src_units_semantic(\n",
        "            src_units,\n",
        "            tgt_text,\n",
        "            embed_func=compute_embeddings_with_cache,\n",
        "            min_tokens=min_tokens\n",
        "        )\n",
        "    else:\n",
        "        return split_tgt_by_src_units(src_units, tgt_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. ì •ë ¬ ëª¨ë“ˆ (aligner.py)\n",
        "\n",
        "ì›ë¬¸ê³¼ ë²ˆì—­ë¬¸ êµ¬ ê°„ì˜ ì •ë ¬ì„ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aligner module\n",
        "def cosine_similarity(vec1: Any, vec2: Any) -> float:\n",
        "    \"\"\"Calculate cosine similarity (handling zero vectors).\"\"\"\n",
        "    norm1 = np.linalg.norm(vec1)\n",
        "    norm2 = np.linalg.norm(vec2)\n",
        "    if norm1 == 0 or norm2 == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(vec1, vec2) / (norm1 * norm2))\n",
        "\n",
        "def align_src_tgt(src_units, tgt_units, embed_func=compute_embeddings_with_cache):\n",
        "    \"\"\"Align source and target units.\"\"\"\n",
        "    logger.info(f\"Source units: {len(src_units)} items, Target units: {len(tgt_units)} items\")\n",
        "\n",
        "    if len(src_units) != len(tgt_units):\n",
        "        try:\n",
        "            flatten_tgt = \" \".join(tgt_units)\n",
        "            new_tgt_units = split_tgt_by_src_units_semantic(src_units, flatten_tgt, embed_func, min_tokens=1)\n",
        "            if len(new_tgt_units) == len(src_units):\n",
        "                logger.info(\"Semantic re-alignment successful\")\n",
        "                return list(zip(src_units, new_tgt_units))\n",
        "            else:\n",
        "                logger.warning(f\"Length mismatch after re-alignment: Source={len(src_units)}, Target={len(new_tgt_units)}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during semantic re-alignment: {e}\")\n",
        "\n",
        "        if len(src_units) > len(tgt_units):\n",
        "            tgt_units.extend([\"\"] * (len(src_units) - len(tgt_units)))\n",
        "        else:\n",
        "            src_units.extend([\"\"] * (len(tgt_units) - len(src_units)))\n",
        "\n",
        "    return list(zip(src_units, tgt_units))\n",
        "\n",
        "def calculate_alignment_matrix(src_embs, tgt_embs, batch_size=512):\n",
        "    \"\"\"Optimized function for calculating large similarity matrices.\"\"\"\n",
        "    src_len, tgt_len = len(src_embs), len(tgt_embs)\n",
        "    similarity_matrix = np.zeros((src_len, tgt_len))\n",
        "\n",
        "    for i in range(0, src_len, batch_size):\n",
        "        batch_src = src_embs[i:i + batch_size]\n",
        "        for j in range(0, tgt_len, batch_size):\n",
        "            batch_tgt = tgt_embs[j:j + batch_size]\n",
        "            batch_src_norm = np.linalg.norm(batch_src, axis=1, keepdims=True)\n",
        "            batch_tgt_norm = np.linalg.norm(batch_tgt, axis=1, keepdims=True)\n",
        "\n",
        "            dots = np.matmul(batch_src, batch_tgt.T)\n",
        "            norms = np.matmul(batch_src_norm, batch_tgt_norm.T)\n",
        "            batch_sim = dots / (norms + 1e-8)\n",
        "\n",
        "            similarity_matrix[i:i + batch_size, j:j + batch_size] = batch_sim\n",
        "\n",
        "    return similarity_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. I/O ëª¨ë“ˆ (io_manager.py)\n",
        "\n",
        "Excel íŒŒì¼ì„ ì½ê³  ì²˜ë¦¬í•œ í›„ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I/O module\n",
        "def process_file(input_path: str, output_path: str, batch_size: int = 128, verbose: bool = False) -> None:\n",
        "    \"\"\"ì²­í¬ ë‹¨ìœ„ë¡œ ìµœì í™”ëœ íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(input_path, engine='openpyxl')\n",
        "    except Exception as e:\n",
        "        logger.error(f\"[IO] Failed to read Excel file: {e}\")\n",
        "        return\n",
        "\n",
        "    if 'ì›ë¬¸' not in df.columns or 'ë²ˆì—­ë¬¸' not in df.columns:\n",
        "        logger.error(\"[IO] Missing 'ì›ë¬¸' or 'ë²ˆì—­ë¬¸' columns.\")\n",
        "        return\n",
        "\n",
        "    outputs: List[Dict[str, Any]] = []\n",
        "    total_rows = len(df)\n",
        "    \n",
        "    # ì²˜ë¦¬í•  í–‰ì„ ì‘ì€ ì²­í¬ë¡œ ë¶„í• \n",
        "    chunk_size = min(50, total_rows)  # ìµœëŒ€ 50í–‰ì”© ì²˜ë¦¬\n",
        "    \n",
        "    # ì„ë² ë”© ìºì‹œ ì´ˆê¸°í™” - ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
        "    global _embedding_cache\n",
        "    \n",
        "    for chunk_start in tqdm(range(0, total_rows, chunk_size), desc=\"Processing chunks\"):\n",
        "        chunk_end = min(chunk_start + chunk_size, total_rows)\n",
        "        chunk_df = df.iloc[chunk_start:chunk_end]\n",
        "        \n",
        "        # ì²­í¬ë³„ ì„ë² ë”© ìºì‹œ ê´€ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)\n",
        "        if len(_embedding_cache) > 10000:  # ìºì‹œê°€ ë„ˆë¬´ í¬ë©´\n",
        "            _embedding_cache = {}  # ì´ˆê¸°í™”\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        # ì²­í¬ ë‚´ í–‰ ì²˜ë¦¬\n",
        "        for idx, row in enumerate(chunk_df.itertuples(index=False), start=chunk_start+1):\n",
        "            src_text = str(getattr(row, 'ì›ë¬¸', '') or '')\n",
        "            tgt_text = str(getattr(row, 'ë²ˆì—­ë¬¸', '') or '')\n",
        "            if verbose:\n",
        "                print(f\"\\n[========= ROW {idx} =========]\")\n",
        "                print(\"Source (input):\", src_text)\n",
        "                print(\"Target (input):\", tgt_text)\n",
        "\n",
        "            try:\n",
        "                masked_src, src_masks = mask_brackets(src_text, text_type=\"source\")\n",
        "                masked_tgt, tgt_masks = mask_brackets(tgt_text, text_type=\"target\")\n",
        "\n",
        "                src_units = split_src_meaning_units(masked_src)\n",
        "                tgt_units = split_tgt_meaning_units(masked_src, masked_tgt, use_semantic=True, min_tokens=1)\n",
        "\n",
        "                restored_src_units = [restore_masks(unit, src_masks) for unit in src_units]\n",
        "                restored_tgt_units = [restore_masks(unit, tgt_masks) for unit in tgt_units]\n",
        "\n",
        "                aligned_pairs = align_src_tgt(restored_src_units, restored_tgt_units, compute_embeddings_with_cache)\n",
        "                aligned_src_units, aligned_tgt_units = zip(*aligned_pairs)\n",
        "\n",
        "                if verbose:\n",
        "                    print(\"Alignment result: (source to target comparison)\")\n",
        "                    for src_gu, tgt_gu in zip(aligned_src_units, aligned_tgt_units):\n",
        "                        print(f\"SRC: {src_gu} | TGT: {tgt_gu}\")\n",
        "\n",
        "                for gu_idx, (src_gu, tgt_gu) in enumerate(zip(aligned_src_units, aligned_tgt_units), start=1):\n",
        "                    outputs.append({\n",
        "                        \"ë¬¸ì¥ì‹ë³„ì\": idx,\n",
        "                        \"êµ¬ì‹ë³„ì\": gu_idx,\n",
        "                        \"ì›ë¬¸êµ¬\": src_gu,\n",
        "                        \"ë²ˆì—­êµ¬\": tgt_gu,\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"[IO] Failed to process row {idx}: {e}\")\n",
        "                outputs.append({\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx,\n",
        "                    \"êµ¬ì‹ë³„ì\": 1,\n",
        "                    \"ì›ë¬¸êµ¬\": src_text,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "                })\n",
        "\n",
        "    try:\n",
        "        output_df = pd.DataFrame(outputs, columns=[\"ë¬¸ì¥ì‹ë³„ì\", \"êµ¬ì‹ë³„ì\", \"ì›ë¬¸êµ¬\", \"ë²ˆì—­êµ¬\"])\n",
        "        output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "        if verbose:\n",
        "            logger.info(f\"[IO] Results saved successfully: {output_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"[IO] Failed to save results: {e}\")\n",
        "        \n",
        "\n",
        "def process_file_parallel(input_path: str, output_path: str, num_workers: int = None, chunk_size: int = 20, gpu_strategy: str = \"single\") -> None:\n",
        "    \"\"\"\n",
        "    ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•œ íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ - GPU ìµœì í™” ë²„ì „\n",
        "    \n",
        "    Args:\n",
        "        input_path: ì…ë ¥ Excel íŒŒì¼ ê²½ë¡œ\n",
        "        output_path: ì¶œë ¥ Excel íŒŒì¼ ê²½ë¡œ\n",
        "        num_workers: ë³‘ë ¬ ì²˜ë¦¬ì— ì‚¬ìš©í•  ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜ì— ë”°ë¼ ìë™ ì„¤ì •)\n",
        "        chunk_size: ê° ì›Œì»¤ì—ê²Œ í• ë‹¹í•  í–‰ ìˆ˜\n",
        "        gpu_strategy: GPU í™œìš© ì „ëµ \n",
        "            - \"single\": ì²« ë²ˆì§¸ ì›Œì»¤ë§Œ GPU ì‚¬ìš© (ì•ˆì „)\n",
        "            - \"shared\": ëª¨ë“  ì›Œì»¤ê°€ GPU ê³µìœ  (ê³ ì„±ëŠ¥ GPU í•„ìš”)\n",
        "            - \"multi\": ì—¬ëŸ¬ GPUì— ë¶„ì‚° (ë‹¤ì¤‘ GPU í™˜ê²½)\n",
        "            - \"none\": GPU ì‚¬ìš© ì•ˆ í•¨ (CPUë§Œ ì‚¬ìš©)\n",
        "    \"\"\"\n",
        "    import concurrent.futures\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    import torch\n",
        "    from tqdm import tqdm\n",
        "    import logging\n",
        "    \n",
        "    # ë¡œê±° ì‚¬ìš© (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ ë¡œê±° ì‚¬ìš©)\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    try:\n",
        "        # íŒŒì¼ ì½ê¸° (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹)\n",
        "        try:\n",
        "            df = pd.read_excel(input_path, engine='openpyxl')\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[IO] Failed to read Excel file: {e}\")\n",
        "            return\n",
        "\n",
        "        if 'ì›ë¬¸' not in df.columns or 'ë²ˆì—­ë¬¸' not in df.columns:\n",
        "            logger.error(\"[IO] Missing 'ì›ë¬¸' or 'ë²ˆì—­ë¬¸' columns.\")\n",
        "            return\n",
        "        \n",
        "        # ì›Œì»¤ ìˆ˜ ê²°ì •\n",
        "        if num_workers is None:\n",
        "            cpu_cores = os.cpu_count() or 4\n",
        "            if gpu_strategy == \"multi\" and torch.cuda.is_available():\n",
        "                # ë©€í‹° GPU í™˜ê²½ì—ì„œëŠ” GPU ìˆ˜ì— ë§ê²Œ ì¡°ì •\n",
        "                gpu_count = torch.cuda.device_count()\n",
        "                num_workers = max(min(cpu_cores, gpu_count * 2 if gpu_count > 0 else 4), 1)\n",
        "            else:\n",
        "                # ì¼ë°˜ í™˜ê²½ì—ì„œëŠ” CPU ì½”ì–´ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •\n",
        "                num_workers = min(cpu_cores, 4)\n",
        "        \n",
        "        # GPU ì„¤ì • ì¤€ë¹„\n",
        "        gpu_count = 0\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_count = torch.cuda.device_count()\n",
        "            logger.info(f\"Available GPUs: {gpu_count}\")\n",
        "            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            logger.info(\"No GPU available, using CPU only\")\n",
        "            if gpu_strategy != \"none\":\n",
        "                logger.warning(\"GPU strategy requested but no GPU available, falling back to CPU\")\n",
        "                gpu_strategy = \"none\"\n",
        "        \n",
        "        # ë°ì´í„° ë¶„í• \n",
        "        total_rows = len(df)\n",
        "        row_indices = list(range(1, total_rows + 1))  # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ìŠ¤\n",
        "        \n",
        "        # ë°ì´í„° ì¤€ë¹„ - ì‘ì—…ëŸ‰ì´ ê· ë“±í•˜ë„ë¡ ë¶„ë°°\n",
        "        data_chunks = []\n",
        "        for i in range(0, total_rows, chunk_size):\n",
        "            chunk_indices = row_indices[i:i + chunk_size]\n",
        "            chunk_data = []\n",
        "            for idx in chunk_indices:\n",
        "                row_idx = idx - 1  # 0-based ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
        "                if row_idx < len(df):  # ì¸ë±ìŠ¤ ë²”ìœ„ ì²´í¬\n",
        "                    row = df.iloc[row_idx]\n",
        "                    # None ì²´í¬ ë° ë¬¸ìì—´ ë³€í™˜\n",
        "                    src_text = str(row['ì›ë¬¸']) if pd.notnull(row['ì›ë¬¸']) else \"\"\n",
        "                    tgt_text = str(row['ë²ˆì—­ë¬¸']) if pd.notnull(row['ë²ˆì—­ë¬¸']) else \"\"\n",
        "                    \n",
        "                    # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
        "                    text_length = len(src_text) + len(tgt_text)\n",
        "                    chunk_data.append((idx, src_text, tgt_text, text_length))\n",
        "            \n",
        "            if chunk_data:  # ë¹ˆ ì²­í¬ ë°©ì§€\n",
        "                data_chunks.append(chunk_data)\n",
        "        \n",
        "        # ì²­í¬ë³„ ì‘ì—…ëŸ‰ ê³„ì‚° ë° í…ìŠ¤íŠ¸ ê¸¸ì´ ì •ë³´ ì œê±°\n",
        "        processed_chunks = []\n",
        "        for chunk in data_chunks:\n",
        "            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì •ë³´ëŠ” ì œê±°í•˜ê³  ì›ë˜ ë°ì´í„°ë§Œ ìœ ì§€\n",
        "            processed_chunks.append([(item[0], item[1], item[2]) for item in chunk])\n",
        "        \n",
        "        # ì‘ì—…ëŸ‰ ê¸°ì¤€ìœ¼ë¡œ ì²­í¬ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)\n",
        "        chunks_with_size = [(i, sum(len(item[1])+len(item[2]) for item in chunk)) for i, chunk in enumerate(data_chunks)]\n",
        "        chunks_with_size.sort(key=lambda x: x[1], reverse=True)\n",
        "        sorted_chunk_indices = [i for i, _ in chunks_with_size]\n",
        "        \n",
        "        # GPU í• ë‹¹ ì „ëµ ì„¤ì •\n",
        "        if gpu_strategy == \"single\" and gpu_count > 0:\n",
        "            # ì²« ë²ˆì§¸ ì›Œì»¤ë§Œ GPU ì‚¬ìš©\n",
        "            gpu_assignments = [0 if i == 0 else -1 for i in range(num_workers)]\n",
        "        elif gpu_strategy == \"shared\" and gpu_count > 0:\n",
        "            # ëª¨ë“  ì›Œì»¤ê°€ ì²« ë²ˆì§¸ GPU ê³µìœ \n",
        "            gpu_assignments = [0 for _ in range(num_workers)]\n",
        "        elif gpu_strategy == \"multi\" and gpu_count > 1:\n",
        "            # ë‹¤ì¤‘ GPUì— ì›Œì»¤ ë¶„ì‚°\n",
        "            gpu_assignments = [i % gpu_count for i in range(num_workers)]\n",
        "        else:\n",
        "            # GPU ì‚¬ìš© ì•ˆ í•¨\n",
        "            gpu_assignments = [-1 for _ in range(num_workers)]\n",
        "        \n",
        "        logger.info(f\"Starting parallel processing with {num_workers} workers, {len(processed_chunks)} chunks\")\n",
        "        logger.info(f\"GPU strategy: {gpu_strategy}, GPU assignments: {gpu_assignments}\")\n",
        "        \n",
        "        # ì²­í¬ì™€ ì›Œì»¤ ë§¤í•‘ ìµœì í™” (ê¸´ í…ìŠ¤íŠ¸ = ê°•ë ¥í•œ GPUì— í• ë‹¹)\n",
        "        chunk_worker_assignments = {}\n",
        "        for i, chunk_idx in enumerate(sorted_chunk_indices):\n",
        "            worker_idx = i % num_workers\n",
        "            chunk_worker_assignments[chunk_idx] = worker_idx\n",
        "        \n",
        "        # ë³‘ë ¬ ì‹¤í–‰\n",
        "        all_results = []\n",
        "        with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "            future_to_chunk = {}\n",
        "            \n",
        "            for chunk_idx, chunk_data in enumerate(processed_chunks):\n",
        "                worker_idx = chunk_worker_assignments.get(chunk_idx, chunk_idx % num_workers)\n",
        "                gpu_idx = gpu_assignments[worker_idx]\n",
        "                \n",
        "                # ì›Œì»¤ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì •\n",
        "                memory_fraction = 0.8 if gpu_strategy == \"shared\" else 0.95\n",
        "                \n",
        "                future = executor.submit(\n",
        "                    process_chunk_safe,  # ì•ˆì „í•œ ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜ ì‚¬ìš©\n",
        "                    chunk_data,\n",
        "                    gpu_idx=gpu_idx,\n",
        "                    memory_fraction=memory_fraction\n",
        "                )\n",
        "                future_to_chunk[future] = chunk_idx\n",
        "            \n",
        "            # ê²°ê³¼ ìˆ˜ì§‘\n",
        "            for future in tqdm(concurrent.futures.as_completed(future_to_chunk), \n",
        "                              total=len(future_to_chunk), \n",
        "                              desc=\"Processing chunks\"):\n",
        "                chunk_idx = future_to_chunk[future]\n",
        "                try:\n",
        "                    chunk_results = future.result()\n",
        "                    if isinstance(chunk_results, list):  # íƒ€ì… ì²´í¬ ì¶”ê°€\n",
        "                        all_results.extend(chunk_results)\n",
        "                    else:\n",
        "                        logger.error(f\"Invalid result type from chunk {chunk_idx}: {type(chunk_results)}\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing chunk {chunk_idx}: {e}\")\n",
        "        \n",
        "        # ê²°ê³¼ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸\n",
        "        if not all_results:\n",
        "            logger.error(\"No results were produced by parallel processing\")\n",
        "            return\n",
        "            \n",
        "        # ê²°ê³¼ ì •ë ¬ (ë¬¸ì¥ì‹ë³„ì, êµ¬ì‹ë³„ì ìˆœ)\n",
        "        all_results.sort(key=lambda x: (x['ë¬¸ì¥ì‹ë³„ì'], x['êµ¬ì‹ë³„ì']))\n",
        "        \n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        try:\n",
        "            output_df = pd.DataFrame(all_results, columns=[\"ë¬¸ì¥ì‹ë³„ì\", \"êµ¬ì‹ë³„ì\", \"ì›ë¬¸êµ¬\", \"ë²ˆì—­êµ¬\"])\n",
        "            output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "            logger.info(f\"[IO] Results saved successfully: {output_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"[IO] Failed to save results: {e}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Parallel processing failed: {e}\")\n",
        "        raise\n",
        "    \n",
        "\n",
        "def process_chunk_safe(chunk_data, gpu_idx=-1, memory_fraction=0.95):\n",
        "    \"\"\"\n",
        "    ì˜ˆì™¸ ì²˜ë¦¬ ê¸°ëŠ¥ì´ ê°•í™”ëœ ì•ˆì „í•œ ì²­í¬ ì²˜ë¦¬ ë˜í¼ í•¨ìˆ˜\n",
        "    \n",
        "    Args:\n",
        "        chunk_data: (ì¸ë±ìŠ¤, ì›ë¬¸, ë²ˆì—­ë¬¸) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸\n",
        "        gpu_idx: ì‚¬ìš©í•  GPU ì¸ë±ìŠ¤ (-1: CPUë§Œ ì‚¬ìš©)\n",
        "        memory_fraction: GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨ (0.0 ~ 1.0)\n",
        "    \n",
        "    Returns:\n",
        "        ì²˜ë¦¬ëœ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    import logging\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    try:\n",
        "        # ì‹¤ì œ ì²˜ë¦¬ í•¨ìˆ˜ í˜¸ì¶œ\n",
        "        return process_chunk(chunk_data, gpu_idx, memory_fraction)\n",
        "    except Exception as e:\n",
        "        # ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì²˜ë¦¬\n",
        "        logger.error(f\"Critical error in process_chunk: {e}\")\n",
        "        \n",
        "        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ë§Œ ê²°ê³¼ë¡œ ë°˜í™˜\n",
        "        fallback_results = []\n",
        "        try:\n",
        "            for idx, src_text, tgt_text in chunk_data:\n",
        "                fallback_results.append({\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx,\n",
        "                    \"êµ¬ì‹ë³„ì\": 1,\n",
        "                    \"ì›ë¬¸êµ¬\": src_text,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "                })\n",
        "        except:\n",
        "            logger.error(\"Failed to create fallback results\")\n",
        "        \n",
        "        return fallback_results\n",
        "\n",
        "\n",
        "def process_chunk(chunk_data, gpu_idx=-1, memory_fraction=0.95):\n",
        "    \"\"\"\n",
        "    í•˜ë‚˜ì˜ ë°ì´í„° ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜ - GPU ìµœì í™” ë²„ì „\n",
        "    \n",
        "    Args:\n",
        "        chunk_data: (ì¸ë±ìŠ¤, ì›ë¬¸, ë²ˆì—­ë¬¸) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸\n",
        "        gpu_idx: ì‚¬ìš©í•  GPU ì¸ë±ìŠ¤ (-1: CPUë§Œ ì‚¬ìš©)\n",
        "        memory_fraction: GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨ (0.0 ~ 1.0)\n",
        "    \n",
        "    Returns:\n",
        "        ì²˜ë¦¬ëœ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import torch\n",
        "    import logging\n",
        "    import gc\n",
        "    \n",
        "    # ë¡œê±° ì´ˆê¸°í™” (í”„ë¡œì„¸ìŠ¤ë³„)\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    # ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸í•˜ê³  í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸\n",
        "    required_modules = {}\n",
        "    \n",
        "    try:\n",
        "        from colbert.modeling.checkpoint import Checkpoint\n",
        "        required_modules['colbert'] = True\n",
        "    except ImportError:\n",
        "        logger.error(\"colbert ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        required_modules['colbert'] = False\n",
        "    \n",
        "    # ë§ˆìŠ¤í‚¹ ë° ë¶„í•  í•¨ìˆ˜ ì‚¬ìš© ê°€ëŠ¥ì„± í™•ì¸\n",
        "    try:\n",
        "        from your_module import mask_brackets, split_src_meaning_units, split_tgt_meaning_units, restore_masks, align_src_tgt, compute_embeddings_with_cache\n",
        "        required_modules['custom_functions'] = True\n",
        "    except ImportError:\n",
        "        # ëª¨ë“ˆ ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì „ì—­ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œ ì°¾ì•„ë´„\n",
        "        functions_available = True\n",
        "        for func_name in ['mask_brackets', 'split_src_meaning_units', 'split_tgt_meaning_units', 'restore_masks', 'align_src_tgt', 'compute_embeddings_with_cache']:\n",
        "            if func_name not in globals():\n",
        "                logger.error(f\"í•„ìˆ˜ í•¨ìˆ˜ {func_name}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "                functions_available = False\n",
        "        required_modules['custom_functions'] = functions_available\n",
        "    \n",
        "    # í•„ìˆ˜ ëª¨ë“ˆ/í•¨ìˆ˜ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì²˜ë¦¬ë§Œ ìˆ˜í–‰\n",
        "    if not all(required_modules.values()):\n",
        "        logger.error(\"í•„ìˆ˜ ëª¨ë“ˆ ë˜ëŠ” í•¨ìˆ˜ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\")\n",
        "        return [{\"ë¬¸ì¥ì‹ë³„ì\": idx, \"êµ¬ì‹ë³„ì\": 1, \"ì›ë¬¸êµ¬\": src, \"ë²ˆì—­êµ¬\": tgt} for idx, src, tgt in chunk_data]\n",
        "    \n",
        "    # í”„ë¡œì„¸ìŠ¤ë³„ GPU ì„¤ì •\n",
        "    if gpu_idx < 0:\n",
        "        # CPU ëª¨ë“œ\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "        logger.info(f\"Worker using CPU mode\")\n",
        "    else:\n",
        "        # GPU ëª¨ë“œ - íŠ¹ì • GPU ì§€ì •\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_idx)\n",
        "        logger.info(f\"Worker using GPU {gpu_idx} with memory fraction {memory_fraction}\")\n",
        "        \n",
        "        # GPU ë©”ëª¨ë¦¬ ì„¤ì •\n",
        "        if torch.cuda.is_available():\n",
        "            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ \n",
        "            torch.cuda.empty_cache()\n",
        "            try:\n",
        "                torch.cuda.set_per_process_memory_fraction(memory_fraction, 0)\n",
        "            except (AttributeError, RuntimeError) as e:\n",
        "                logger.warning(f\"GPU ë©”ëª¨ë¦¬ ì„¤ì • ì‹¤íŒ¨: {e}\")\n",
        "                # ëŒ€ì²´ ë©”ëª¨ë¦¬ ê´€ë¦¬\n",
        "                gc.collect()\n",
        "    \n",
        "    # ëª¨ë¸ ë¡œë“œ ìµœì í™” (í”„ë¡œì„¸ìŠ¤ë³„ë¡œ ìƒˆë¡œ ë¡œë“œ)\n",
        "    global model, _embedding_cache\n",
        "    \n",
        "    # ì„ë² ë”© ìºì‹œ ì´ˆê¸°í™”\n",
        "    _embedding_cache = {}\n",
        "    \n",
        "    try:\n",
        "        # ëª¨ë¸ ë¡œë“œ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸\n",
        "        model_path = './model/original_colbert_bm25_dense_hybrid_for_pairmining_checkpoint_512x8.torch'\n",
        "        if not os.path.exists(model_path):\n",
        "            logger.error(f\"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}\")\n",
        "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "            \n",
        "        # ëª¨ë¸ ë¡œë“œ\n",
        "        from colbert.modeling.checkpoint import Checkpoint\n",
        "        model = Checkpoint.load(model_path)\n",
        "        model.query_tokenizer.query_maxlen = 512\n",
        "        model.doc_tokenizer.doc_maxlen = 512\n",
        "        \n",
        "        # GPU ë©”ëª¨ë¦¬ ìµœì í™”\n",
        "        if gpu_idx >= 0 and torch.cuda.is_available():\n",
        "            try:\n",
        "                # í˜¼í•© ì •ë°€ë„ ì‚¬ìš© (Float16)\n",
        "                model.half()  # Float16 ë³€í™˜ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ\n",
        "                model.cuda()\n",
        "                # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ì¶”ë¡  ëª¨ë“œ)\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for param in model.parameters():\n",
        "                        param.requires_grad = False\n",
        "            except RuntimeError as e:\n",
        "                logger.error(f\"GPU ëª¨ë“œ ì„¤ì • ì˜¤ë¥˜: {e}\")\n",
        "                # GPU ì˜¤ë¥˜ ì‹œ CPUë¡œ í´ë°±\n",
        "                model.cpu()\n",
        "                model.eval()\n",
        "        else:\n",
        "            # CPU ëª¨ë“œ\n",
        "            model.cpu()\n",
        "            model.eval()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
        "        # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜\n",
        "        return [{\"ë¬¸ì¥ì‹ë³„ì\": idx, \"êµ¬ì‹ë³„ì\": 1, \"ì›ë¬¸êµ¬\": src, \"ë²ˆì—­êµ¬\": tgt} for idx, src, tgt in chunk_data]\n",
        "    \n",
        "    # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
        "    chunk_results = []\n",
        "    \n",
        "    # ì²­í¬ ë‚´ ê° í–‰ ì²˜ë¦¬\n",
        "    for idx, src_text, tgt_text in chunk_data:\n",
        "        try:\n",
        "            # ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬\n",
        "            if not src_text or not tgt_text:\n",
        "                chunk_results.append({\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx,\n",
        "                    \"êµ¬ì‹ë³„ì\": 1,\n",
        "                    \"ì›ë¬¸êµ¬\": src_text,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "                \n",
        "            # ê´„í˜¸ ë§ˆìŠ¤í‚¹\n",
        "            masked_src, src_masks = mask_brackets(src_text, text_type=\"source\")\n",
        "            masked_tgt, tgt_masks = mask_brackets(tgt_text, text_type=\"target\")\n",
        "            \n",
        "            # ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• \n",
        "            src_units = split_src_meaning_units(masked_src)\n",
        "            tgt_units = split_tgt_meaning_units(masked_src, masked_tgt, use_semantic=True, min_tokens=1)\n",
        "            \n",
        "            # ë¶„í•  ê²°ê³¼ ì²´í¬\n",
        "            if not src_units or not tgt_units:\n",
        "                logger.warning(f\"í–‰ {idx}: ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
        "                chunk_results.append({\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx,\n",
        "                    \"êµ¬ì‹ë³„ì\": 1,\n",
        "                    \"ì›ë¬¸êµ¬\": src_text,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "            \n",
        "            # ë§ˆìŠ¤í¬ ë³µì›\n",
        "            restored_src_units = [restore_masks(unit, src_masks) for unit in src_units]\n",
        "            restored_tgt_units = [restore_masks(unit, tgt_masks) for unit in tgt_units]\n",
        "            \n",
        "            # ì •ë ¬\n",
        "            try:\n",
        "                # compute_embeddings_with_cache í•¨ìˆ˜ëŠ” _embedding_cacheë¥¼ ì°¸ì¡°\n",
        "                aligned_pairs = align_src_tgt(restored_src_units, restored_tgt_units, compute_embeddings_with_cache)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"í–‰ {idx} ì •ë ¬ ì˜¤ë¥˜: {e}\")\n",
        "                # ì •ë ¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©\n",
        "                chunk_results.append({\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx,\n",
        "                    \"êµ¬ì‹ë³„ì\": 1,\n",
        "                    \"ì›ë¬¸êµ¬\": src_text,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "            \n",
        "            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ë¡œ ëŒ€ì²´\n",
        "            if not aligned_pairs:\n",
        "                chunk_results.append({\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx,\n",
        "                    \"êµ¬ì‹ë³„ì\": 1,\n",
        "                    \"ì›ë¬¸êµ¬\": src_text,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "                })\n",
        "                continue\n",
        "                \n",
        "            # ì •ë ¬ ê²°ê³¼ ì–¸íŒ¨í‚¹\n",
        "            aligned_src_units, aligned_tgt_units = zip(*aligned_pairs)\n",
        "            \n",
        "            # ê²°ê³¼ ì €ì¥\n",
        "            for gu_idx, (src_gu, tgt_gu) in enumerate(zip(aligned_src_units, aligned_tgt_units), start=1):\n",
        "                # ë¹ˆ ê²°ê³¼ ë°©ì§€\n",
        "                if not src_gu.strip() or not tgt_gu.strip():\n",
        "                    continue\n",
        "                    \n",
        "                chunk_results.append({\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx,\n",
        "                    \"êµ¬ì‹ë³„ì\": gu_idx,\n",
        "                    \"ì›ë¬¸êµ¬\": src_gu,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_gu,\n",
        "                })\n",
        "            \n",
        "            # ê²°ê³¼ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©\n",
        "            if not any(r[\"ë¬¸ì¥ì‹ë³„ì\"] == idx for r in chunk_results):\n",
        "                chunk_results.append({\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx,\n",
        "                    \"êµ¬ì‹ë³„ì\": 1,\n",
        "                    \"ì›ë¬¸êµ¬\": src_text,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "                })\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"í–‰ {idx} ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
        "            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì €ì¥\n",
        "            chunk_results.append({\n",
        "                \"ë¬¸ì¥ì‹ë³„ì\": idx,\n",
        "                \"êµ¬ì‹ë³„ì\": 1,\n",
        "                \"ì›ë¬¸êµ¬\": src_text,\n",
        "                \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "            })\n",
        "    \n",
        "    # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹œ ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "    try:\n",
        "        # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "        _embedding_cache.clear()\n",
        "        del model\n",
        "        gc.collect()\n",
        "        \n",
        "        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
        "        if gpu_idx >= 0 and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    return chunk_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (main.py)\n",
        "\n",
        "íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def main(\n",
        "    input_path: str,\n",
        "    output_path: str,\n",
        "    use_parallel: bool = False,\n",
        "    num_workers: int = 4,\n",
        "    chunk_size: int = 50,\n",
        "    gpu_memory_fraction: float = 0.8,\n",
        "    verbose: bool = False\n",
        "):\n",
        "    \"\"\"ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
        "    \n",
        "    # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ logger ì •ì˜\n",
        "    logger = logging.getLogger(__name__)\n",
        "    \n",
        "    if verbose:\n",
        "        logging.getLogger().setLevel(logging.DEBUG)\n",
        "        logger.info(\"Verbose mode activated: DEBUG level logging enabled.\")\n",
        "    \n",
        "    # Log parallel processing options if enabled\n",
        "    if use_parallel:\n",
        "        logger.info(f\"ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ: ì›Œì»¤ {num_workers}ê°œ, ì²­í¬ í¬ê¸° {chunk_size}\")\n",
        "    else:\n",
        "        logger.info(\"ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ëª¨ë“œ\")\n",
        "    \n",
        "    # íŒŒì¼ ê²½ë¡œ ê²€ì¦\n",
        "    if not os.path.exists(input_path):\n",
        "        logger.error(f\"ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_path}\")\n",
        "        return False\n",
        "    \n",
        "    logger.info(f\"ì…ë ¥ íŒŒì¼: {input_path}\")\n",
        "    logger.info(f\"ì¶œë ¥ íŒŒì¼: {output_path}\")\n",
        "    \n",
        "    try:\n",
        "        # ë°ì´í„° ë¡œë“œ\n",
        "        df = pd.read_excel(input_path, engine='openpyxl')\n",
        "        logger.info(f\"ë¡œë“œëœ ë°ì´í„°: {len(df)}í–‰\")\n",
        "        \n",
        "        if 'ì›ë¬¸' not in df.columns or 'ë²ˆì—­ë¬¸' not in df.columns:\n",
        "            logger.error(\"í•„ìˆ˜ ì»¬ëŸ¼('ì›ë¬¸', 'ë²ˆì—­ë¬¸')ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            return False\n",
        "        \n",
        "        # ì²˜ë¦¬ ê²°ê³¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "        all_results = []\n",
        "        \n",
        "        # ê° í–‰ ì²˜ë¦¬\n",
        "        for idx, row in df.iterrows():\n",
        "            src_text = str(row.get('ì›ë¬¸', ''))\n",
        "            tgt_text = str(row.get('ë²ˆì—­ë¬¸', ''))\n",
        "            \n",
        "            if verbose:\n",
        "                logger.info(f\"ì²˜ë¦¬ ì¤‘: í–‰ {idx+1}\")\n",
        "                logger.info(f\"ì›ë¬¸: {src_text}\")\n",
        "                logger.info(f\"ë²ˆì—­ë¬¸: {tgt_text}\")\n",
        "            \n",
        "            try:\n",
        "                # ê°„ë‹¨í•œ ì²˜ë¦¬ (ì‹¤ì œ íŒŒì´í”„ë¼ì¸ ëŒ€ì‹ )\n",
        "                result = {\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx + 1,\n",
        "                    \"êµ¬ì‹ë³„ì\": 1,\n",
        "                    \"ì›ë¬¸êµ¬\": src_text,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "                }\n",
        "                all_results.append(result)\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"í–‰ {idx+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "                result = {\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx + 1,\n",
        "                    \"êµ¬ì‹ë³„ì\": 1,\n",
        "                    \"ì›ë¬¸êµ¬\": src_text,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "                }\n",
        "                all_results.append(result)\n",
        "        \n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        output_df = pd.DataFrame(all_results)\n",
        "        output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "        logger.info(f\"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
        "        logger.info(f\"ì´ ì²˜ë¦¬ ê²°ê³¼: {len(all_results)}ê°œ\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "print(\"âœ… main í•¨ìˆ˜ê°€ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
        "test_input = \"test_input.xlsx\"\n",
        "test_output = \"test_output.xlsx\"\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\n",
        "test_data = [\n",
        "    {\"ì›ë¬¸\": \"ä½œè©è¨“å‚³æ™‚ì— ç§»å…¶ç¯‡ç¬¬í•˜ê³  å› æ”¹ä¹‹è€³ë¼\", \"ë²ˆì—­ë¬¸\": \"ì£¼ì„ê³¼ í•´ì„¤ì„ ì‘ì„±í•  ë•Œì— ê·¸ í¸ê³¼ ì¥ì„ ì˜®ê¸°ê³  ê·¸ì— ë”°ë¼ ê³ ì³¤ì„ ë¿ì´ë‹¤.\"},\n",
        "    {\"ì›ë¬¸\": \"å¤ä¾†ç›¸å‚³í•˜ì•¼ å­¸è€…ê°€ æ–¼å…¶èªªì— æœªå˜—è‡´ç–‘í•˜ë‹ˆë¼\", \"ë²ˆì—­ë¬¸\": \"ì˜ˆë¡œë¶€í„° ì„œë¡œ ì „í•´ì ¸ í•™ìë“¤ì€ ê·¸ ì„¤ì— ëŒ€í•´ ì˜ì‹¬ì„ í’ˆì€ ì ì´ ì—†ì—ˆë‹¤.\"}\n",
        "]\n",
        "\n",
        "df_test = pd.DataFrame(test_data)\n",
        "df_test.to_excel(test_input, index=False, engine='openpyxl')\n",
        "logger.info(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: {test_input}\")\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
        "success = main(test_input, test_output, verbose=True)\n",
        "\n",
        "# ê²°ê³¼ í™•ì¸\n",
        "if success:\n",
        "    try:\n",
        "        result_df = pd.read_excel(test_output)\n",
        "        logger.info(\"âœ… ì²˜ë¦¬ ê²°ê³¼:\")\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(result_df.to_string(index=False))\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ê²°ê³¼ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
        "else:\n",
        "    logger.error(\"âŒ ì²˜ë¦¬ ì‹¤íŒ¨\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. í…ŒìŠ¤íŠ¸\n",
        "\n",
        "ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ê°„ë‹¨í•œ ì˜ˆì œë¡œ íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing function\n",
        "def create_test_data(file_path=\"test_input.xlsx\"):\n",
        "    \"\"\"Generate test data.\"\"\"\n",
        "    test_data = [\n",
        "        {\n",
        "            \"ì›ë¬¸\": \"ä½œè©è¨“å‚³æ™‚ì— ç§»å…¶ç¯‡ç¬¬í•˜ê³  å› æ”¹ä¹‹è€³ë¼\",\n",
        "            \"ë²ˆì—­ë¬¸\": \"ì£¼ì„ê³¼ í•´ì„¤ì„ ì‘ì„±í•  ë•Œì— ê·¸ í¸ê³¼ ì¥ì„ ì˜®ê¸°ê³  ê·¸ì— ë”°ë¼ ê³ ì³¤ì„ ë¿ì´ë‹¤.\"\n",
        "        },\n",
        "        {\n",
        "            \"ì›ë¬¸\": \"å¤ä¾†ç›¸å‚³í•˜ì•¼ å­¸è€…ê°€ æ–¼å…¶èªªì— æœªå˜—è‡´ç–‘í•˜ë‹ˆë¼\",\n",
        "            \"ë²ˆì—­ë¬¸\": \"ì˜ˆë¡œë¶€í„° ì„œë¡œ ì „í•´ì ¸ í•™ìë“¤ì€ ê·¸ ì„¤ì— ëŒ€í•´ ì˜ì‹¬ì„ í’ˆì€ ì ì´ ì—†ì—ˆë‹¤.\"\n",
        "        },\n",
        "        {\n",
        "            \"ì›ë¬¸\": \"å¤«é›…é Œä¹‹ä½œä¹Ÿ è©©äººå„æœ‰æ‰€å±¬è€…ä¹Ÿ\",\n",
        "            \"ë²ˆì—­ë¬¸\": \"ë¬´ë¦‡ ì•„ì†¡ì˜ ì°½ì‘ì€ ì‹œì¸ë§ˆë‹¤ ê°ì ì†í•œ ë°”ê°€ ìˆì—ˆë‹¤.\"\n",
        "        },\n",
        "        {    \n",
        "            \"ì›ë¬¸\": \"ç„¶è€Œå­”å­å–è€Œæ¬¡ä¹‹è€…í•˜ì•¼ å‰‡æœ‰å®¶åœ‹ä¹‹æ¬¡çŸ£\",\n",
        "            \"ë²ˆì—­ë¬¸\": \"ê·¸ëŸ°ë° ê³µìê°€ ì´ê²ƒì„ ì·¨í•˜ì—¬ ì°¨ë¡€ë¥¼ ë§¤ê¸´ ê²ƒì€ ê°€êµ­ì˜ ìˆœì„œì— ë”°ë¥¸ ê²ƒì´ë‹¤.\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    df = pd.DataFrame(test_data)\n",
        "    df.to_excel(file_path, index=False, engine='openpyxl')\n",
        "    return file_path\n",
        "\n",
        "# Testing execution\n",
        "test_input = create_test_data()\n",
        "test_output = \"test_output.xlsx\"\n",
        "\n",
        "# Run the pipeline\n",
        "main(test_input, test_output, verbose=True)\n",
        "\n",
        "# Check results\n",
        "try:\n",
        "    result_df = pd.read_excel(test_output)\n",
        "    display(result_df)\n",
        "except Exception as e:\n",
        "    print(f\"Failed to read result file: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. ë‹¨ì¼ ë¬¸ì¥ í…ŒìŠ¤íŠ¸\n",
        "\n",
        "íŠ¹ì • ë¬¸ì¥ ìŒì— ëŒ€í•´ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸í•´ë³¼ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from konlpy.tag import Kkma\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "# ============================================================================\n",
        "# 1. ì´ˆê¸° ì„¤ì •\n",
        "# ============================================================================\n",
        "\n",
        "# ë¡œê±° ì„¤ì •\n",
        "logging.basicConfig(\n",
        "    format=\"[%(levelname)s] %(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
        "os.environ['PYTORCH_DISABLE_TORCH_LOAD_SECURITY_CHECK'] = '1'\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ëª¨ë¸ ë¡œë”©\n",
        "# ============================================================================\n",
        "\n",
        "def load_embedding_model():\n",
        "    \"\"\"ì—¬ëŸ¬ ì˜µì…˜ì„ ì‹œë„í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\"\"\"\n",
        "    \n",
        "    # Option 1: BGE ëª¨ë¸\n",
        "    try:\n",
        "        from FlagEmbedding import BGEM3FlagModel\n",
        "        model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
        "        logger.info(\"âœ… BGE model loaded successfully\")\n",
        "        return model, 'bge'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"âŒ BGE model failed: {e}\")\n",
        "    \n",
        "    # Option 2: SentenceTransformers\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        logger.info(\"âœ… SentenceTransformer model loaded successfully\")\n",
        "        return model, 'sentence_transformer'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"âŒ SentenceTransformer failed: {e}\")\n",
        "    \n",
        "    # Option 3: ë”ë¯¸ ëª¨ë¸\n",
        "    logger.info(\"ğŸ”„ Using dummy embeddings\")\n",
        "    return None, 'dummy'\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "model, model_type = load_embedding_model()\n",
        "\n",
        "# ============================================================================\n",
        "# 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤\n",
        "# ============================================================================\n",
        "\n",
        "def compute_embeddings(texts, model=model, model_type=model_type):\n",
        "    \"\"\"ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì„ë² ë”© ê³„ì‚°\"\"\"\n",
        "    if model_type == 'bge':\n",
        "        output = model.encode(texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
        "        return output['dense_vecs']\n",
        "    elif model_type == 'sentence_transformer':\n",
        "        return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    else:  # dummy\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            import hashlib\n",
        "            text_hash = hashlib.md5(str(text).encode()).hexdigest()\n",
        "            seed = int(text_hash[:8], 16) % (2**31)\n",
        "            np.random.seed(seed)\n",
        "            dummy_emb = np.random.randn(384).astype(np.float32)\n",
        "            dummy_emb = dummy_emb / (np.linalg.norm(dummy_emb) + 1e-8)\n",
        "            embeddings.append(dummy_emb)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "# ============================================================================\n",
        "# 4. ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from konlpy.tag import Kkma\n",
        "from typing import List, Tuple, Dict, Any, Optional\n",
        "\n",
        "# ============================================================================\n",
        "# 1. ì´ˆê¸° ì„¤ì •\n",
        "# ============================================================================\n",
        "\n",
        "# ë¡œê±° ì„¤ì •\n",
        "logging.basicConfig(\n",
        "    format=\"[%(levelname)s] %(asctime)s - %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
        "os.environ['PYTORCH_DISABLE_TORCH_LOAD_SECURITY_CHECK'] = '1'\n",
        "\n",
        "# ============================================================================\n",
        "# 2. ëª¨ë¸ ë¡œë”©\n",
        "# ============================================================================\n",
        "\n",
        "def load_embedding_model():\n",
        "    \"\"\"ì—¬ëŸ¬ ì˜µì…˜ì„ ì‹œë„í•˜ì—¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\"\"\"\n",
        "    \n",
        "    # Option 1: BGE ëª¨ë¸\n",
        "    try:\n",
        "        from FlagEmbedding import BGEM3FlagModel\n",
        "        model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
        "        logger.info(\"âœ… BGE model loaded successfully\")\n",
        "        return model, 'bge'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"âŒ BGE model failed: {e}\")\n",
        "    \n",
        "    # Option 2: SentenceTransformers\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        logger.info(\"âœ… SentenceTransformer model loaded successfully\")\n",
        "        return model, 'sentence_transformer'\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"âŒ SentenceTransformer failed: {e}\")\n",
        "    \n",
        "    # Option 3: ë”ë¯¸ ëª¨ë¸\n",
        "    logger.info(\"ğŸ”„ Using dummy embeddings\")\n",
        "    return None, 'dummy'\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "model, model_type = load_embedding_model()\n",
        "\n",
        "# ============================================================================\n",
        "# 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤\n",
        "# ============================================================================\n",
        "\n",
        "def compute_embeddings(texts, model=model, model_type=model_type):\n",
        "    \"\"\"ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ì„ë² ë”© ê³„ì‚°\"\"\"\n",
        "    if model_type == 'bge':\n",
        "        output = model.encode(texts, return_dense=True, return_sparse=False, return_colbert_vecs=False)\n",
        "        return output['dense_vecs']\n",
        "    elif model_type == 'sentence_transformer':\n",
        "        return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "    else:  # dummy\n",
        "        embeddings = []\n",
        "        for text in texts:\n",
        "            import hashlib\n",
        "            text_hash = hashlib.md5(str(text).encode()).hexdigest()\n",
        "            seed = int(text_hash[:8], 16) % (2**31)\n",
        "            np.random.seed(seed)\n",
        "            dummy_emb = np.random.randn(384).astype(np.float32)\n",
        "            dummy_emb = dummy_emb / (np.linalg.norm(dummy_emb) + 1e-8)\n",
        "            embeddings.append(dummy_emb)\n",
        "        return np.array(embeddings)\n",
        "\n",
        "# ============================================================================\n",
        "# 4. ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# ============================================================================\n",
        "\n",
        "def main(\n",
        "    input_path: str,\n",
        "    output_path: str,\n",
        "    use_parallel: bool = False,\n",
        "    num_workers: int = 4,\n",
        "    chunk_size: int = 50,\n",
        "    gpu_memory_fraction: float = 0.8,\n",
        "    verbose: bool = False\n",
        "):\n",
        "    \"\"\"ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜\"\"\"\n",
        "    \n",
        "    if verbose:\n",
        "        logging.getLogger().setLevel(logging.DEBUG)\n",
        "        logger.info(\"Verbose mode activated: DEBUG level logging enabled.\")\n",
        "    \n",
        "    # íŒŒì¼ ê²½ë¡œ ê²€ì¦\n",
        "    if not os.path.exists(input_path):\n",
        "        logger.error(f\"ì…ë ¥ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {input_path}\")\n",
        "        return False\n",
        "    \n",
        "    logger.info(f\"ì…ë ¥ íŒŒì¼: {input_path}\")\n",
        "    logger.info(f\"ì¶œë ¥ íŒŒì¼: {output_path}\")\n",
        "    \n",
        "    if use_parallel:\n",
        "        logger.info(f\"ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ: ì›Œì»¤ {num_workers}ê°œ, ì²­í¬ í¬ê¸° {chunk_size}\")\n",
        "    else:\n",
        "        logger.info(\"ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ëª¨ë“œ\")\n",
        "    \n",
        "    try:\n",
        "        # ë°ì´í„° ë¡œë“œ\n",
        "        df = pd.read_excel(input_path, engine='openpyxl')\n",
        "        logger.info(f\"ë¡œë“œëœ ë°ì´í„°: {len(df)}í–‰\")\n",
        "        \n",
        "        if 'ì›ë¬¸' not in df.columns or 'ë²ˆì—­ë¬¸' not in df.columns:\n",
        "            logger.error(\"í•„ìˆ˜ ì»¬ëŸ¼('ì›ë¬¸', 'ë²ˆì—­ë¬¸')ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            return False\n",
        "        \n",
        "        # ì²˜ë¦¬ ê²°ê³¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
        "        all_results = []\n",
        "        \n",
        "        # ê° í–‰ ì²˜ë¦¬\n",
        "        for idx, row in df.iterrows():\n",
        "            src_text = str(row.get('ì›ë¬¸', ''))\n",
        "            tgt_text = str(row.get('ë²ˆì—­ë¬¸', ''))\n",
        "            \n",
        "            if verbose:\n",
        "                logger.info(f\"ì²˜ë¦¬ ì¤‘: í–‰ {idx+1}\")\n",
        "                logger.info(f\"ì›ë¬¸: {src_text}\")\n",
        "                logger.info(f\"ë²ˆì—­ë¬¸: {tgt_text}\")\n",
        "            \n",
        "            try:\n",
        "                # ê°„ë‹¨í•œ ì²˜ë¦¬ (ì‹¤ì œ íŒŒì´í”„ë¼ì¸ ëŒ€ì‹ )\n",
        "                result = {\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx + 1,\n",
        "                    \"êµ¬ì‹ë³„ì\": 1,\n",
        "                    \"ì›ë¬¸êµ¬\": src_text,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "                }\n",
        "                all_results.append(result)\n",
        "                \n",
        "            except Exception as e:\n",
        "                logger.error(f\"í–‰ {idx+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
        "                result = {\n",
        "                    \"ë¬¸ì¥ì‹ë³„ì\": idx + 1,\n",
        "                    \"êµ¬ì‹ë³„ì\": 1,\n",
        "                    \"ì›ë¬¸êµ¬\": src_text,\n",
        "                    \"ë²ˆì—­êµ¬\": tgt_text,\n",
        "                }\n",
        "                all_results.append(result)\n",
        "        \n",
        "        # ê²°ê³¼ ì €ì¥\n",
        "        output_df = pd.DataFrame(all_results)\n",
        "        output_df.to_excel(output_path, index=False, engine='openpyxl')\n",
        "        logger.info(f\"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
        "        logger.info(f\"ì´ ì²˜ë¦¬ ê²°ê³¼: {len(all_results)}ê°œ\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# ============================================================================\n",
        "# 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
        "# ============================================================================\n",
        "\n",
        "logger.info(f\"âœ… Setup complete. Using {model_type} embeddings.\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\n",
        "test_input = \"test_input.xlsx\"\n",
        "test_output = \"test_output.xlsx\"\n",
        "\n",
        "test_data = [\n",
        "    {\"ì›ë¬¸\": \"ä½œè©è¨“å‚³æ™‚ì— ç§»å…¶ç¯‡ç¬¬í•˜ê³  å› æ”¹ä¹‹è€³ë¼\", \"ë²ˆì—­ë¬¸\": \"ì£¼ì„ê³¼ í•´ì„¤ì„ ì‘ì„±í•  ë•Œì— ê·¸ í¸ê³¼ ì¥ì„ ì˜®ê¸°ê³  ê·¸ì— ë”°ë¼ ê³ ì³¤ì„ ë¿ì´ë‹¤.\"},\n",
        "    {\"ì›ë¬¸\": \"å¤ä¾†ç›¸å‚³í•˜ì•¼ å­¸è€…ê°€ æ–¼å…¶èªªì— æœªå˜—è‡´ç–‘í•˜ë‹ˆë¼\", \"ë²ˆì—­ë¬¸\": \"ì˜ˆë¡œë¶€í„° ì„œë¡œ ì „í•´ì ¸ í•™ìë“¤ì€ ê·¸ ì„¤ì— ëŒ€í•´ ì˜ì‹¬ì„ í’ˆì€ ì ì´ ì—†ì—ˆë‹¤.\"}\n",
        "]\n",
        "\n",
        "df_test = pd.DataFrame(test_data)\n",
        "df_test.to_excel(test_input, index=False, engine='openpyxl')\n",
        "logger.info(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: {test_input}\")\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
        "success = main(test_input, test_output, verbose=True)\n",
        "\n",
        "# ê²°ê³¼ í™•ì¸\n",
        "if success:\n",
        "    try:\n",
        "        result_df = pd.read_excel(test_output)\n",
        "        logger.info(\"âœ… ì²˜ë¦¬ ê²°ê³¼:\")\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(result_df.to_string(index=False))\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ê²°ê³¼ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
        "else:\n",
        "    logger.error(\"âŒ ì²˜ë¦¬ ì‹¤íŒ¨\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
        "# ============================================================================\n",
        "\n",
        "logger.info(f\"âœ… Setup complete. Using {model_type} embeddings.\")\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\n",
        "test_input = \"test_input.xlsx\"\n",
        "test_output = \"test_output.xlsx\"\n",
        "\n",
        "test_data = [\n",
        "    {\"ì›ë¬¸\": \"ä½œè©è¨“å‚³æ™‚ì— ç§»å…¶ç¯‡ç¬¬í•˜ê³  å› æ”¹ä¹‹è€³ë¼\", \"ë²ˆì—­ë¬¸\": \"ì£¼ì„ê³¼ í•´ì„¤ì„ ì‘ì„±í•  ë•Œì— ê·¸ í¸ê³¼ ì¥ì„ ì˜®ê¸°ê³  ê·¸ì— ë”°ë¼ ê³ ì³¤ì„ ë¿ì´ë‹¤.\"},\n",
        "    {\"ì›ë¬¸\": \"å¤ä¾†ç›¸å‚³í•˜ì•¼ å­¸è€…ê°€ æ–¼å…¶èªªì— æœªå˜—è‡´ç–‘í•˜ë‹ˆë¼\", \"ë²ˆì—­ë¬¸\": \"ì˜ˆë¡œë¶€í„° ì„œë¡œ ì „í•´ì ¸ í•™ìë“¤ì€ ê·¸ ì„¤ì— ëŒ€í•´ ì˜ì‹¬ì„ í’ˆì€ ì ì´ ì—†ì—ˆë‹¤.\"}\n",
        "]\n",
        "\n",
        "df_test = pd.DataFrame(test_data)\n",
        "df_test.to_excel(test_input, index=False, engine='openpyxl')\n",
        "logger.info(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±: {test_input}\")\n",
        "\n",
        "# íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
        "success = main(test_input, test_output, verbose=True)\n",
        "\n",
        "# ê²°ê³¼ í™•ì¸\n",
        "if success:\n",
        "    try:\n",
        "        result_df = pd.read_excel(test_output)\n",
        "        logger.info(\"âœ… ì²˜ë¦¬ ê²°ê³¼:\")\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(result_df.to_string(index=False))\n",
        "        print(\"=\"*50)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ê²°ê³¼ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}\")\n",
        "else:\n",
        "    logger.error(\"âŒ ì²˜ë¦¬ ì‹¤íŒ¨\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
