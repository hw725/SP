"""í†µí•©ëœ PA ì •ë ¬ê¸° - ëª¨ë“  ìš”ì†Œ ì—°ê²°"""

import sys
import os
sys.path.append('../sa')
import pandas as pd
from typing import List, Dict
import re
import numpy as np

# âœ… 1. í†µí•©ëœ ì„ë² ë” ë¡œë”©
def get_unified_embedder(embedder_name: str):
    """í†µí•©ëœ ì„ë² ë” ë¡œë”"""
    
    print(f"ğŸ§  ì„ë² ë” ë¡œë”©: {embedder_name}")
    
    # SA ì„ë² ë” ìš°ì„  ì‹œë„
    try:
        if embedder_name == 'bge':
            from sa_embedders.bge import compute_embeddings_with_cache
            print("âœ… SA BGE ì„ë² ë” ë¡œë“œ ì„±ê³µ")
            return compute_embeddings_with_cache
            
        elif embedder_name == 'st':
            from sa_embedders.sentence_transformer import compute_embeddings_with_cache
            print("âœ… SA SentenceTransformer ì„ë² ë” ë¡œë“œ ì„±ê³µ")
            return compute_embeddings_with_cache
            
        elif embedder_name == 'openai':
            from sa_embedders.openai import compute_embeddings_with_cache
            print("âœ… SA OpenAI ì„ë² ë” ë¡œë“œ ì„±ê³µ")
            return compute_embeddings_with_cache
            
    except ImportError as e:
        print(f"âš ï¸ SA ì„ë² ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ë…ë¦½ ì„ë² ë” ì‹œë„
    try:
        if embedder_name == 'bge':
            from FlagEmbedding import FlagModel
            model = FlagModel('BAAI/bge-m3', use_fp16=True)
            
            def bge_embedder(texts: List[str]) -> np.ndarray:
                return model.encode(texts)
            
            print("âœ… ë…ë¦½ BGE ì„ë² ë” ë¡œë“œ ì„±ê³µ")
            return bge_embedder
            
        elif embedder_name == 'st':
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            
            def st_embedder(texts: List[str]) -> np.ndarray:
                return model.encode(texts)
            
            print("âœ… ë…ë¦½ SentenceTransformer ì„ë² ë” ë¡œë“œ ì„±ê³µ")
            return st_embedder
            
    except ImportError as e:
        print(f"âš ï¸ ë…ë¦½ ì„ë² ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ìµœí›„ ìˆ˜ë‹¨: TF-IDF
    print("ğŸ”„ TF-IDF ëŒ€ì²´ ì„ë² ë” ì‚¬ìš©")
    return create_tfidf_embedder()

def create_tfidf_embedder():
    """TF-IDF ê¸°ë°˜ ëŒ€ì²´ ì„ë² ë”"""
    
    from sklearn.feature_extraction.text import TfidfVectorizer
    
    def tfidf_embedder(texts: List[str]) -> np.ndarray:
        if not texts:
            return np.array([]).reshape(0, 512)
        
        try:
            vectorizer = TfidfVectorizer(
                max_features=512, 
                ngram_range=(1, 2),
                analyzer='char'  # í•œì¤‘ì¼ ë¬¸ì ì²˜ë¦¬
            )
            embeddings = vectorizer.fit_transform(texts).toarray()
            
            # L2 ì •ê·œí™”
            norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
            embeddings = embeddings / (norms + 1e-8)
            
            return embeddings
        except Exception as e:
            print(f"âš ï¸ TF-IDF ì‹¤íŒ¨: {e}")
            return np.random.randn(len(texts), 512)
    
    return tfidf_embedder

# âœ… 2. í†µí•©ëœ ì „ì²˜ë¦¬ (sentence_splitter.pyì—ì„œ ê°€ì ¸ì˜´)
def preprocess_text_unified(text: str) -> str:
    """í†µí•©ëœ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    
    if not text or not isinstance(text, str):
        return ""
    
    # ê°œí–‰ ì •ë¦¬
    text = re.sub(r'\r\n|\r|\n', ' ', text)
    
    # ê³µë°± ì •ë¦¬
    text = re.sub(r'[\u00A0\u1680\u2000-\u200B\u202F\u205F\u3000\uFEFF\t]+', ' ', text)
    text = re.sub(r' {2,}', ' ', text)
    
    # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
    text = re.sub(r'[\u200B-\u200D\uFEFF]', '', text)
    text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
    
    return text.strip()

# âœ… 3. ë²ˆì—­ë¬¸ ìš°ì„  ë¶„í•  (aligner_correct.pyì—ì„œ ê°œì„ )
def split_target_by_punctuation_space(text: str, max_length: int = 150) -> List[str]:
    """ë²ˆì—­ë¬¸ êµ¬ë‘ì +ê³µë°± ë¶„í• """
    
    # ì „ì²˜ë¦¬
    text = preprocess_text_unified(text)
    
    # êµ¬ë‘ì +ê³µë°± íŒ¨í„´
    punctuation_space_pattern = r'([.!?ã€‚ï¼ï¼Ÿ])\s+'
    parts = re.split(punctuation_space_pattern, text.strip())
    
    sentences = []
    current = ""
    
    i = 0
    while i < len(parts):
        part = parts[i].strip()
        
        if not part:
            i += 1
            continue
            
        if part in '.!?ã€‚ï¼ï¼Ÿ':
            current += part
            if current.strip():
                sentences.append(current.strip())
                current = ""
        else:
            current += part
        
        i += 1
    
    if current.strip():
        sentences.append(current.strip())
    
    # 150ì ì´ˆê³¼ ì‹œ ë§¥ë½ ë¶„í• 
    final_sentences = []
    for sentence in sentences:
        if len(sentence) <= max_length:
            final_sentences.append(sentence)
        else:
            context_splits = split_by_context_unified(sentence, max_length)
            final_sentences.extend(context_splits)
    
    return final_sentences

def split_by_context_unified(text: str, max_length: int) -> List[str]:
    """ë§¥ë½ ê¸°ë°˜ ë¶„í•  (ê°œì„ )"""
    
    if len(text) <= max_length:
        return [text]
    
    context_patterns = [
        r'([,ï¼Œ]\s*(?:ê·¸ëŸ°ë°|ê·¸ëŸ¬ë‚˜|í•˜ì§€ë§Œ|ë”°ë¼ì„œ|ê·¸ë¦¬í•˜ì—¬|ê·¸ëŸ¬ë¯€ë¡œ|ë˜í•œ|ë˜|ê·¸ë¦¬ê³ ))',
        r'([,ï¼Œ]\s*[\'"])',
        r'(ë¼ê³ \s+[í–ˆë§])',
        r'(ë‹¤ê³ \s+[í–ˆë§])',
        r'([,ï¼Œ]\s*(?:ì´ì œ|ê·¸ë•Œ|ê·¸í›„|ë¨¼ì €|ë‹¤ìŒì—|ì´ì–´ì„œ))',
        r'([,ï¼Œ]\s*<[^>]+>)',
        r'([,ï¼Œ]\s*)',
        r'(ì—ì„œ\s+)',
        r'(ì—ê²Œ\s+)',
        r'(ìœ¼ë¡œ\s+)',
    ]
    
    # ìµœì  ë¶„í• ì  ì°¾ê¸°
    min_pos = int(max_length * 0.3)
    max_pos = int(min(max_length * 0.7, len(text) - 20))
    
    best_pos = None
    best_score = -1
    
    for priority, pattern in enumerate(context_patterns):
        matches = list(re.finditer(pattern, text))
        
        for match in matches:
            pos = match.end()
            
            if min_pos <= pos <= max_pos:
                center = max_length * 0.5
                distance_score = 1.0 - abs(pos - center) / center
                priority_score = 1.0 - (priority * 0.1)
                total_score = distance_score * 0.7 + priority_score * 0.3
                
                if total_score > best_score:
                    best_score = total_score
                    best_pos = pos
    
    if best_pos:
        left = text[:best_pos].strip()
        right = text[best_pos:].strip()
        
        result = []
        if left:
            if len(left) > max_length:
                result.extend(split_by_context_unified(left, max_length))
            else:
                result.append(left)
        
        if right:
            if len(right) > max_length:
                result.extend(split_by_context_unified(right, max_length))
            else:
                result.append(right)
        
        return result
    else:
        mid = len(text) // 2
        return [text[:mid].strip(), text[mid:].strip()]

# âœ… 4. ì›ë¬¸ ë§¤ì¹­ ë¶„í• 
def split_source_to_match_target_unified(src_text: str, target_count: int) -> List[str]:
    """ì›ë¬¸ì„ ë²ˆì—­ë¬¸ ê°œìˆ˜ì— ë§ì¶° ë¶„í• """
    
    src_text = preprocess_text_unified(src_text)
    
    if target_count <= 1:
        return [src_text]
    
    # í•œë¬¸ êµ¬ë¬¸ ê²½ê³„ë¡œ ë¶„í• 
    boundary_patterns = [
        r'([ä¹ŸçŸ£ç„‰å“‰])\s*',
        r'([è€Œç„¶å‰‡æ•…ä¸”])\s*',
        r'([æ›°äº‘][:ï¼š]\s*)',
        r'([ï¼Œ,]\s*)',
        r'([ã€‚.]\s*)',
    ]
    
    chunks = []
    remaining = src_text.strip()
    
    while remaining:
        found = False
        
        for pattern in boundary_patterns:
            match = re.search(pattern, remaining)
            if match:
                end_pos = match.end()
                chunk = remaining[:end_pos].strip()
                
                if chunk:
                    chunks.append(chunk)
                
                remaining = remaining[end_pos:].strip()
                found = True
                break
        
        if not found:
            if remaining.strip():
                chunks.append(remaining.strip())
            break
    
    chunks = chunks if chunks else [src_text]
    
    # ê°œìˆ˜ ì¡°ì •
    if len(chunks) == target_count:
        return chunks
    elif len(chunks) < target_count:
        return expand_source_chunks_unified(chunks, target_count)
    else:
        return merge_source_chunks_unified(chunks, target_count)

def expand_source_chunks_unified(chunks: List[str], target_count: int) -> List[str]:
    """ì›ë¬¸ ì²­í¬ í™•ì¥"""
    
    expanded = []
    need_expand = target_count - len(chunks)
    
    chunks_with_length = [(i, chunk, len(chunk)) for i, chunk in enumerate(chunks)]
    chunks_with_length.sort(key=lambda x: x[2], reverse=True)
    
    expand_indices = set(x[0] for x in chunks_with_length[:need_expand])
    
    for i, chunk in enumerate(chunks):
        if i in expand_indices and len(chunk) > 10:
            mid = len(chunk) // 2
            
            # ì ì ˆí•œ ë¶„í• ì  ì°¾ê¸°
            for offset in range(min(5, len(chunk)//4)):
                pos = mid + offset
                if pos < len(chunk) and chunk[pos] in ' \tï¼Œ,ã€‚.':
                    mid = pos + 1
                    break
                pos = mid - offset
                if pos > 0 and chunk[pos-1] in ' \tï¼Œ,ã€‚.':
                    mid = pos
                    break
            
            left = chunk[:mid].strip()
            right = chunk[mid:].strip()
            
            if left and right:
                expanded.extend([left, right])
            else:
                expanded.append(chunk)
        else:
            expanded.append(chunk)
    
    return expanded

def merge_source_chunks_unified(chunks: List[str], target_count: int) -> List[str]:
    """ì›ë¬¸ ì²­í¬ ë³‘í•©"""
    
    if len(chunks) <= target_count:
        return chunks
    
    merged = []
    chunks_per_group = len(chunks) / target_count
    
    current_group = []
    group_size = 0
    
    for chunk in chunks:
        current_group.append(chunk)
        group_size += 1
        
        if group_size >= chunks_per_group or chunk == chunks[-1]:
            merged_text = ' '.join(current_group).strip()
            merged.append(merged_text)
            current_group = []
            group_size = 0
    
    return merged

# âœ… 5. í†µí•©ëœ ì •ë ¬ í•¨ìˆ˜
def unified_alignment(
    src_paragraph: str,
    tgt_paragraph: str,
    embedder_name: str = 'bge',
    max_length: int = 150,
    similarity_threshold: float = 0.3
) -> List[Dict]:
    """í†µí•©ëœ ë²ˆì—­ë¬¸ ìš°ì„  ì •ë ¬"""
    
    print(f"ğŸ¯ í†µí•© PA ì •ë ¬ ì‹œì‘")
    print(f"   ì›ë¬¸ ê¸¸ì´: {len(src_paragraph)}")
    print(f"   ë²ˆì—­ë¬¸ ê¸¸ì´: {len(tgt_paragraph)}")
    
    # 1. ì„ë² ë” ë¡œë“œ
    embed_func = get_unified_embedder(embedder_name)
    
    # 2. ë²ˆì—­ë¬¸ ë¶„í•  (ìš°ì„ )
    tgt_units = split_target_by_punctuation_space(tgt_paragraph, max_length)
    print(f"ğŸ“ ë²ˆì—­ë¬¸ ë¶„í• : {len(tgt_units)}ê°œ")
    
    # 3. ì›ë¬¸ ë¶„í•  (ë²ˆì—­ë¬¸ì— ë§ì¶¤)
    src_units = split_source_to_match_target_unified(src_paragraph, len(tgt_units))
    print(f"ğŸ” ì›ë¬¸ ë¶„í• : {len(src_units)}ê°œ")
    
    # 4. 1:1 ì •ë ¬
    alignments = create_one_to_one_alignment_unified(
        src_units, tgt_units, embed_func, similarity_threshold
    )
    
    print(f"âœ… ì •ë ¬ ì™„ë£Œ: {len(alignments)}ê°œ")
    
    return alignments

def create_one_to_one_alignment_unified(
    src_units: List[str],
    tgt_units: List[str],
    embed_func,
    similarity_threshold: float
) -> List[Dict]:
    """1:1 ì •ë ¬ ìƒì„± (í†µí•©)"""
    
    alignments = []
    max_len = max(len(src_units), len(tgt_units))
    
    # ê¸¸ì´ ë§ì¶”ê¸°
    while len(src_units) < max_len:
        src_units.append("")
    while len(tgt_units) < max_len:
        tgt_units.append("")
    
    # ìœ ì‚¬ë„ ê³„ì‚°
    try:
        from sklearn.metrics.pairwise import cosine_similarity
        
        valid_src = [s for s in src_units if s.strip()]
        valid_tgt = [t for t in tgt_units if t.strip()]
        
        if valid_src and valid_tgt:
            src_embeddings = embed_func(valid_src)
            tgt_embeddings = embed_func(valid_tgt)
            
            similarities = []
            valid_src_idx = 0
            valid_tgt_idx = 0
            
            for i in range(max_len):
                if (i < len(src_units) and src_units[i].strip() and 
                    i < len(tgt_units) and tgt_units[i].strip()):
                    
                    if (valid_src_idx < len(src_embeddings) and 
                        valid_tgt_idx < len(tgt_embeddings)):
                        
                        sim = cosine_similarity(
                            [tgt_embeddings[valid_tgt_idx]], 
                            [src_embeddings[valid_src_idx]]
                        )[0][0]
                        similarities.append(sim)
                        
                        valid_src_idx += 1
                        valid_tgt_idx += 1
                    else:
                        similarities.append(0.0)
                else:
                    similarities.append(0.0)
        else:
            similarities = [0.0] * max_len
            
    except Exception as e:
        print(f"âš ï¸ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
        similarities = [0.0] * max_len
    
    # ì •ë ¬ ê²°ê³¼ ìƒì„±
    for i in range(max_len):
        alignments.append({
            'ì›ë¬¸': src_units[i] if i < len(src_units) else "",
            'ë²ˆì—­ë¬¸': tgt_units[i] if i < len(tgt_units) else "",
            'similarity': float(similarities[i]) if i < len(similarities) else 0.0,
            'split_method': 'unified_target_first',
            'align_method': 'unified_1to1'
        })
    
    return alignments

# âœ… 6. íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜
def process_file_unified(
    input_file: str,
    output_file: str,
    embedder_name: str = 'bge',
    max_length: int = 150,
    similarity_threshold: float = 0.3
) -> pd.DataFrame:
    """í†µí•©ëœ íŒŒì¼ ì²˜ë¦¬"""
    
    print(f"ğŸ“‚ í†µí•© PA íŒŒì¼ ì²˜ë¦¬: {input_file}")
    
    # íŒŒì¼ ë¡œë“œ
    try:
        df = pd.read_excel(input_file)
        print(f"ğŸ“„ {len(df)}ê°œ ë¬¸ë‹¨ ë¡œë“œ")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    if 'ì›ë¬¸' not in df.columns or 'ë²ˆì—­ë¬¸' not in df.columns:
        print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ ì—†ìŒ: {list(df.columns)}")
        return None
    
    all_results = []
    
    for idx, row in df.iterrows():
        src_paragraph = str(row.get('ì›ë¬¸', '')).strip()
        tgt_paragraph = str(row.get('ë²ˆì—­ë¬¸', '')).strip()
        
        if not src_paragraph or not tgt_paragraph:
            print(f"âš ï¸ ë¹ˆ ë‚´ìš© ê±´ë„ˆëœ€: í–‰ {idx + 1}")
            continue
        
        try:
            print(f"ğŸ“ ì²˜ë¦¬ ì¤‘: ë¬¸ë‹¨ {idx + 1}/{len(df)}")
            
            alignments = unified_alignment(
                src_paragraph,
                tgt_paragraph,
                embedder_name=embedder_name,
                max_length=max_length,
                similarity_threshold=similarity_threshold
            )
            
            # ë¬¸ë‹¨ì‹ë³„ì ì¶”ê°€
            for result in alignments:
                result['ë¬¸ë‹¨ì‹ë³„ì'] = idx + 1
            
            all_results.extend(alignments)
            
        except Exception as e:
            print(f"âŒ ë¬¸ë‹¨ {idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if not all_results:
        print("âŒ ì²˜ë¦¬ ê²°ê³¼ ì—†ìŒ")
        return None
    
    # ê²°ê³¼ ì €ì¥
    try:
        result_df = pd.DataFrame(all_results)
        
        columns = ['ë¬¸ë‹¨ì‹ë³„ì', 'ì›ë¬¸', 'ë²ˆì—­ë¬¸', 'similarity', 'split_method', 'align_method']
        result_df = result_df[columns]
        
        result_df.to_excel(output_file, index=False)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
        print(f"ğŸ“Š ì´ {len(all_results)}ê°œ ì •ë ¬")
        
        # ê²°ê³¼ ë¶„ì„
        analyze_results_unified(result_df)
        
        return result_df
        
    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def analyze_results_unified(df: pd.DataFrame):
    """ê²°ê³¼ ë¶„ì„"""
    
    print(f"\nğŸ“Š ê²°ê³¼ ë¶„ì„:")
    print(f"   í‰ê·  ìœ ì‚¬ë„: {df['similarity'].mean():.3f}")
    print(f"   ìµœê³  ìœ ì‚¬ë„: {df['similarity'].max():.3f}")
    print(f"   ìµœì € ìœ ì‚¬ë„: {df['similarity'].min():.3f}")
    
    high_quality = sum(1 for x in df['similarity'] if x > 0.7)
    medium_quality = sum(1 for x in df['similarity'] if 0.5 <= x <= 0.7)
    low_quality = sum(1 for x in df['similarity'] if x < 0.5)
    total = len(df)
    
    print(f"   ê³ í’ˆì§ˆ (>0.7): {high_quality}/{total} ({high_quality/total*100:.1f}%)")
    print(f"   ì¤‘í’ˆì§ˆ (0.5-0.7): {medium_quality}/{total} ({medium_quality/total*100:.1f}%)")
    print(f"   ì €í’ˆì§ˆ (<0.5): {low_quality}/{total} ({low_quality/total*100:.1f}%)")

# âœ… 7. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_unified_system():
    """í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    test_case = {
        "src": "è’¹è‘­è’¼è’¼ç™½éœ²ç‚ºéœœæ‰€è¬‚ä¼Šäººåœ¨æ°´ä¸€æ–¹é¡æ´„å¾ä¹‹é“é˜»ä¸”é•·é¡æ¸¸å¾ä¹‹å®›åœ¨æ°´ä¸­å¤®",
        "tgt": "è’¹è‘­ëŠ” í‘¸ë¥´ë¥´ê³  ç™½éœ²ëŠ” ì„œë¦¬ê°€ ë˜ì—ˆë‹¤. ì´ë¥¸ë°” ê·¸ ì‚¬ëŒì€ ë¬¼ í•œí¸ì— ìˆë‹¤. ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ë”°ë¼ê°€ë‹ˆ ê¸¸ì´ í—˜í•˜ê³  ë©€ë‹¤. ë¬¼ì‚´ ë”°ë¼ ë‚´ë ¤ê°€ë©° ë”°ë¼ê°€ë‹ˆ ë¬¼ í•œê°€ìš´ë° ìˆëŠ” ë“¯í•˜ë‹¤."
    }
    
    print("ğŸ§ª í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    
    result = unified_alignment(test_case['src'], test_case['tgt'])
    
    print("\nğŸ¯ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    for i, r in enumerate(result, 1):
        print(f"{i}. ì›ë¬¸: {r['ì›ë¬¸']}")
        print(f"   ë²ˆì—­: {r['ë²ˆì—­ë¬¸']}")
        print(f"   ìœ ì‚¬ë„: {r['similarity']:.3f}")

if __name__ == "__main__":
    test_unified_system()