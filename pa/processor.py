"""PA ë©”ì¸ í”„ë¡œì„¸ì„œ - ê°œì„ ëœ ì •ë ¬"""

import pandas as pd
from typing import List, Dict
import numpy as np
from sentence_splitter import split_target_sentences_advanced, split_source_with_spacy
import torch
from aligner import get_embedder_function  # âœ… alignerì˜ ì„ë² ë” í•¨ìˆ˜ë§Œ ì‚¬ìš©

def get_device(device_preference="cuda"):
    if device_preference == "cuda" and not torch.cuda.is_available():
        print("âš ï¸ CUDA(GPU)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        return "cpu"
    return device_preference

def improved_align_paragraphs(
    tgt_sentences: List[str], 
    src_chunks: List[str], 
    embed_func,
    similarity_threshold: float = 0.3
) -> List[Dict]:
    """ê°œì„ ëœ ì •ë ¬ - 1:1 ë§¤ì¹­ ë³´ì¥"""
    
    from sklearn.metrics.pairwise import cosine_similarity
    
    if not tgt_sentences or not src_chunks:
        return []
    
    # ì„ë² ë”© ìƒì„± (í•­ìƒ numpy arrayë¡œ ë³€í™˜)
    tgt_embeddings = np.array(embed_func(tgt_sentences))
    src_embeddings = np.array(embed_func(src_chunks))

    # ì„ë² ë”© ì°¨ì› ì²´í¬
    if tgt_embeddings.shape[1] != src_embeddings.shape[1]:
        print(f"âŒ ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: tgt={tgt_embeddings.shape}, src={src_embeddings.shape}")
        return []
    
    # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
    sim_matrix = cosine_similarity(tgt_embeddings, src_embeddings)
    
    alignments = []
    
    # âœ… ê°œì„ ëœ ì •ë ¬: ê¸¸ì´ì— ë”°ë¼ ì „ëµ ì„ íƒ
    if len(tgt_sentences) == len(src_chunks):
        # 1:1 ìˆœì„œ ë§¤ì¹­
        for i in range(len(tgt_sentences)):
            alignments.append({
                'ì›ë¬¸': src_chunks[i],
                'ë²ˆì—­ë¬¸': tgt_sentences[i],
                'similarity': sim_matrix[i][i] if i < len(src_chunks) else 0.0,
                'split_method': 'spacy_lg',
                'align_method': 'sequential_1to1'
            })
    
    elif len(tgt_sentences) > len(src_chunks):
        # ë²ˆì—­ë¬¸ì´ ë” ë§ìŒ: ì›ë¬¸ì„ ì—¬ëŸ¬ ë²ˆì—­ë¬¸ì— ë¶„ë°°
        alignments = distribute_sources_to_targets(
            tgt_sentences, src_chunks, sim_matrix, 'target_rich'
        )
    
    else:
        # ì›ë¬¸ì´ ë” ë§ìŒ: ë²ˆì—­ë¬¸ì„ ì—¬ëŸ¬ ì›ë¬¸ì— ë¶„ë°°
        alignments = distribute_targets_to_sources(
            tgt_sentences, src_chunks, sim_matrix, 'source_rich'
        )
    
    return alignments

def distribute_sources_to_targets(
    tgt_sentences: List[str], 
    src_chunks: List[str], 
    sim_matrix: np.ndarray,
    method: str
) -> List[Dict]:
    """ì›ë¬¸ì„ ë²ˆì—­ë¬¸ì— ë¶„ë°°"""
    
    alignments = []
    src_per_tgt = len(tgt_sentences) // len(src_chunks)
    remaining = len(tgt_sentences) % len(src_chunks)
    
    tgt_idx = 0
    
    for src_idx, src_chunk in enumerate(src_chunks):
        # í˜„ì¬ ì›ë¬¸ì— í• ë‹¹í•  ë²ˆì—­ë¬¸ ê°œìˆ˜
        assign_count = src_per_tgt + (1 if src_idx < remaining else 0)
        
        # ê°€ì¥ ìœ ì‚¬í•œ ë²ˆì—­ë¬¸ë“¤ ì°¾ê¸°
        if tgt_idx < len(tgt_sentences):
            end_idx = min(tgt_idx + assign_count, len(tgt_sentences))
            
            for t_idx in range(tgt_idx, end_idx):
                similarity = sim_matrix[t_idx][src_idx] if t_idx < sim_matrix.shape[0] else 0.0
                
                alignments.append({
                    'ì›ë¬¸': src_chunk,
                    'ë²ˆì—­ë¬¸': tgt_sentences[t_idx],
                    'similarity': similarity,
                    'split_method': 'spacy_lg',
                    'align_method': method
                })
            
            tgt_idx = end_idx
    
    # ë‚¨ì€ ë²ˆì—­ë¬¸ ì²˜ë¦¬
    while tgt_idx < len(tgt_sentences):
        alignments.append({
            'ì›ë¬¸': "",
            'ë²ˆì—­ë¬¸': tgt_sentences[tgt_idx],
            'similarity': 0.0,
            'split_method': 'spacy_lg',
            'align_method': 'unmatched_target'
        })
        tgt_idx += 1
    
    return alignments

def distribute_targets_to_sources(
    tgt_sentences: List[str], 
    src_chunks: List[str], 
    sim_matrix: np.ndarray,
    method: str
) -> List[Dict]:
    """ë²ˆì—­ë¬¸ì„ ì›ë¬¸ì— ë¶„ë°°"""
    
    alignments = []
    tgt_per_src = len(src_chunks) // len(tgt_sentences)
    remaining = len(src_chunks) % len(tgt_sentences)
    
    src_idx = 0
    
    for tgt_idx, tgt_sentence in enumerate(tgt_sentences):
        # í˜„ì¬ ë²ˆì—­ë¬¸ì— í• ë‹¹í•  ì›ë¬¸ ê°œìˆ˜
        assign_count = tgt_per_src + (1 if tgt_idx < remaining else 0)
        
        if src_idx < len(src_chunks):
            end_idx = min(src_idx + assign_count, len(src_chunks))
            
            # ì²« ë²ˆì§¸ ì›ë¬¸ê³¼ ë§¤ì¹­
            if src_idx < len(src_chunks):
                similarity = sim_matrix[tgt_idx][src_idx] if tgt_idx < sim_matrix.shape[0] else 0.0
                
                # ì—¬ëŸ¬ ì›ë¬¸ì„ í•©ì³ì„œ í•˜ë‚˜ì˜ ë§¤ì¹­ ìƒì„±
                combined_src = " ".join(src_chunks[src_idx:end_idx])
                
                alignments.append({
                    'ì›ë¬¸': combined_src,
                    'ë²ˆì—­ë¬¸': tgt_sentence,
                    'similarity': similarity,
                    'split_method': 'spacy_lg',
                    'align_method': method
                })
            
            src_idx = end_idx
    
    # ë‚¨ì€ ì›ë¬¸ ì²˜ë¦¬
    while src_idx < len(src_chunks):
        alignments.append({
            'ì›ë¬¸': src_chunks[src_idx],
            'ë²ˆì—­ë¬¸': "",
            'similarity': 0.0,
            'split_method': 'spacy_lg',
            'align_method': 'unmatched_source'
        })
        src_idx += 1
    
    return alignments

def process_paragraph_file(
    input_file: str, 
    output_file: str, 
    embedder_name: str = 'bge',
    max_length: int = 150,
    similarity_threshold: float = 0.3,
    device: str = "cuda"   # ê¸°ë³¸ê°’ë„ cudaë¡œ!
):
    """íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬ (ë©”ì¸ í•¨ìˆ˜)"""
    
    print(f"ğŸ“‚ PA íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file}")
    
    try:
        # Excel íŒŒì¼ ë¡œë“œ
        df = pd.read_excel(input_file)
        print(f"ğŸ“„ {len(df)}ê°œ ë¬¸ë‹¨ ë¡œë“œë¨")
        
    except FileNotFoundError:
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        return None
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì—ëŸ¬: {e}")
        return None
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    if 'ì›ë¬¸' not in df.columns or 'ë²ˆì—­ë¬¸' not in df.columns:
        print(f"âŒ í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: 'ì›ë¬¸', 'ë²ˆì—­ë¬¸'")
        print(f"í˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")
        return None
    
    # ì„ë² ë” ë¡œë“œ
    try:
        embed_func = get_embedder_function(embedder_name, device=device)
        print(f"ğŸ§  ì„ë² ë” ë¡œë“œ ì™„ë£Œ: {embedder_name} (device={device})")
    except Exception as e:
        print(f"âŒ ì„ë² ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
        from aligner import fallback_embedder_bge
        embed_func = fallback_embedder_bge(device)
    
    all_results = []
    
    for idx, row in df.iterrows():
        src_paragraph = str(row.get('ì›ë¬¸', '')).strip()
        tgt_paragraph = str(row.get('ë²ˆì—­ë¬¸', '')).strip()
        
        if not src_paragraph or not tgt_paragraph:
            print(f"âš ï¸ ë¹ˆ ë‚´ìš© ê±´ë„ˆëœ€: í–‰ {idx + 1}")
            continue
        
        try:
            print(f"ğŸ“ ì²˜ë¦¬ ì¤‘: ë¬¸ë‹¨ {idx + 1}/{len(df)}")
            
            # ë¬¸ì¥ ë¶„í• 
            tgt_sentences = split_target_sentences_advanced(tgt_paragraph, max_length)
            src_chunks = split_source_with_spacy(src_paragraph, tgt_sentences)
            
            print(f"   ë²ˆì—­ë¬¸: {len(tgt_sentences)}ê°œ ë¬¸ì¥")
            print(f"   ì›ë¬¸: {len(src_chunks)}ê°œ ì²­í¬")
            
            # âœ… ê°œì„ ëœ ì •ë ¬ ì‚¬ìš©
            alignments = improved_align_paragraphs(
                tgt_sentences, 
                src_chunks, 
                embed_func, 
                similarity_threshold
            )
            
            # ë¬¸ë‹¨ì‹ë³„ì ì—…ë°ì´íŠ¸
            for result in alignments:
                result['ë¬¸ë‹¨ì‹ë³„ì'] = idx + 1
            
            all_results.extend(alignments)
            
        except Exception as e:
            print(f"âŒ ë¬¸ë‹¨ {idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if not all_results:
        print("âŒ ì²˜ë¦¬ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # ê²°ê³¼ ì €ì¥
    try:
        result_df = pd.DataFrame(all_results)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        available_columns = result_df.columns.tolist()
        desired_columns = ['ë¬¸ë‹¨ì‹ë³„ì', 'ì›ë¬¸', 'ë²ˆì—­ë¬¸', 'similarity', 'split_method', 'align_method']
        final_columns = [col for col in desired_columns if col in available_columns]
        
        result_df = result_df[final_columns]
        result_df.to_excel(output_file, index=False)
        
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        print(f"ğŸ“Š ì´ {len(all_results)}ê°œ ë¬¸ì¥ ìŒ ìƒì„±")
        
        # âœ… ê²°ê³¼ ë¶„ì„ ì¶”ê°€
        analyze_alignment_results(result_df)
        
        return result_df
        
    except Exception as e:
        print(f"âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None

def analyze_alignment_results(result_df: pd.DataFrame):
    """ì •ë ¬ ê²°ê³¼ ë¶„ì„ (ê°œì„ ëœ ë²„ì „)"""
    
    print("\nğŸ“Š ì •ë ¬ ê²°ê³¼ ë¶„ì„:")
    
    # ë¬¸ë‹¨ë³„ í†µê³„
    paragraph_stats = result_df.groupby('ë¬¸ë‹¨ì‹ë³„ì').agg({
        'ì›ë¬¸': lambda x: sum(1 for text in x if str(text).strip()),
        'ë²ˆì—­ë¬¸': lambda x: sum(1 for text in x if str(text).strip()),
        'similarity': 'mean'
    }).round(3)
    
    print("ğŸ“ˆ ë¬¸ë‹¨ë³„ í†µê³„:")
    for idx, row in paragraph_stats.iterrows():
        print(f"   ë¬¸ë‹¨ {idx}: ì›ë¬¸ {row['ì›ë¬¸']}ê°œ, ë²ˆì—­ë¬¸ {row['ë²ˆì—­ë¬¸']}ê°œ, ìœ ì‚¬ë„ {row['similarity']:.3f}")
    
    # ì „ì²´ ìœ ì‚¬ë„ ë¶„í¬
    print(f"\nğŸ¯ ì „ì²´ ìœ ì‚¬ë„:")
    print(f"   í‰ê· : {result_df['similarity'].mean():.3f}")
    print(f"   ìµœê³ : {result_df['similarity'].max():.3f}")
    print(f"   ìµœì €: {result_df['similarity'].min():.3f}")
    
    # ê³ í’ˆì§ˆ ë§¤ì¹­ ë¹„ìœ¨
    high_quality = sum(1 for x in result_df['similarity'] if x > 0.7)
    medium_quality = sum(1 for x in result_df['similarity'] if 0.5 <= x <= 0.7)
    low_quality = sum(1 for x in result_df['similarity'] if x < 0.5)
    total = len(result_df)
    
    print(f"\nğŸ“Š í’ˆì§ˆë³„ ë§¤ì¹­:")
    print(f"   ê³ í’ˆì§ˆ (>0.7): {high_quality}/{total} ({high_quality/total*100:.1f}%)")
    print(f"   ì¤‘í’ˆì§ˆ (0.5-0.7): {medium_quality}/{total} ({medium_quality/total*100:.1f}%)")
    print(f"   ì €í’ˆì§ˆ (<0.5): {low_quality}/{total} ({low_quality/total*100:.1f}%)")
    
    # ë¹ˆ ë§¤ì¹­ í™•ì¸
    empty_source = sum(1 for x in result_df['ì›ë¬¸'] if not str(x).strip())
    empty_target = sum(1 for x in result_df['ë²ˆì—­ë¬¸'] if not str(x).strip())
    
    if empty_source > 0:
        print(f"âš ï¸ ë¹ˆ ì›ë¬¸: {empty_source}ê°œ")
    if empty_target > 0:
        print(f"âš ï¸ ë¹ˆ ë²ˆì—­ë¬¸: {empty_target}ê°œ")
    
    # ì •ë ¬ ë°©ë²•ë³„ í†µê³„
    if 'align_method' in result_df.columns:
        align_stats = result_df['align_method'].value_counts()
        print(f"\nğŸ”€ ì •ë ¬ ë°©ë²•ë³„ í†µê³„:")
        for method, count in align_stats.items():
            avg_sim = result_df[result_df['align_method'] == method]['similarity'].mean()
            print(f"   {method}: {count}íšŒ (í‰ê·  ìœ ì‚¬ë„ {avg_sim:.3f})")
    
    return paragraph_stats