"""PA ì „ìš© ì •ë ¬ê¸° - spaCy ìˆœì°¨ì  ë¶„í•  ì •ë ¬ë§Œ ì‚¬ìš© (SA ì—°ë™ ì™„ì „ ì œê±°, circular import ì™„ì „ ì œê±°)"""
import sys
import os
import importlib
import numpy as np
import pandas as pd

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from typing import List, Dict
from sentence_splitter import split_target_sentences_advanced

# íŒ¨í‚¤ì§€ import ë°©ì‹ìœ¼ë¡œ ë³µì›
from sa.sa_embedders import get_embedder

try:
    import torch
except ImportError:
    torch = None

def get_embedder_function(embedder_name: str, device: str = "cpu", openai_model: str = None, openai_api_key: str = None):
    # Robust device selection: if device=="cuda" but not available, fallback to cpu
    if device == "cuda":
        if torch is None or not torch.cuda.is_available():
            print("âš ï¸ torch ë¯¸ì„¤ì¹˜ ë˜ëŠ” CUDA ë¯¸ì§€ì›: CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            device = "cpu"
    if embedder_name == 'bge':
        return get_embedder("bge", device_id=device)
    elif embedder_name == 'openai':
        sa_openai = importlib.import_module('sa.sa_embedders.openai')
        compute_embeddings_with_cache = sa_openai.compute_embeddings_with_cache
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
        def embed_func(texts):
            return compute_embeddings_with_cache(
                texts, 
                model=openai_model if openai_model else "text-embedding-3-large"
            )
        return embed_func
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”: {embedder_name}. ì§€ì›: openai, bge")

# improved_align_paragraphs ì§ì ‘ í¬í•¨ (circular import ì œê±°)
def improved_align_paragraphs(
    tgt_sentences: List[str], 
    src_chunks: List[str], 
    embed_func,
    similarity_threshold: float = 0.3
) -> List[Dict]:
    if not tgt_sentences:
        return []
    if isinstance(src_chunks, str):
        source_text = src_chunks
    elif isinstance(src_chunks, list) and len(src_chunks) == 1:
        source_text = src_chunks[0]
    elif isinstance(src_chunks, list):
        source_text = ' '.join(str(chunk) for chunk in src_chunks)
    else:
        source_text = str(src_chunks) if src_chunks else ""
    if not source_text.strip():
        return [{
            'ì›ë¬¸': '',
            'ë²ˆì—­ë¬¸': tgt_sent,
            'similarity': 0.0,
            'split_method': 'whitespace',
            'align_method': 'no_source'
        } for tgt_sent in tgt_sentences]
    print(f"ğŸ”„ ì˜ë¯¸ì  ë³‘í•© ì •ë ¬ ì‹œì‘: {len(tgt_sentences)}ê°œ ë²ˆì—­ë¬¸")
    from sentence_splitter import split_source_by_whitespace_and_align
    aligned_src_chunks = split_source_by_whitespace_and_align(source_text, tgt_sentences, embed_func, similarity_threshold)
    # ì„ë² ë”© ìœ ì‚¬ë„ ê³„ì‚° ë° ê²°ê³¼ ìƒì„±
    from sklearn.metrics.pairwise import cosine_similarity
    def safe_embed(texts):
        try:
            return np.array(embed_func(texts))
        except Exception as e:
            print(f"ì„ë² ë”© ì˜¤ë¥˜: {e}")
            return np.zeros((len(texts), 768))
    tgt_embeddings = safe_embed(tgt_sentences)
    src_embeddings = safe_embed(aligned_src_chunks)
    if tgt_embeddings.shape[1] != src_embeddings.shape[1]:
        min_dim = min(tgt_embeddings.shape[1], src_embeddings.shape[1])
        tgt_embeddings = tgt_embeddings[:, :min_dim]
        src_embeddings = src_embeddings[:, :min_dim]
    sim_matrix = cosine_similarity(tgt_embeddings, src_embeddings)
    alignments = []
    for i in range(len(tgt_sentences)):
        alignments.append({
            'ì›ë¬¸': aligned_src_chunks[i] if i < len(aligned_src_chunks) else '',
            'ë²ˆì—­ë¬¸': tgt_sentences[i],
            'similarity': sim_matrix[i, i] if i < sim_matrix.shape[0] and i < sim_matrix.shape[1] else 0.0,
            'split_method': 'whitespace',
            'align_method': 'semantic_merge'
        })
    # ë‚¨ì€ ì›ë¬¸ ì²­í¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    for j in range(len(tgt_sentences), len(aligned_src_chunks)):
        alignments.append({
            'ì›ë¬¸': aligned_src_chunks[j],
            'ë²ˆì—­ë¬¸': '',
            'similarity': 0.0,
            'split_method': 'whitespace',
            'align_method': 'semantic_merge_unmatched_src'
        })
    print(f"âœ… ì˜ë¯¸ì  ë³‘í•© ì •ë ¬ ì™„ë£Œ: {len(alignments)}ê°œ í•­ëª©")
    return alignments

def process_paragraph_alignment(
    src_paragraph: str, 
    tgt_paragraph: str, 
    embedder_name: str = 'bge',
    max_length: int = 150,
    similarity_threshold: float = 0.3,
    device: str = "cpu"
):
    """PA ì²˜ë¦¬ (ê³µë°±/êµ¬ë‘ì  ê¸°ë°˜ ìˆœì°¨ì  ë¶„í• ë§Œ ì‚¬ìš©)"""
    print(f"ğŸ”„ PA ì²˜ë¦¬ ì‹œì‘ (ê³µë°±/êµ¬ë‘ì  ìˆœì°¨ì  ë¶„í• )")
    tgt_sentences = split_target_sentences_advanced(tgt_paragraph, max_length, splitter="spacy")
    # ì›ë¬¸ ë¶„í• : spaCy ì™„ì „ ë°°ì œ, ê³µë°±/êµ¬ë‘ì  ê¸°ì¤€ ë¶„í• ë§Œ ì‚¬ìš©
    src_chunks = src_paragraph  # improved_align_paragraphsì—ì„œ ì§ì ‘ ë¶„í• 
    print(f"   ë²ˆì—­ë¬¸: {len(tgt_sentences)}ê°œ ë¬¸ì¥")
    print(f"   ì›ë¬¸: {len(src_paragraph)}ê°œ í† í°")
    embed_func = get_embedder_function(embedder_name, device=device)
    alignments = improved_align_paragraphs(
        tgt_sentences, 
        src_chunks, 
        embed_func, 
        similarity_threshold
    )
    # ë¬¸ë‹¨ì‹ë³„ì ë¶€ì—¬
    for a in alignments:
        a['ë¬¸ë‹¨ì‹ë³„ì'] = 1
    return alignments


def process_paragraph_file(
    input_file: str, 
    output_file: str, 
    embedder_name: str = 'bge',
    max_length: int = 150,
    similarity_threshold: float = 0.3,
    device: str = "cpu"
):
    """íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬ - spaCy ìˆœì°¨ì  ë¶„í•  ì •ë ¬ë§Œ ì‚¬ìš©"""
    print(f"ğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file}")
    df = pd.read_excel(input_file)
    all_results = []
    for idx, row in df.iterrows():
        src_paragraph = str(row.get('ì›ë¬¸', ''))
        tgt_paragraph = str(row.get('ë²ˆì—­ë¬¸', ''))
        if src_paragraph and tgt_paragraph:
            alignments = process_paragraph_alignment(
                src_paragraph,
                tgt_paragraph,
                embedder_name=embedder_name,
                max_length=max_length,
                similarity_threshold=similarity_threshold,
                device=device
            )
            all_results.extend(alignments)
    result_df = pd.DataFrame(all_results)
    final_columns = ['ë¬¸ë‹¨ì‹ë³„ì', 'ì›ë¬¸', 'ë²ˆì—­ë¬¸', 'similarity', 'split_method', 'align_method']
    result_df = result_df[final_columns]
    result_df.to_excel(output_file, index=False)
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    print(f"ğŸ“Š ì´ {len(all_results)}ê°œ ë¬¸ì¥ ìŒ ìƒì„±")
    return result_df