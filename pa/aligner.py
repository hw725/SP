"""PA ì „ìš© ì •ë ¬ê¸° - spaCy ìˆœì°¨ì  ë¶„í•  ì •ë ¬ë§Œ ì‚¬ìš© (SA ì—°ë™ ì™„ì „ ì œê±°, circular import ì™„ì „ ì œê±°)"""
import sys
import os
import importlib
import numpy as np
import pandas as pd

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from typing import List, Dict
from sentence_splitter import split_target_sentences_advanced, split_source_by_whitespace_and_align

# íŒ¨í‚¤ì§€ import ë°©ì‹ìœ¼ë¡œ ë³µì›
from sa.sa_embedders import get_embedder

try:
    import torch
except ImportError:
    torch = None

def get_embedder_function(embedder_name: str, device: str = "cpu", openai_model: str = None, openai_api_key: str = None):
    # Robust device selection: if device=="cuda" but not available, fallback to cpu
    if device == "cuda":
        if torch is None or not torch.cuda.is_available():
            print("âš ï¸ torch ë¯¸ì„¤ì¹˜ ë˜ëŠ” CUDA ë¯¸ì§€ì›: CPUë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            device = "cpu"
    if embedder_name == 'bge':
        return get_embedder("bge", device_id=device)
    elif embedder_name == 'openai':
        sa_openai = importlib.import_module('sa.sa_embedders.openai')
        compute_embeddings_with_cache = sa_openai.compute_embeddings_with_cache
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key
        def embed_func(texts):
            return compute_embeddings_with_cache(
                texts, 
                model=openai_model if openai_model else "text-embedding-3-large"
            )
        return embed_func
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”: {embedder_name}. ì§€ì›: openai, bge")

# improved_align_paragraphs ì§ì ‘ í¬í•¨ (circular import ì œê±°)
def improved_align_paragraphs(
    tgt_sentences: List[str], 
    src_text: str, 
    embed_func=None,
    similarity_threshold: float = 0.3
) -> List[Dict]:
    """
    ìˆœì°¨ì  1:1 ì •ë ¬ (ê³µë°±/í¬ë§· 100% ë³´ì¡´, ì˜ë¯¸ì  align ì œê±°)
    """
    if not tgt_sentences:
        return []
    
    # ì›ë¬¸ì„ ë²ˆì—­ë¬¸ ê°œìˆ˜ì— ë§ì¶° ìˆœì°¨ì ìœ¼ë¡œ ë¶„í• 
    aligned_src_chunks = split_source_by_whitespace_and_align(src_text, len(tgt_sentences))
    
    alignments = []
    for i in range(len(tgt_sentences)):
        alignments.append({
            'ì›ë¬¸': aligned_src_chunks[i] if i < len(aligned_src_chunks) else '',
            'ë²ˆì—­ë¬¸': tgt_sentences[i],
            'similarity': 1.0,  # ìˆœì°¨ì  ì •ë ¬ì´ë¯€ë¡œ ìœ ì‚¬ë„ëŠ” 1.0
            'split_method': 'punctuation',
            'align_method': 'sequential'
        })
    
    # ë‚¨ì€ ì›ë¬¸ ì²­í¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
    for j in range(len(tgt_sentences), len(aligned_src_chunks)):
        alignments.append({
            'ì›ë¬¸': aligned_src_chunks[j],
            'ë²ˆì—­ë¬¸': '',
            'similarity': 0.0,
            'split_method': 'punctuation',
            'align_method': 'sequential_unmatched_src'
        })
    
    return alignments

def process_paragraph_alignment(
    src_paragraph: str, 
    tgt_paragraph: str, 
    embedder_name: str = 'bge',
    max_length: int = 150,
    similarity_threshold: float = 0.3,
    device: str = "cpu",
    quality_threshold: float = 0.8
):
    """
    PA ì²˜ë¦¬: ìˆœì°¨ì /ì˜ë¯¸ì  ì •ë ¬ ëª¨ë‘ ì‹¤í–‰, ê°€ì¤‘í•© similarity(0.4/0.6)ë¡œ í’ˆì§ˆ ê¸°ì¤€(0.8) ì´ìƒì´ë©´ ê°€ì¤‘í•© ê²°ê³¼, ì•„ë‹ˆë©´ ì˜ë¯¸ì  ê²°ê³¼ë§Œ ì±„íƒ
    """
    print(f"ğŸ”„ PA ì²˜ë¦¬ ì‹œì‘ (ìˆœì°¨ì +ì˜ë¯¸ì  ë³‘í•©)")
    # 1. ìˆœì°¨ì  ì •ë ¬
    tgt_sentences_seq = split_target_sentences_advanced(tgt_paragraph, max_length, splitter="punctuation")
    alignments_seq = improved_align_paragraphs(
        tgt_sentences_seq, 
        src_paragraph
    )
    # 2. ì˜ë¯¸ì  ë³‘í•©
    tgt_sentences_sem = split_target_sentences_advanced(tgt_paragraph, max_length, splitter="spacy")
    embed_func = get_embedder_function(embedder_name, device=device)
    alignments_sem = improved_align_paragraphs(
        tgt_sentences_sem,
        src_paragraph,
        embed_func,
        similarity_threshold
    )
    # 3. ìŒë³„ ê°€ì¤‘í•© ë° ì¡°ê±´ë¶€ ì„ íƒ
    results = []
    max_len = max(len(alignments_seq), len(alignments_sem))
    for i in range(max_len):
        seq = alignments_seq[i] if i < len(alignments_seq) else {'ì›ë¬¸':'','ë²ˆì—­ë¬¸':'','similarity':0.0,'split_method':'punctuation','align_method':'sequential'}
        sem = alignments_sem[i] if i < len(alignments_sem) else {'ì›ë¬¸':'','ë²ˆì—­ë¬¸':'','similarity':0.0,'split_method':'spacy','align_method':'semantic'}
        # ê°€ì¤‘í•© similarity
        weighted_sim = seq['similarity']*0.4 + sem['similarity']*0.6
        if weighted_sim >= quality_threshold:
            # ê°€ì¤‘í•© ê²°ê³¼ ì±„íƒ, ì •ë³´ ë³‘í•©
            result = {
                'ì›ë¬¸': sem['ì›ë¬¸'] if sem['ì›ë¬¸'] else seq['ì›ë¬¸'],
                'ë²ˆì—­ë¬¸': sem['ë²ˆì—­ë¬¸'] if sem['ë²ˆì—­ë¬¸'] else seq['ë²ˆì—­ë¬¸'],
                'similarity': weighted_sim,
                'split_method': f"seq+sem",
                'align_method': 'hybrid'
            }
        else:
            # ì˜ë¯¸ì  ê²°ê³¼ë§Œ ì±„íƒ
            result = sem.copy()
            result['align_method'] = 'semantic_only'
        results.append(result)
    # ë¬¸ë‹¨ì‹ë³„ì ë¶€ì—¬
    for a in results:
        a['ë¬¸ë‹¨ì‹ë³„ì'] = 1
    return results


def process_paragraph_file(
    input_file: str, 
    output_file: str, 
    embedder_name: str = 'bge',
    max_length: int = 150,
    similarity_threshold: float = 0.3,
    device: str = "cpu",
    quality_threshold: float = 0.8
):
    """íŒŒì¼ ë‹¨ìœ„ ì²˜ë¦¬ - ìˆœì°¨ì /ì˜ë¯¸ì  ì •ë ¬ ëª¨ë‘ ì ìš©, í’ˆì§ˆ ê¸°ì¤€ ì¡°ê±´ë¶€ ì„ íƒ"""
    print(f"ğŸ“‚ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {input_file}")
    df = pd.read_excel(input_file)
    all_results = []
    for idx, row in df.iterrows():
        src_paragraph = str(row.get('ì›ë¬¸', ''))
        tgt_paragraph = str(row.get('ë²ˆì—­ë¬¸', ''))
        if src_paragraph and tgt_paragraph:
            alignments = process_paragraph_alignment(
                src_paragraph,
                tgt_paragraph,
                embedder_name=embedder_name,
                max_length=max_length,
                similarity_threshold=similarity_threshold,
                device=device,
                quality_threshold=quality_threshold
            )
            # ë¬¸ë‹¨ì‹ë³„ì idx+1ë¡œ ë¶€ì—¬
            for a in alignments:
                a['ë¬¸ë‹¨ì‹ë³„ì'] = idx + 1
            all_results.extend(alignments)
    result_df = pd.DataFrame(all_results)
    final_columns = ['ë¬¸ë‹¨ì‹ë³„ì', 'ì›ë¬¸', 'ë²ˆì—­ë¬¸', 'similarity', 'split_method', 'align_method']
    result_df = result_df[final_columns]

    # === ë¬´ê²°ì„± ê²€ì¦ ë° ë³´ì™„ ===
    # ì…ë ¥ ì „ì²´ ì›ë¬¸/ë²ˆì—­ë¬¸ ì—°ê²°
    input_src_all = ''.join([str(row.get('ì›ë¬¸','')) for _, row in df.iterrows()])
    input_tgt_all = ''.join([str(row.get('ë²ˆì—­ë¬¸','')) for _, row in df.iterrows()])
    # ê²°ê³¼ ì „ì²´ ì›ë¬¸/ë²ˆì—­ë¬¸ ì—°ê²°
    output_src_all = ''.join(result_df['ì›ë¬¸'].fillna(''))
    output_tgt_all = ''.join(result_df['ë²ˆì—­ë¬¸'].fillna(''))
    # ì›ë¬¸ ë³´ì™„
    if input_src_all != output_src_all:
        print('âš ï¸ ì›ë¬¸ ë¬´ê²°ì„± ë¶ˆì¼ì¹˜: ëˆ„ë½/ì¤‘ë³µ ë³´ì • ì‹œë„')
        # ëˆ„ë½ë¶„ ì°¾ê¸°
        from difflib import SequenceMatcher
        sm = SequenceMatcher(None, output_src_all, input_src_all)
        opcodes = sm.get_opcodes()
        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'insert':
                # input_src_all[j1:j2]ê°€ ëˆ„ë½ë¨ â†’ ë§ˆì§€ë§‰ ì›ë¬¸ì— ë§ë¶™ì„
                if len(result_df) > 0:
                    result_df.at[result_df.index[-1], 'ì›ë¬¸'] += input_src_all[j1:j2]
                else:
                    # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìƒˆ ìŒ ì¶”ê°€
                    result_df.loc[len(result_df)] = [df.shape[0], input_src_all[j1:j2], '', 1.0, 'integrity', 'src_patch']
            elif tag == 'delete':
                # output_src_all[i1:i2]ê°€ ì¤‘ë³µë¨ â†’ ë§ˆì§€ë§‰ ì›ë¬¸ì—ì„œ ì œê±°
                if len(result_df) > 0:
                    last = result_df.at[result_df.index[-1], 'ì›ë¬¸']
                    result_df.at[result_df.index[-1], 'ì›ë¬¸'] = last.replace(output_src_all[i1:i2], '', 1)
    # ë²ˆì—­ë¬¸ ë³´ì™„
    if input_tgt_all != output_tgt_all:
        print('âš ï¸ ë²ˆì—­ë¬¸ ë¬´ê²°ì„± ë¶ˆì¼ì¹˜: ëˆ„ë½/ì¤‘ë³µ ë³´ì • ì‹œë„')
        from difflib import SequenceMatcher
        sm = SequenceMatcher(None, output_tgt_all, input_tgt_all)
        opcodes = sm.get_opcodes()
        for tag, i1, i2, j1, j2 in opcodes:
            if tag == 'insert':
                if len(result_df) > 0:
                    result_df.at[result_df.index[-1], 'ë²ˆì—­ë¬¸'] += input_tgt_all[j1:j2]
                else:
                    result_df.loc[len(result_df)] = [df.shape[0], '', input_tgt_all[j1:j2], 1.0, 'integrity', 'tgt_patch']
            elif tag == 'delete':
                if len(result_df) > 0:
                    last = result_df.at[result_df.index[-1], 'ë²ˆì—­ë¬¸']
                    result_df.at[result_df.index[-1], 'ë²ˆì—­ë¬¸'] = last.replace(output_tgt_all[i1:i2], '', 1)
    # ìµœì¢… ì¬ì •ë ¬(ì»¬ëŸ¼ ìˆœì„œ ë³´ì¥)
    result_df = result_df[final_columns]
    result_df.to_excel(output_file, index=False)
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
    print(f"ğŸ“Š ì´ {len(all_results)}ê°œ ë¬¸ì¥ ìŒ ìƒì„±")
    return result_df